"""
å®Œå…¨å‹•æ…‹MCP - é›¶ç¡¬ç·¨ç¢¼
æµç¨‹: ç”¨æˆ¶è¼¸å…¥ â†’ Cloud Search MCP â†’ å¤§æ¨¡å‹è­˜åˆ¥é ˜åŸŸ â†’ Domainå°ˆå®¶ â†’ å›ç­”
"""

import asyncio
import aiohttp
import json
from typing import List, Dict, Any

class DynamicMCP:
    """å®Œå…¨å‹•æ…‹MCP - é›¶ç¡¬ç·¨ç¢¼"""
    
    def __init__(self, llm_api_config: Dict[str, Any]):
        self.llm_config = llm_api_config
        # ä¸é å®šç¾©ä»»ä½•å°ˆå®¶æˆ–é ˜åŸŸ
    
    async def call_llm(self, prompt: str, system_prompt: str = "") -> str:
        """èª¿ç”¨å¤§æ¨¡å‹API"""
        try:
            # å¯é…ç½®ä¸åŒçš„LLM API
            if self.llm_config.get("provider") == "openai":
                return await self._call_openai(prompt, system_prompt)
            elif self.llm_config.get("provider") == "claude":
                return await self._call_claude(prompt, system_prompt)
            elif self.llm_config.get("provider") == "local":
                return await self._call_local_llm(prompt, system_prompt)
            else:
                # é»˜èªæ¨¡æ“¬
                await asyncio.sleep(0.1)
                return f"LLMåˆ†æ: {prompt[:100]}..."
        except Exception as e:
            return f"LLMèª¿ç”¨å¤±æ•—: {str(e)}"
    
    async def _call_openai(self, prompt: str, system_prompt: str) -> str:
        """èª¿ç”¨OpenAI API"""
        # å¯¦ç¾OpenAI APIèª¿ç”¨
        pass
    
    async def _call_claude(self, prompt: str, system_prompt: str) -> str:
        """èª¿ç”¨Claude API"""
        # å¯¦ç¾Claude APIèª¿ç”¨
        pass
    
    async def _call_local_llm(self, prompt: str, system_prompt: str) -> str:
        """èª¿ç”¨æœ¬åœ°LLM"""
        # å¯¦ç¾æœ¬åœ°LLMèª¿ç”¨
        pass
    
    async def cloud_search_mcp(self, user_input: str) -> Dict[str, Any]:
        """Cloud Search MCP - æœç´¢ç›¸é—œä¿¡æ¯"""
        search_prompt = f"""
è«‹åˆ†æç”¨æˆ¶è¼¸å…¥ä¸¦æœç´¢ç›¸é—œèƒŒæ™¯ä¿¡æ¯ï¼š
ç”¨æˆ¶è¼¸å…¥: {user_input}

è«‹æä¾›ï¼š
1. é—œéµæ¦‚å¿µè§£é‡‹
2. ç›¸é—œèƒŒæ™¯ä¿¡æ¯
3. å¯èƒ½éœ€è¦çš„å°ˆæ¥­çŸ¥è­˜é ˜åŸŸ
"""
        
        search_result = await self.call_llm(
            search_prompt,
            "ä½ æ˜¯ä¸€å€‹æ™ºèƒ½æœç´¢åŠ©æ‰‹ï¼Œå¹«åŠ©åˆ†æç”¨æˆ¶éœ€æ±‚ä¸¦æä¾›èƒŒæ™¯ä¿¡æ¯ã€‚"
        )
        
        return {
            "search_result": search_result,
            "context_enriched": True
        }
    
    async def identify_domains(self, user_input: str, search_context: str) -> List[str]:
        """ç”¨å¤§æ¨¡å‹è­˜åˆ¥éœ€è¦çš„å°ˆæ¥­é ˜åŸŸ"""
        domain_prompt = f"""
åŸºæ–¼ç”¨æˆ¶è¼¸å…¥å’Œæœç´¢èƒŒæ™¯ï¼Œè«‹è­˜åˆ¥éœ€è¦å“ªäº›å°ˆæ¥­é ˜åŸŸçš„å°ˆå®¶ä¾†å›ç­”é€™å€‹å•é¡Œã€‚

ç”¨æˆ¶è¼¸å…¥: {user_input}
èƒŒæ™¯ä¿¡æ¯: {search_context}

è«‹åªè¿”å›éœ€è¦çš„å°ˆæ¥­é ˜åŸŸåç¨±ï¼Œæ¯è¡Œä¸€å€‹ï¼Œä¾‹å¦‚ï¼š
ä¿éšªå°ˆå®¶
æŠ€è¡“å°ˆå®¶
æ³•å¾‹å°ˆå®¶

ä¸è¦è§£é‡‹ï¼Œåªè¿”å›é ˜åŸŸåç¨±ã€‚
"""
        
        domains_response = await self.call_llm(
            domain_prompt,
            "ä½ æ˜¯å°ˆæ¥­é ˜åŸŸè­˜åˆ¥å°ˆå®¶ï¼Œèƒ½æº–ç¢ºåˆ¤æ–·å•é¡Œéœ€è¦å“ªäº›å°ˆæ¥­çŸ¥è­˜ã€‚"
        )
        
        # è§£æè¿”å›çš„é ˜åŸŸåˆ—è¡¨
        if domains_response:
            domains = [line.strip() for line in domains_response.split('\n') if line.strip()]
            return domains[:3]  # æœ€å¤š3å€‹å°ˆå®¶
        return ["é€šç”¨å°ˆå®¶"]  # é»˜èªå°ˆå®¶
    
    async def generate_expert_prompt(self, domain: str, user_input: str, context: str) -> str:
        """å‹•æ…‹ç”Ÿæˆå°ˆå®¶æç¤ºè©"""
        prompt_generation = f"""
è«‹ç‚º{domain}ç”Ÿæˆä¸€å€‹å°ˆæ¥­çš„æç¤ºè©æ¨¡æ¿ï¼Œç”¨æ–¼å›ç­”ç”¨æˆ¶å•é¡Œã€‚

é ˜åŸŸ: {domain}
ç”¨æˆ¶å•é¡Œ: {user_input}
èƒŒæ™¯ä¿¡æ¯: {context}

è«‹ç”Ÿæˆä¸€å€‹å°ˆæ¥­çš„æç¤ºè©ï¼Œè®“{domain}èƒ½å¤ æä¾›æº–ç¢ºã€å°ˆæ¥­çš„å›ç­”ã€‚
"""
        
        expert_prompt = await self.call_llm(
            prompt_generation,
            "ä½ æ˜¯æç¤ºè©å·¥ç¨‹å°ˆå®¶ï¼Œèƒ½ç‚ºä¸åŒå°ˆæ¥­é ˜åŸŸç”Ÿæˆæœ€é©åˆçš„æç¤ºè©ã€‚"
        )
        
        return expert_prompt
    
    async def ask_domain_expert(self, domain: str, expert_prompt: str, user_input: str) -> str:
        """èª¿ç”¨é ˜åŸŸå°ˆå®¶"""
        final_prompt = f"""
{expert_prompt}

ç”¨æˆ¶å•é¡Œ: {user_input}

è«‹åŸºæ–¼ä½ çš„å°ˆæ¥­çŸ¥è­˜æä¾›è©³ç´°ã€æº–ç¢ºçš„å›ç­”ã€‚
"""
        
        expert_response = await self.call_llm(
            final_prompt,
            f"ä½ æ˜¯{domain}ï¼Œå…·æœ‰è±å¯Œçš„å°ˆæ¥­çŸ¥è­˜å’Œå¯¦å‹™ç¶“é©—ã€‚"
        )
        
        return f"ã€{domain}ã€‘\n{expert_response}"
    
    async def aggregate_expert_responses(self, responses: List[str], user_input: str) -> str:
        """èšåˆå°ˆå®¶å›ç­”"""
        if len(responses) == 1:
            return responses[0]
        
        aggregation_prompt = f"""
è«‹æ•´åˆä»¥ä¸‹å°ˆå®¶çš„å›ç­”ï¼Œç‚ºç”¨æˆ¶æä¾›ä¸€å€‹ç¶œåˆã€é€£è²«çš„æœ€çµ‚ç­”æ¡ˆã€‚

ç”¨æˆ¶å•é¡Œ: {user_input}

å°ˆå®¶å›ç­”:
{chr(10).join(responses)}

è«‹æä¾›ä¸€å€‹æ•´åˆçš„æœ€çµ‚å›ç­”ï¼Œçªå‡ºé‡é»ä¸¦çµ¦å‡ºå¯¦ç”¨å»ºè­°ã€‚
"""
        
        final_answer = await self.call_llm(
            aggregation_prompt,
            "ä½ æ˜¯æ•´åˆå°ˆå®¶ï¼Œèƒ½å°‡å¤šå€‹å°ˆæ¥­è§€é»æ•´åˆæˆé€£è²«çš„æœ€çµ‚ç­”æ¡ˆã€‚"
        )
        
        return final_answer
    
    async def process(self, user_input: str) -> Dict[str, Any]:
        """ä¸»è™•ç†æµç¨‹"""
        try:
            # 1. Cloud Search MCP
            search_result = await self.cloud_search_mcp(user_input)
            
            # 2. å¤§æ¨¡å‹è­˜åˆ¥é ˜åŸŸ
            domains = await self.identify_domains(user_input, search_result["search_result"])
            
            # 3. å‹•æ…‹ç”Ÿæˆå°ˆå®¶æç¤ºè©ä¸¦èª¿ç”¨
            expert_responses = []
            for domain in domains:
                expert_prompt = await self.generate_expert_prompt(
                    domain, user_input, search_result["search_result"]
                )
                response = await self.ask_domain_expert(domain, expert_prompt, user_input)
                expert_responses.append(response)
            
            # 4. èšåˆå›ç­”
            final_answer = await self.aggregate_expert_responses(expert_responses, user_input)
            
            return {
                "final_answer": final_answer,
                "domains_identified": domains,
                "expert_count": len(domains),
                "search_context": search_result["search_result"],
                "process_type": "fully_dynamic"
            }
            
        except Exception as e:
            return {
                "final_answer": f"è™•ç†å¤±æ•—: {str(e)}",
                "domains_identified": [],
                "expert_count": 0,
                "search_context": "",
                "process_type": "error"
            }

# ä½¿ç”¨ç¤ºä¾‹
async def test_dynamic_mcp():
    """æ¸¬è©¦å®Œå…¨å‹•æ…‹MCP"""
    
    # é…ç½®LLM API
    llm_config = {
        "provider": "local",  # æˆ– "openai", "claude"
        "api_key": "your_api_key",
        "base_url": "http://localhost:11434/v1"
    }
    
    mcp = DynamicMCP(llm_config)
    
    # æ¸¬è©¦ç”¨æˆ¶è¼¸å…¥
    user_input = "è‡ºéŠ€äººå£½ä¿å–®è¡Œæ”¿ä½œæ¥­SOPå¤§æ¦‚è¦èŠ±å¤šå°‘äººè™•ç†è¡¨å–®ï¼Œè‡ªå‹•åŒ–æ¯”ç‡åœ¨æ¥­ç•Œæœ‰å¤šé«˜ï¼Ÿ"
    
    print("ğŸš€ å®Œå…¨å‹•æ…‹MCPè™•ç†ä¸­...")
    result = await mcp.process(user_input)
    
    print(f"è­˜åˆ¥é ˜åŸŸ: {result['domains_identified']}")
    print(f"å°ˆå®¶æ•¸é‡: {result['expert_count']}")
    print(f"è™•ç†é¡å‹: {result['process_type']}")
    print("\næœ€çµ‚å›ç­”:")
    print(result['final_answer'])

if __name__ == "__main__":
    asyncio.run(test_dynamic_mcp())

