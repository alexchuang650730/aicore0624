"""
AICore 2.0 - å…­éšæ®µè™•ç†æµç¨‹èˆ‡å°ˆå®¶ç³»çµ±æ•´åˆ
Enhanced AICore with Six-Stage Processing and Expert System Integration
"""

import asyncio
import time
import logging
import json
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, asdict
from enum import Enum
from datetime import datetime

# å°å…¥çµ„ä»¶
from components.general_processor_mcp import GeneralProcessorMCP, create_general_processor_mcp
from tools.tool_registry import ToolRegistry
from actions.action_executor import ActionExecutor

logger = logging.getLogger(__name__)

class ExpertType(Enum):
    """é ˜åŸŸå°ˆå®¶é¡å‹"""
    TECHNICAL_EXPERT = "technical_expert"
    BUSINESS_EXPERT = "business_expert"
    DATA_EXPERT = "data_expert"
    INTEGRATION_EXPERT = "integration_expert"
    API_EXPERT = "api_expert"
    SECURITY_EXPERT = "security_expert"
    PERFORMANCE_EXPERT = "performance_expert"

class ProcessingStage(Enum):
    """è™•ç†éšæ®µ"""
    BACKGROUND_SEARCH = "background_search"
    EXPERT_IDENTIFICATION = "expert_identification"
    EXPERT_RESPONSE_GENERATION = "expert_response_generation"
    EXPERT_AGGREGATION = "expert_aggregation"
    INTELLIGENT_TOOL_EXECUTION = "intelligent_tool_execution"
    FINAL_RESULT_GENERATION = "final_result_generation"

@dataclass
class UserRequest:
    """ç”¨æˆ¶è«‹æ±‚æ•¸æ“šçµæ§‹"""
    id: str
    content: str
    context: Dict[str, Any]
    priority: str = "normal"
    metadata: Dict[str, Any] = None
    timestamp: float = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.timestamp is None:
            self.timestamp = time.time()

@dataclass
class ExpertResponse:
    """å°ˆå®¶éŸ¿æ‡‰æ•¸æ“šçµæ§‹"""
    expert_type: ExpertType
    analysis: str
    recommendations: List[str]
    tool_suggestions: List[Dict[str, Any]]
    confidence: float
    next_action: str = None
    processing_time: float = 0.0
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

@dataclass
class ProcessingResult:
    """è™•ç†çµæœæ•¸æ“šçµæ§‹"""
    request_id: str
    success: bool
    stage_results: Dict[str, Any]
    expert_analysis: List[ExpertResponse]
    tool_execution_results: List[Dict[str, Any]]
    final_answer: str
    confidence: float
    execution_time: float
    metadata: Dict[str, Any]
    timestamp: float = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()

class MCPClient:
    """MCPå®¢æˆ¶ç«¯ï¼Œè² è²¬èˆ‡å„ç¨®MCPæœå‹™é€šä¿¡"""
    
    def __init__(self):
        self.service_endpoints = {
            'cloud_search': 'http://localhost:8080/search',
            'expert_identification': 'http://localhost:8081/identify',
            'expert_generation': 'http://localhost:8082/generate',
            'aggregation': 'http://localhost:8083/aggregate'
        }
        self.mock_mode = True  # é–‹ç™¼éšæ®µä½¿ç”¨æ¨¡æ“¬æ¨¡å¼
    
    async def search_background_info(self, params: Dict) -> Dict:
        """æœç´¢èƒŒæ™¯ä¿¡æ¯"""
        if self.mock_mode:
            return {
                "results": [
                    f"ç›¸é—œæœç´¢çµæœ1: {params.get('query', '')}",
                    f"ç›¸é—œæœç´¢çµæœ2: {params.get('query', '')}"
                ],
                "documents": ["æ–‡æª”1", "æ–‡æª”2"],
                "context": {"enriched": True, "relevance_score": 0.85}
            }
        # å¯¦éš›MCPèª¿ç”¨é‚è¼¯
        # async with aiohttp.ClientSession() as session:
        #     async with session.post(self.service_endpoints['cloud_search'], json=params) as response:
        #         return await response.json()
    
    async def identify_experts(self, params: Dict) -> Dict:
        """è­˜åˆ¥ç›¸é—œå°ˆå®¶"""
        content = params.get("request_content", "")
        context = params.get("context", {})
        
        # æ™ºèƒ½å°ˆå®¶è­˜åˆ¥é‚è¼¯
        experts = []
        content_lower = content.lower()
        
        if any(keyword in content_lower for keyword in ["api", "æ¥å£", "interface"]):
            experts.append("api_expert")
        if any(keyword in content_lower for keyword in ["é›†æˆ", "æ•´åˆ", "integration"]):
            experts.append("integration_expert")
        if any(keyword in content_lower for keyword in ["æŠ€è¡“", "ä»£ç¢¼", "technical", "code"]):
            experts.append("technical_expert")
        if any(keyword in content_lower for keyword in ["æ¥­å‹™", "éœ€æ±‚", "business", "requirement"]):
            experts.append("business_expert")
        if any(keyword in content_lower for keyword in ["æ•¸æ“š", "åˆ†æ", "data", "analysis"]):
            experts.append("data_expert")
        if any(keyword in content_lower for keyword in ["å®‰å…¨", "security"]):
            experts.append("security_expert")
        if any(keyword in content_lower for keyword in ["æ€§èƒ½", "å„ªåŒ–", "performance"]):
            experts.append("performance_expert")
        
        return {
            "recommended_experts": experts or ["technical_expert"],
            "confidence": 0.9,
            "reasoning": f"åŸºæ–¼å…§å®¹åˆ†æè­˜åˆ¥å‡º {len(experts)} ä½ç›¸é—œå°ˆå®¶"
        }
    
    async def generate_expert_response(self, params: Dict) -> Dict:
        """ç”Ÿæˆå°ˆå®¶å›ç­”"""
        expert_type = params.get("expert_type")
        content = params.get("request_content")
        context = params.get("context", {})
        
        # æ¨¡æ“¬ä¸åŒå°ˆå®¶çš„å›ç­”
        expert_responses = {
            "technical_expert": {
                "analysis": f"ä½œç‚ºæŠ€è¡“å°ˆå®¶ï¼Œæˆ‘åˆ†æäº†è«‹æ±‚ï¼š{content[:100]}...\n\nå¾æŠ€è¡“è§’åº¦ä¾†çœ‹ï¼Œé€™å€‹å•é¡Œæ¶‰åŠç³»çµ±æ¶æ§‹ã€ä»£ç¢¼å¯¦ç¾å’ŒæŠ€è¡“é¸å‹ã€‚å»ºè­°æ¡ç”¨æ¨¡å¡ŠåŒ–è¨­è¨ˆï¼Œç¢ºä¿ä»£ç¢¼çš„å¯ç¶­è­·æ€§å’Œæ“´å±•æ€§ã€‚",
                "recommendations": [
                    "æ¡ç”¨å¾®æœå‹™æ¶æ§‹æé«˜ç³»çµ±å¯æ“´å±•æ€§",
                    "å¯¦æ–½ä»£ç¢¼å¯©æŸ¥å’Œè‡ªå‹•åŒ–æ¸¬è©¦",
                    "ä½¿ç”¨å®¹å™¨åŒ–æŠ€è¡“ç°¡åŒ–éƒ¨ç½²",
                    "å»ºç«‹ç›£æ§å’Œæ—¥èªŒç³»çµ±"
                ],
                "tool_suggestions": [
                    {"tool_name": "general_processor_mcp", "mode": "text", "reason": "éœ€è¦æŠ€è¡“æ–‡æª”åˆ†æ", "priority": "high"},
                    {"tool_name": "test_flow_mcp", "reason": "éœ€è¦æŠ€è¡“æ¸¬è©¦é©—è­‰", "priority": "medium"}
                ],
                "confidence": 0.92
            },
            "api_expert": {
                "analysis": f"ä½œç‚ºAPIå°ˆå®¶ï¼Œæˆ‘å»ºè­°ï¼š{content[:100]}...\n\nAPIè¨­è¨ˆéœ€è¦è€ƒæ…®RESTfulåŸå‰‡ã€ç‰ˆæœ¬æ§åˆ¶ã€å®‰å…¨æ€§å’Œæ€§èƒ½å„ªåŒ–ã€‚å»ºè­°ä½¿ç”¨æ¨™æº–çš„HTTPç‹€æ…‹ç¢¼å’ŒéŒ¯èª¤è™•ç†æ©Ÿåˆ¶ã€‚",
                "recommendations": [
                    "éµå¾ªRESTful APIè¨­è¨ˆåŸå‰‡",
                    "å¯¦æ–½APIç‰ˆæœ¬æ§åˆ¶ç­–ç•¥",
                    "æ·»åŠ APIèªè­‰å’Œæˆæ¬Šæ©Ÿåˆ¶",
                    "å¯¦ç¾APIé™æµå’Œç›£æ§"
                ],
                "tool_suggestions": [
                    {"tool_name": "general_processor_mcp", "mode": "json", "reason": "éœ€è¦APIæ•¸æ“šè™•ç†", "priority": "high"},
                    {"tool_name": "test_flow_mcp", "reason": "éœ€è¦APIæ¸¬è©¦", "priority": "high"}
                ],
                "confidence": 0.88
            },
            "business_expert": {
                "analysis": f"å¾æ¥­å‹™è§’åº¦åˆ†æï¼š{content[:100]}...\n\néœ€è¦è€ƒæ…®æ¥­å‹™éœ€æ±‚ã€ç”¨æˆ¶é«”é©—å’Œå•†æ¥­åƒ¹å€¼ã€‚å»ºè­°é€²è¡Œéœ€æ±‚åˆ†æå’Œç”¨æˆ¶èª¿ç ”ï¼Œç¢ºä¿è§£æ±ºæ–¹æ¡ˆç¬¦åˆæ¥­å‹™ç›®æ¨™ã€‚",
                "recommendations": [
                    "é€²è¡Œè©³ç´°çš„éœ€æ±‚åˆ†æ",
                    "è€ƒæ…®ç”¨æˆ¶é«”é©—å’Œç•Œé¢è¨­è¨ˆ",
                    "è©•ä¼°å•†æ¥­åƒ¹å€¼å’ŒæŠ•è³‡å›å ±",
                    "åˆ¶å®šé …ç›®æ™‚é–“ç·šå’Œé‡Œç¨‹ç¢‘"
                ],
                "tool_suggestions": [
                    {"tool_name": "general_processor_mcp", "mode": "general", "reason": "éœ€è¦æ¥­å‹™åˆ†æè™•ç†", "priority": "medium"}
                ],
                "confidence": 0.85
            },
            "data_expert": {
                "analysis": f"æ•¸æ“šå°ˆå®¶åˆ†æï¼š{content[:100]}...\n\næ•¸æ“šè™•ç†éœ€è¦è€ƒæ…®æ•¸æ“šè³ªé‡ã€å­˜å„²æ•ˆç‡å’Œåˆ†ææ€§èƒ½ã€‚å»ºè­°å»ºç«‹æ•¸æ“šæ²»ç†æ¡†æ¶å’Œæ•¸æ“šè³ªé‡ç›£æ§æ©Ÿåˆ¶ã€‚",
                "recommendations": [
                    "å»ºç«‹æ•¸æ“šè³ªé‡ç›£æ§é«”ç³»",
                    "å¯¦æ–½æ•¸æ“šå‚™ä»½å’Œæ¢å¾©ç­–ç•¥",
                    "å„ªåŒ–æ•¸æ“šå­˜å„²å’ŒæŸ¥è©¢æ€§èƒ½",
                    "ç¢ºä¿æ•¸æ“šå®‰å…¨å’Œéš±ç§ä¿è­·"
                ],
                "tool_suggestions": [
                    {"tool_name": "general_processor_mcp", "mode": "json", "reason": "éœ€è¦æ•¸æ“šçµæ§‹åˆ†æ", "priority": "high"},
                    {"tool_name": "file_processor_adapter_mcp", "reason": "éœ€è¦æ•¸æ“šæ–‡ä»¶è™•ç†", "priority": "medium"}
                ],
                "confidence": 0.90
            },
            "integration_expert": {
                "analysis": f"é›†æˆå°ˆå®¶å»ºè­°ï¼š{content[:100]}...\n\nç³»çµ±é›†æˆéœ€è¦è€ƒæ…®æ¥å£æ¨™æº–åŒ–ã€æ•¸æ“šä¸€è‡´æ€§å’ŒéŒ¯èª¤è™•ç†ã€‚å»ºè­°æ¡ç”¨äº‹ä»¶é©…å‹•æ¶æ§‹å’Œç•°æ­¥è™•ç†æ©Ÿåˆ¶ã€‚",
                "recommendations": [
                    "æ¨™æº–åŒ–ç³»çµ±é–“æ¥å£å”è­°",
                    "å¯¦æ–½æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥",
                    "å»ºç«‹éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶",
                    "ä½¿ç”¨æ¶ˆæ¯éšŠåˆ—è™•ç†ç•°æ­¥ä»»å‹™"
                ],
                "tool_suggestions": [
                    {"tool_name": "general_processor_mcp", "mode": "auto", "reason": "éœ€è¦é›†æˆæ•¸æ“šè™•ç†", "priority": "high"},
                    {"tool_name": "test_flow_mcp", "reason": "éœ€è¦é›†æˆæ¸¬è©¦", "priority": "high"}
                ],
                "confidence": 0.87
            }
        }
        
        default_response = {
            "analysis": f"ä½œç‚º{expert_type}å°ˆå®¶çš„åˆ†æ...",
            "recommendations": ["é€šç”¨å»ºè­°1", "é€šç”¨å»ºè­°2"],
            "tool_suggestions": [
                {"tool_name": "general_processor_mcp", "mode": "auto", "reason": "é€šç”¨è™•ç†éœ€æ±‚", "priority": "medium"}
            ],
            "confidence": 0.8
        }
        
        return expert_responses.get(expert_type, default_response)
    
    async def aggregate_responses(self, params: Dict) -> Dict:
        """èšåˆå°ˆå®¶å›ç­”"""
        expert_responses = params.get("expert_responses", [])
        
        if not expert_responses:
            return {
                "aggregated_analysis": "ç„¡å°ˆå®¶å›ç­”å¯èšåˆ",
                "consensus_recommendations": [],
                "confidence_score": 0.0
            }
        
        # èšåˆåˆ†æ
        all_analyses = [resp.get("analysis", "") for resp in expert_responses]
        all_recommendations = []
        confidence_scores = []
        
        for resp in expert_responses:
            all_recommendations.extend(resp.get("recommendations", []))
            confidence_scores.append(resp.get("confidence", 0.8))
        
        # å»é‡å»ºè­°
        unique_recommendations = list(dict.fromkeys(all_recommendations))
        
        # è¨ˆç®—å¹³å‡ä¿¡å¿ƒåº¦
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
        
        return {
            "aggregated_analysis": f"ç¶œåˆ {len(expert_responses)} ä½å°ˆå®¶çš„åˆ†æçµæœ",
            "consensus_recommendations": unique_recommendations[:10],  # å–å‰10å€‹å»ºè­°
            "confidence_score": round(avg_confidence, 2),
            "expert_count": len(expert_responses),
            "recommendation_count": len(unique_recommendations)
        }

class AICore2:
    """
    AICore 2.0 - å…­éšæ®µè™•ç†æµç¨‹èˆ‡å°ˆå®¶ç³»çµ±æ•´åˆ
    
    å…­å€‹è™•ç†éšæ®µï¼š
    1. æœç´¢èƒŒæ™¯ä¿¡æ¯
    2. è­˜åˆ¥é ˜åŸŸå°ˆå®¶
    3. ç”Ÿæˆå°ˆå®¶å›ç­” (ä¸¦è¡Œ)
    4. èšåˆå°ˆå®¶å»ºè­°
    5. æ™ºèƒ½å·¥å…·é¸æ“‡å’ŒåŸ·è¡Œ
    6. ç”Ÿæˆæœ€çµ‚çµæœ
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.mcp_client = MCPClient()
        self.general_processor = create_general_processor_mcp()
        self.tool_registry = ToolRegistry()
        self.action_executor = ActionExecutor()
        
        # åŸ·è¡Œçµ±è¨ˆ
        self.execution_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_execution_time': 0.0,
            'stage_performance': {stage.value: {'count': 0, 'avg_time': 0.0} for stage in ProcessingStage},
            'expert_usage_stats': {expert.value: 0 for expert in ExpertType},
            'tool_usage_stats': {},
            'confidence_distribution': {'high': 0, 'medium': 0, 'low': 0}
        }
        
        logger.info("AICore 2.0 åˆå§‹åŒ–å®Œæˆ - å…­éšæ®µè™•ç†æµç¨‹å·²å°±ç·’")
    
    async def initialize(self):
        """åˆå§‹åŒ–AICoreç³»çµ±"""
        logger.info("ğŸš€ åˆå§‹åŒ–AICore 2.0ç³»çµ±...")
        
        # åˆå§‹åŒ–Tool Registry
        await self.tool_registry.initialize()
        
        # Action Executorä¸éœ€è¦åˆå§‹åŒ–æ–¹æ³•ï¼Œç›´æ¥è·³é
        logger.info("âœ… Action Executor å·²å°±ç·’")
        
        logger.info("âœ… AICore 2.0ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    async def process_request(self, request: UserRequest) -> ProcessingResult:
        """
        è™•ç†ç”¨æˆ¶è«‹æ±‚çš„ä¸»è¦å…¥å£é»
        å¯¦ç¾å…­éšæ®µè™•ç†æµç¨‹
        """
        start_time = time.time()
        stage_results = {}
        
        logger.info(f"ğŸš€ AICore 2.0 é–‹å§‹è™•ç†è«‹æ±‚: {request.id}")
        
        try:
            # éšæ®µ1: æœç´¢èƒŒæ™¯ä¿¡æ¯
            stage_results['background_search'] = await self._stage1_search_background_info(request)
            
            # éšæ®µ2: è­˜åˆ¥é ˜åŸŸå°ˆå®¶
            stage_results['expert_identification'] = await self._stage2_identify_expert_domains(
                request, stage_results['background_search']
            )
            
            # éšæ®µ3: ç”Ÿæˆå°ˆå®¶å›ç­” (ä¸¦è¡Œ)
            stage_results['expert_response_generation'] = await self._stage3_generate_expert_responses(
                request, stage_results['expert_identification']['experts']
            )
            
            # éšæ®µ4: èšåˆå°ˆå®¶å»ºè­°
            stage_results['expert_aggregation'] = await self._stage4_aggregate_expert_analysis(
                stage_results['expert_response_generation']['expert_responses']
            )
            
            # éšæ®µ5: æ™ºèƒ½å·¥å…·é¸æ“‡å’ŒåŸ·è¡Œ
            stage_results['intelligent_tool_execution'] = await self._stage5_execute_intelligent_tools(
                request, stage_results['expert_response_generation']['expert_responses']
            )
            
            # éšæ®µ6: ç”Ÿæˆæœ€çµ‚çµæœ
            final_result = await self._stage6_generate_final_result(
                request, stage_results, start_time
            )
            
            # æ›´æ–°çµ±è¨ˆ
            self._update_execution_stats(final_result, stage_results)
            
            logger.info(f"âœ… è«‹æ±‚è™•ç†å®Œæˆ: {request.id}, è€—æ™‚: {final_result.execution_time:.2f}s")
            return final_result
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ è«‹æ±‚è™•ç†å¤±æ•—: {request.id}, éŒ¯èª¤: {e}")
            
            return ProcessingResult(
                request_id=request.id,
                success=False,
                stage_results=stage_results,
                expert_analysis=[],
                tool_execution_results=[],
                final_answer=f"è™•ç†å¤±æ•—: {str(e)}",
                confidence=0.0,
                execution_time=execution_time,
                metadata={"error": str(e), "failed_stage": len(stage_results)}
            )
    
    async def _stage1_search_background_info(self, request: UserRequest) -> Dict[str, Any]:
        """éšæ®µ1: æœç´¢èƒŒæ™¯ä¿¡æ¯"""
        stage_start = time.time()
        logger.info(f"ğŸ” éšæ®µ1: æœç´¢èƒŒæ™¯ä¿¡æ¯ - {request.id}")
        
        search_params = {
            "query": request.content,
            "context": request.context,
            "max_results": 10,
            "include_context": True
        }
        
        search_result = await self.mcp_client.search_background_info(search_params)
        
        stage_time = time.time() - stage_start
        self._update_stage_stats(ProcessingStage.BACKGROUND_SEARCH, stage_time)
        
        return {
            "search_results": search_result.get("results", []),
            "relevant_documents": search_result.get("documents", []),
            "context_enrichment": search_result.get("context", {}),
            "stage_execution_time": stage_time,
            "relevance_score": search_result.get("context", {}).get("relevance_score", 0.0)
        }
    
    async def _stage2_identify_expert_domains(self, request: UserRequest, background_info: Dict) -> Dict[str, Any]:
        """éšæ®µ2: è­˜åˆ¥é ˜åŸŸå°ˆå®¶"""
        stage_start = time.time()
        logger.info(f"ğŸ¯ éšæ®µ2: è­˜åˆ¥é ˜åŸŸå°ˆå®¶ - {request.id}")
        
        identification_params = {
            "request_content": request.content,
            "context": request.context,
            "background_info": background_info
        }
        
        expert_identification = await self.mcp_client.identify_experts(identification_params)
        
        # è§£æå°ˆå®¶é¡å‹
        expert_types = []
        for expert_name in expert_identification.get("recommended_experts", []):
            try:
                expert_type = ExpertType(expert_name)
                expert_types.append(expert_type)
            except ValueError:
                logger.warning(f"æœªçŸ¥å°ˆå®¶é¡å‹: {expert_name}")
        
        # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹å°ˆå®¶
        if not expert_types:
            expert_types = [ExpertType.TECHNICAL_EXPERT]
        
        stage_time = time.time() - stage_start
        self._update_stage_stats(ProcessingStage.EXPERT_IDENTIFICATION, stage_time)
        
        logger.info(f"ğŸ“‹ è­˜åˆ¥åˆ°å°ˆå®¶: {[e.value for e in expert_types]}")
        
        return {
            "experts": expert_types,
            "identification_confidence": expert_identification.get("confidence", 0.8),
            "reasoning": expert_identification.get("reasoning", ""),
            "stage_execution_time": stage_time,
            "expert_count": len(expert_types)
        }
    
    async def _stage3_generate_expert_responses(self, request: UserRequest, experts: List[ExpertType]) -> Dict[str, Any]:
        """éšæ®µ3: ç”Ÿæˆå°ˆå®¶å›ç­” (ä¸¦è¡Œ)"""
        stage_start = time.time()
        logger.info(f"ğŸ­ éšæ®µ3: ç”Ÿæˆå°ˆå®¶å›ç­” - {request.id}")
        
        # ä¸¦è¡Œèª¿ç”¨å¤šå€‹å°ˆå®¶
        expert_tasks = []
        for expert_type in experts:
            task = self._call_domain_expert(expert_type, request)
            expert_tasks.append(task)
        
        expert_responses = await asyncio.gather(*expert_tasks, return_exceptions=True)
        
        # è™•ç†ç•°å¸¸çµæœ
        valid_responses = []
        for i, response in enumerate(expert_responses):
            if isinstance(response, Exception):
                logger.error(f"å°ˆå®¶ {experts[i].value} èª¿ç”¨å¤±æ•—: {response}")
            else:
                valid_responses.append(response)
        
        stage_time = time.time() - stage_start
        self._update_stage_stats(ProcessingStage.EXPERT_RESPONSE_GENERATION, stage_time)
        
        logger.info(f"âœ… ç²å¾— {len(valid_responses)} å€‹æœ‰æ•ˆå°ˆå®¶å›ç­”")
        
        return {
            "expert_responses": valid_responses,
            "total_experts_called": len(experts),
            "successful_responses": len(valid_responses),
            "stage_execution_time": stage_time,
            "parallel_efficiency": len(valid_responses) / len(experts) if experts else 0.0
        }
    
    async def _call_domain_expert(self, expert_type: ExpertType, request: UserRequest) -> ExpertResponse:
        """èª¿ç”¨ç‰¹å®šé ˜åŸŸå°ˆå®¶"""
        expert_start = time.time()
        logger.info(f"ğŸ‘¨â€ğŸ’¼ èª¿ç”¨ {expert_type.value} å°ˆå®¶")
        
        expert_params = {
            "expert_type": expert_type.value,
            "request_content": request.content,
            "context": request.context
        }
        
        expert_result = await self.mcp_client.generate_expert_response(expert_params)
        
        processing_time = time.time() - expert_start
        
        # æ›´æ–°å°ˆå®¶ä½¿ç”¨çµ±è¨ˆ
        self.execution_stats['expert_usage_stats'][expert_type.value] += 1
        
        return ExpertResponse(
            expert_type=expert_type,
            analysis=expert_result.get("analysis", ""),
            recommendations=expert_result.get("recommendations", []),
            tool_suggestions=expert_result.get("tool_suggestions", []),
            confidence=expert_result.get("confidence", 0.8),
            next_action=expert_result.get("next_action"),
            processing_time=processing_time,
            metadata={
                "expert_call_time": processing_time,
                "recommendation_count": len(expert_result.get("recommendations", [])),
                "tool_suggestion_count": len(expert_result.get("tool_suggestions", []))
            }
        )
    
    async def _stage4_aggregate_expert_analysis(self, expert_responses: List[ExpertResponse]) -> Dict[str, Any]:
        """éšæ®µ4: èšåˆå°ˆå®¶å»ºè­°"""
        stage_start = time.time()
        logger.info(f"ğŸ”„ éšæ®µ4: èšåˆå°ˆå®¶å»ºè­°")
        
        if not expert_responses:
            return {
                "aggregated_analysis": "ç„¡å°ˆå®¶å›ç­”å¯èšåˆ",
                "consensus_recommendations": [],
                "confidence_score": 0.0,
                "stage_execution_time": 0.0
            }
        
        # æº–å‚™èšåˆåƒæ•¸
        aggregation_params = {
            "expert_responses": [
                {
                    "expert_type": resp.expert_type.value,
                    "analysis": resp.analysis,
                    "recommendations": resp.recommendations,
                    "confidence": resp.confidence
                }
                for resp in expert_responses
            ]
        }
        
        aggregation_result = await self.mcp_client.aggregate_responses(aggregation_params)
        
        stage_time = time.time() - stage_start
        self._update_stage_stats(ProcessingStage.EXPERT_AGGREGATION, stage_time)
        
        return {
            **aggregation_result,
            "stage_execution_time": stage_time,
            "expert_response_count": len(expert_responses)
        }
    
    async def _stage5_execute_intelligent_tools(self, request: UserRequest, expert_responses: List[ExpertResponse]) -> Dict[str, Any]:
        """éšæ®µ5: æ™ºèƒ½å·¥å…·é¸æ“‡å’ŒåŸ·è¡Œ"""
        stage_start = time.time()
        logger.info(f"ğŸ› ï¸ éšæ®µ5: æ™ºèƒ½å·¥å…·é¸æ“‡å’ŒåŸ·è¡Œ")
        
        # å¾å°ˆå®¶å»ºè­°ä¸­æå–å·¥å…·å»ºè­°
        tool_suggestions = []
        for expert_response in expert_responses:
            tool_suggestions.extend(expert_response.tool_suggestions)
        
        # æ™ºèƒ½å·¥å…·é¸æ“‡
        selected_tools = await self._intelligent_tool_selection(request, tool_suggestions)
        
        # ä¸¦è¡ŒåŸ·è¡Œå·¥å…·
        execution_results = []
        for tool_config in selected_tools:
            try:
                if tool_config.get("tool_name") == "general_processor_mcp":
                    # èª¿ç”¨General_Processor MCP
                    result = await self._execute_general_processor(request, tool_config, expert_responses)
                    execution_results.append(result)
                else:
                    # èª¿ç”¨å…¶ä»–å·¥å…·
                    result = await self._execute_other_tool(tool_config, request)
                    execution_results.append(result)
                    
                # æ›´æ–°å·¥å…·ä½¿ç”¨çµ±è¨ˆ
                tool_name = tool_config.get("tool_name", "unknown")
                self.execution_stats['tool_usage_stats'][tool_name] = (
                    self.execution_stats['tool_usage_stats'].get(tool_name, 0) + 1
                )
                
            except Exception as e:
                logger.error(f"å·¥å…·åŸ·è¡Œå¤±æ•—: {tool_config.get('tool_name')}, éŒ¯èª¤: {e}")
                execution_results.append({
                    "tool_name": tool_config.get("tool_name"),
                    "success": False,
                    "error": str(e),
                    "execution_time": 0.0
                })
        
        stage_time = time.time() - stage_start
        self._update_stage_stats(ProcessingStage.INTELLIGENT_TOOL_EXECUTION, stage_time)
        
        return {
            "tool_execution_results": execution_results,
            "tools_selected": len(selected_tools),
            "successful_executions": len([r for r in execution_results if r.get("success", False)]),
            "stage_execution_time": stage_time,
            "execution_efficiency": len([r for r in execution_results if r.get("success", False)]) / len(selected_tools) if selected_tools else 0.0
        }
    
    async def _intelligent_tool_selection(self, request: UserRequest, tool_suggestions: List[Dict]) -> List[Dict[str, Any]]:
        """æ™ºèƒ½å·¥å…·é¸æ“‡é‚è¼¯"""
        selected_tools = []
        tool_priorities = {}
        
        # åˆ†æå·¥å…·å»ºè­°
        for suggestion in tool_suggestions:
            tool_name = suggestion.get("tool_name", "general_processor_mcp")
            reason = suggestion.get("reason", "")
            priority = suggestion.get("priority", "medium")
            mode = suggestion.get("mode", "auto")
            
            # ç´¯ç©å·¥å…·å„ªå…ˆç´š
            if tool_name not in tool_priorities:
                tool_priorities[tool_name] = {"count": 0, "priority_sum": 0, "modes": [], "reasons": []}
            
            tool_priorities[tool_name]["count"] += 1
            tool_priorities[tool_name]["priority_sum"] += {"high": 3, "medium": 2, "low": 1}.get(priority, 2)
            tool_priorities[tool_name]["modes"].append(mode)
            tool_priorities[tool_name]["reasons"].append(reason)
        
        # é¸æ“‡å·¥å…·
        for tool_name, stats in tool_priorities.items():
            avg_priority = stats["priority_sum"] / stats["count"]
            most_common_mode = max(set(stats["modes"]), key=stats["modes"].count) if stats["modes"] else "auto"
            
            selected_tools.append({
                "tool_name": tool_name,
                "mode": most_common_mode,
                "priority": "high" if avg_priority >= 2.5 else "medium" if avg_priority >= 1.5 else "low",
                "suggestion_count": stats["count"],
                "reasons": stats["reasons"]
            })
        
        # ç¢ºä¿è‡³å°‘æœ‰General_Processor MCP
        if not any(tool["tool_name"] == "general_processor_mcp" for tool in selected_tools):
            selected_tools.append({
                "tool_name": "general_processor_mcp",
                "mode": "general",
                "priority": "high",
                "suggestion_count": 1,
                "reasons": ["é»˜èªé€šç”¨è™•ç†"]
            })
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        priority_order = {"high": 3, "medium": 2, "low": 1}
        selected_tools.sort(key=lambda x: priority_order.get(x["priority"], 0), reverse=True)
        
        logger.info(f"ğŸ¯ é¸æ“‡å·¥å…·: {[t['tool_name'] for t in selected_tools]}")
        return selected_tools
    
    async def _execute_general_processor(self, request: UserRequest, tool_config: Dict, expert_responses: List[ExpertResponse]) -> Dict[str, Any]:
        """åŸ·è¡ŒGeneral_Processor MCP"""
        logger.info(f"âš™ï¸ åŸ·è¡Œ General_Processor MCP")
        
        # æº–å‚™è™•ç†æ•¸æ“š
        processing_data = {
            "content": request.content,
            "context": request.context,
            "expert_insights": [
                {
                    "expert_type": resp.expert_type.value,
                    "analysis": resp.analysis,
                    "recommendations": resp.recommendations,
                    "confidence": resp.confidence
                }
                for resp in expert_responses
            ],
            "metadata": {
                "request_id": request.id,
                "expert_count": len(expert_responses),
                "processing_timestamp": time.time()
            }
        }
        
        # èª¿ç”¨General_Processor MCP
        mode = tool_config.get("mode", "auto")
        options = {
            "priority": tool_config.get("priority", "medium"),
            "reasons": tool_config.get("reasons", [])
        }
        
        result = await self.general_processor.process(processing_data, mode, options)
        
        return {
            "tool_name": "general_processor_mcp",
            "mode_used": result.mode_used,
            "success": result.success,
            "output": result.data,
            "execution_time": result.execution_time,
            "metadata": result.metadata
        }
    
    async def _execute_other_tool(self, tool_config: Dict, request: UserRequest) -> Dict[str, Any]:
        """åŸ·è¡Œå…¶ä»–å·¥å…·"""
        tool_name = tool_config.get("tool_name")
        logger.info(f"ğŸ”§ åŸ·è¡Œå·¥å…·: {tool_name}")
        
        # æ¨¡æ“¬å·¥å…·åŸ·è¡Œ
        execution_time = 0.5  # æ¨¡æ“¬åŸ·è¡Œæ™‚é–“
        await asyncio.sleep(execution_time)
        
        return {
            "tool_name": tool_name,
            "success": True,
            "output": f"{tool_name} æˆåŠŸè™•ç†äº†è«‹æ±‚: {request.content[:50]}...",
            "execution_time": execution_time,
            "metadata": {
                "tool_config": tool_config,
                "request_id": request.id
            }
        }
    
    async def _stage6_generate_final_result(self, request: UserRequest, stage_results: Dict, start_time: float) -> ProcessingResult:
        """éšæ®µ6: ç”Ÿæˆæœ€çµ‚çµæœ"""
        stage_start = time.time()
        logger.info(f"ğŸ“Š éšæ®µ6: ç”Ÿæˆæœ€çµ‚çµæœ")
        
        execution_time = time.time() - start_time
        
        # æå–å°ˆå®¶å›ç­”
        expert_responses = stage_results.get('expert_response_generation', {}).get('expert_responses', [])
        
        # æå–å·¥å…·åŸ·è¡Œçµæœ
        tool_results = stage_results.get('intelligent_tool_execution', {}).get('tool_execution_results', [])
        
        # èšåˆæ‰€æœ‰çµæœç”Ÿæˆæœ€çµ‚å›ç­”
        final_answer_parts = []
        
        # æ·»åŠ èƒŒæ™¯ä¿¡æ¯æ‘˜è¦
        background_info = stage_results.get('background_search', {})
        if background_info.get('relevance_score', 0) > 0.7:
            final_answer_parts.append(f"**èƒŒæ™¯åˆ†æ**: åŸºæ–¼ç›¸é—œåº¦ {background_info.get('relevance_score', 0):.2f} çš„èƒŒæ™¯ä¿¡æ¯åˆ†æ")
        
        # æ·»åŠ å°ˆå®¶åˆ†æ
        expert_aggregation = stage_results.get('expert_aggregation', {})
        if expert_aggregation.get('expert_count', 0) > 0:
            final_answer_parts.append(f"**å°ˆå®¶åˆ†æ**: ç¶œåˆäº† {expert_aggregation.get('expert_count')} ä½å°ˆå®¶çš„å°ˆæ¥­æ„è¦‹")
            
            # æ·»åŠ å…±è­˜å»ºè­°
            consensus_recommendations = expert_aggregation.get('consensus_recommendations', [])
            if consensus_recommendations:
                final_answer_parts.append(f"**å°ˆå®¶å»ºè­°**:\n" + "\n".join([f"â€¢ {rec}" for rec in consensus_recommendations[:5]]))
        
        # æ·»åŠ å·¥å…·åŸ·è¡Œçµæœ
        successful_tools = [result for result in tool_results if result.get("success")]
        if successful_tools:
            final_answer_parts.append(f"**è™•ç†çµæœ**: æˆåŠŸåŸ·è¡Œäº† {len(successful_tools)} å€‹è™•ç†å·¥å…·")
            
            for tool_result in successful_tools[:3]:  # åªé¡¯ç¤ºå‰3å€‹çµæœ
                tool_name = tool_result.get("tool_name", "unknown")
                output = tool_result.get("output", "")
                if isinstance(output, dict):
                    summary = output.get("processing_result", {}).get("message", "è™•ç†å®Œæˆ")
                else:
                    summary = str(output)[:100] + "..." if len(str(output)) > 100 else str(output)
                final_answer_parts.append(f"**{tool_name}**: {summary}")
        
        # ç”Ÿæˆæœ€çµ‚å›ç­”
        final_answer = "\n\n".join(final_answer_parts) if final_answer_parts else "è™•ç†å®Œæˆï¼Œä½†æœªç”Ÿæˆå…·é«”çµæœã€‚"
        
        # è¨ˆç®—ç¸½é«”ä¿¡å¿ƒåº¦
        expert_confidence = expert_aggregation.get('confidence_score', 0.0)
        tool_success_rate = len(successful_tools) / len(tool_results) if tool_results else 1.0
        total_confidence = (expert_confidence * 0.7 + tool_success_rate * 0.3)
        
        stage_time = time.time() - stage_start
        self._update_stage_stats(ProcessingStage.FINAL_RESULT_GENERATION, stage_time)
        
        return ProcessingResult(
            request_id=request.id,
            success=True,
            stage_results=stage_results,
            expert_analysis=expert_responses,
            tool_execution_results=tool_results,
            final_answer=final_answer,
            confidence=round(total_confidence, 2),
            execution_time=execution_time,
            metadata={
                "processing_stages": len(stage_results),
                "experts_consulted": [resp.expert_type.value for resp in expert_responses],
                "tools_executed": [result["tool_name"] for result in tool_results],
                "stage_breakdown": {stage: result.get("stage_execution_time", 0.0) for stage, result in stage_results.items()},
                "quality_metrics": {
                    "expert_confidence": expert_confidence,
                    "tool_success_rate": tool_success_rate,
                    "overall_confidence": total_confidence
                }
            }
        )
    
    def _update_stage_stats(self, stage: ProcessingStage, execution_time: float):
        """æ›´æ–°éšæ®µçµ±è¨ˆ"""
        stage_stats = self.execution_stats['stage_performance'][stage.value]
        stage_stats['count'] += 1
        
        # æ›´æ–°å¹³å‡æ™‚é–“
        current_avg = stage_stats['avg_time']
        count = stage_stats['count']
        stage_stats['avg_time'] = (current_avg * (count - 1) + execution_time) / count
    
    def _update_execution_stats(self, result: ProcessingResult, stage_results: Dict):
        """æ›´æ–°åŸ·è¡Œçµ±è¨ˆ"""
        self.execution_stats['total_requests'] += 1
        
        if result.success:
            self.execution_stats['successful_requests'] += 1
        else:
            self.execution_stats['failed_requests'] += 1
        
        # æ›´æ–°å¹³å‡åŸ·è¡Œæ™‚é–“
        current_avg = self.execution_stats['average_execution_time']
        total_requests = self.execution_stats['total_requests']
        self.execution_stats['average_execution_time'] = (
            (current_avg * (total_requests - 1) + result.execution_time) / total_requests
        )
        
        # æ›´æ–°ä¿¡å¿ƒåº¦åˆ†å¸ƒ
        confidence = result.confidence
        if confidence >= 0.8:
            self.execution_stats['confidence_distribution']['high'] += 1
        elif confidence >= 0.6:
            self.execution_stats['confidence_distribution']['medium'] += 1
        else:
            self.execution_stats['confidence_distribution']['low'] += 1
    
    async def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        return {
            "system_info": {
                "name": "AICore 2.0",
                "version": "2.0.0",
                "description": "å…­éšæ®µè™•ç†æµç¨‹èˆ‡å°ˆå®¶ç³»çµ±æ•´åˆ",
                "uptime": "running"
            },
            "execution_statistics": self.execution_stats,
            "component_status": {
                "general_processor_mcp": await self.general_processor.health_check(),
                "tool_registry": "healthy",
                "action_executor": "healthy",
                "mcp_client": "healthy"
            },
            "performance_metrics": {
                "success_rate": self.execution_stats['successful_requests'] / max(self.execution_stats['total_requests'], 1),
                "average_execution_time": self.execution_stats['average_execution_time'],
                "stage_performance": self.execution_stats['stage_performance']
            }
        }
    
    async def get_capabilities(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±èƒ½åŠ›"""
        general_processor_capabilities = await self.general_processor.get_capabilities()
        
        return {
            "processing_stages": [stage.value for stage in ProcessingStage],
            "expert_types": [expert.value for expert in ExpertType],
            "core_capabilities": {
                "six_stage_processing": "å…­éšæ®µæ™ºèƒ½è™•ç†æµç¨‹",
                "expert_system_integration": "å¤šé ˜åŸŸå°ˆå®¶ç³»çµ±æ•´åˆ",
                "intelligent_tool_selection": "æ™ºèƒ½å·¥å…·é¸æ“‡å’ŒåŸ·è¡Œ",
                "parallel_processing": "ä¸¦è¡Œå°ˆå®¶èª¿ç”¨å’Œå·¥å…·åŸ·è¡Œ",
                "adaptive_confidence": "è‡ªé©æ‡‰ä¿¡å¿ƒåº¦è©•ä¼°"
            },
            "integrated_components": {
                "general_processor_mcp": general_processor_capabilities,
                "tool_registry": "å·¥å…·è¨»å†Šå’Œç™¼ç¾ç³»çµ±",
                "action_executor": "å‹•ä½œåŸ·è¡Œå¼•æ“",
                "mcp_client": "MCPæœå‹™å®¢æˆ¶ç«¯"
            }
        }

# å·¥å» å‡½æ•¸
def create_aicore2(config: Dict[str, Any] = None) -> AICore2:
    """å‰µå»ºAICore 2.0å¯¦ä¾‹"""
    return AICore2(config)

# ç¤ºä¾‹ä½¿ç”¨
async def example_usage():
    """ç¤ºä¾‹ç”¨æ³•"""
    # å‰µå»ºAICore 2.0å¯¦ä¾‹
    aicore = create_aicore2()
    await aicore.initialize()
    
    # å‰µå»ºç”¨æˆ¶è«‹æ±‚
    request = UserRequest(
        id="req_001",
        content="æˆ‘éœ€è¦åˆ†æAPIæ¥å£çš„æ€§èƒ½å•é¡Œï¼Œä¸¦æä¾›å„ªåŒ–å»ºè­°",
        context={
            "api_endpoint": "/api/users",
            "current_response_time": "2.5s",
            "expected_response_time": "500ms",
            "user_count": 1000
        }
    )
    
    print("ğŸš€ é–‹å§‹è™•ç†ç”¨æˆ¶è«‹æ±‚...")
    
    # è™•ç†è«‹æ±‚
    result = await aicore.process_request(request)
    
    # è¼¸å‡ºçµæœ
    print(f"\nğŸ“Š è™•ç†çµæœ:")
    print(f"æˆåŠŸ: {result.success}")
    print(f"åŸ·è¡Œæ™‚é–“: {result.execution_time:.2f}s")
    print(f"ä¿¡å¿ƒåº¦: {result.confidence:.2f}")
    print(f"å°ˆå®¶æ•¸é‡: {len(result.expert_analysis)}")
    print(f"å·¥å…·åŸ·è¡Œ: {len(result.tool_execution_results)}")
    print(f"\næœ€çµ‚å›ç­”:\n{result.final_answer}")
    
    # ç²å–ç³»çµ±ç‹€æ…‹
    status = await aicore.get_system_status()
    print(f"\nğŸ“ˆ ç³»çµ±çµ±è¨ˆ:")
    print(f"ç¸½è«‹æ±‚æ•¸: {status['execution_statistics']['total_requests']}")
    print(f"æˆåŠŸç‡: {status['performance_metrics']['success_rate']:.2%}")

if __name__ == "__main__":
    asyncio.run(example_usage())

