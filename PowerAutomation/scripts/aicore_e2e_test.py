"""
AICore æ™ºæ…§è·¯ç”±å„ªåŒ– - ç«¯åˆ°ç«¯æ•´åˆæ¸¬è©¦
æ¸¬è©¦å®Œæ•´çš„å°ˆå®¶å»ºè­° â†’ æ™ºæ…§è·¯ç”± â†’ MCP åŸ·è¡Œæµç¨‹
"""

import asyncio
import logging
import time
import json
import sys
import os
from typing import Dict, List, Any, Optional

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append('/home/ubuntu/aicore0624/PowerAutomation')

# å°å…¥æ ¸å¿ƒçµ„ä»¶
from core.enhanced_aicore3 import EnhancedAICore3
from components.dynamic_expert_registry import create_dynamic_expert_registry
from components.smart_routing_engine import SmartRoutingEngine, ToolEndpoint, ToolHealth, LoadMetrics
from components.expert_routing_integrator import create_expert_routing_integrator
from components.testing_expert_config import TESTING_EXPERT_CONFIG

logger = logging.getLogger(__name__)

class AICorE2ETestSuite:
    """AICore ç«¯åˆ°ç«¯æ¸¬è©¦å¥—ä»¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¸¬è©¦å¥—ä»¶"""
        self.test_results = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "test_details": []
        }
        
        # æ ¸å¿ƒçµ„ä»¶
        self.enhanced_aicore = None
        self.expert_registry = None
        self.smart_routing_engine = None
        self.expert_routing_integrator = None
        
        logger.info("âœ… AICore ç«¯åˆ°ç«¯æ¸¬è©¦å¥—ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    async def setup_test_environment(self):
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        
        logger.info("ğŸ”§ è¨­ç½®æ¸¬è©¦ç’°å¢ƒ...")
        
        try:
            # 1. å‰µå»ºå°ˆå®¶è¨»å†Šè¡¨
            self.expert_registry = create_dynamic_expert_registry()
            await self.expert_registry.initialize()
            
            # 2. è¨»å†Šæ¸¬è©¦å°ˆå®¶
            await self.expert_registry.register_expert_directly(TESTING_EXPERT_CONFIG)
            
            # 3. å‰µå»ºæ™ºæ…§è·¯ç”±å¼•æ“
            routing_config = {
                'default_strategy': 'intelligent',
                'load_threshold': 0.8,
                'latency_threshold': 1000,
                'error_rate_threshold': 0.1,
                'failover_enabled': True
            }
            self.smart_routing_engine = SmartRoutingEngine(routing_config)
            
            # 4. è¨»å†Š MCP ç«¯é»
            await self._register_mcp_endpoints()
            
            # 5. å‰µå»ºå°ˆå®¶å»ºè­°è·¯ç”±æ•´åˆå™¨
            self.expert_routing_integrator = create_expert_routing_integrator(
                self.smart_routing_engine, self.expert_registry
            )
            
            # 6. å‰µå»ºå¢å¼·çš„ AICore (æš«æ™‚è·³éï¼Œå› ç‚ºæœ‰åˆå§‹åŒ–å•é¡Œ)
            # self.enhanced_aicore = EnhancedAICore3()
            # await self.enhanced_aicore.initialize()
            
            logger.info("âœ… æ¸¬è©¦ç’°å¢ƒè¨­ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦ç’°å¢ƒè¨­ç½®å¤±æ•—: {e}")
            return False
    
    async def _register_mcp_endpoints(self):
        """è¨»å†Š MCP ç«¯é»"""
        
        mcp_endpoints = [
            ("test_flow_mcp", ToolEndpoint(
                tool_id="test_flow_mcp",
                endpoint_url="http://localhost:8095",
                health=ToolHealth.HEALTHY,
                capabilities=["testing", "unit_testing", "integration_testing", "test_automation", "qa"],
                load_metrics=LoadMetrics(cpu_usage=0.2, memory_usage=0.3, active_requests=5, response_time_avg=0.8)
            )),
            ("smartinvention_adapter_mcp", ToolEndpoint(
                tool_id="smartinvention_adapter_mcp",
                endpoint_url="http://localhost:8000",
                health=ToolHealth.HEALTHY,
                capabilities=["data_retrieval", "task_analysis", "file_management", "cross_task_analysis"],
                load_metrics=LoadMetrics(cpu_usage=0.15, memory_usage=0.25, active_requests=3, response_time_avg=0.6)
            )),
            ("manus_adapter_mcp", ToolEndpoint(
                tool_id="manus_adapter_mcp",
                endpoint_url="http://localhost:8001",
                health=ToolHealth.HEALTHY,
                capabilities=["requirement_analysis", "manus_comparison", "expert_coordination"],
                load_metrics=LoadMetrics(cpu_usage=0.25, memory_usage=0.35, active_requests=4, response_time_avg=1.0)
            ))
        ]
        
        for tool_id, endpoint in mcp_endpoints:
            self.smart_routing_engine.register_tool_endpoint(tool_id, endpoint)
            logger.info(f"è¨»å†Š MCP ç«¯é»: {tool_id}")
    
    async def test_testing_expert_recommendation(self):
        """æ¸¬è©¦ 1: æ¸¬è©¦å°ˆå®¶æ¨è–¦ Test Flow MCP"""
        
        test_name = "æ¸¬è©¦å°ˆå®¶æ¨è–¦ Test Flow MCP"
        self.test_results["total_tests"] += 1
        
        logger.info(f"ğŸ§ª åŸ·è¡Œæ¸¬è©¦: {test_name}")
        
        try:
            # æ¨¡æ“¬æ¸¬è©¦ç›¸é—œè«‹æ±‚
            test_requests = [
                "åŸ·è¡Œå–®å…ƒæ¸¬è©¦ä¾†é©—è­‰ä»£ç¢¼åŠŸèƒ½",
                "éœ€è¦é€²è¡Œé›†æˆæ¸¬è©¦",
                "é‹è¡Œè‡ªå‹•åŒ–æ¸¬è©¦å¥—ä»¶",
                "åŸ·è¡Œ QA æ¸¬è©¦æµç¨‹",
                "é©—è­‰ç³»çµ±åŠŸèƒ½æ˜¯å¦æ­£å¸¸"
            ]
            
            success_count = 0
            total_requests = len(test_requests)
            
            for i, request_content in enumerate(test_requests):
                logger.info(f"  æ¸¬è©¦è«‹æ±‚ {i+1}/{total_requests}: {request_content}")
                
                # ç²å–å°ˆå®¶å»ºè­°
                expert_responses = await self.expert_registry.get_expert_recommendations(
                    request_content, {"type": "testing", "priority": "high"}
                )
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æ¸¬è©¦å°ˆå®¶éŸ¿æ‡‰
                testing_expert_found = False
                test_flow_recommended = False
                
                for response in expert_responses:
                    if response.expert_id == "testing_expert":
                        testing_expert_found = True
                        
                        # æª¢æŸ¥æ˜¯å¦æ¨è–¦äº† Test Flow MCP
                        for tool_suggestion in response.tool_suggestions:
                            if tool_suggestion.get("tool_name") == "test_flow_mcp":
                                test_flow_recommended = True
                                break
                        break
                
                if testing_expert_found and test_flow_recommended:
                    success_count += 1
                    logger.info(f"    âœ… æ¸¬è©¦å°ˆå®¶æ­£ç¢ºæ¨è–¦ Test Flow MCP")
                else:
                    logger.warning(f"    âŒ æ¸¬è©¦å°ˆå®¶æœªæ¨è–¦ Test Flow MCP")
            
            # è¨ˆç®—æˆåŠŸç‡
            success_rate = success_count / total_requests
            
            if success_rate >= 0.8:  # 80% æˆåŠŸç‡é–¾å€¼
                self.test_results["passed_tests"] += 1
                test_result = "PASSED"
                logger.info(f"âœ… æ¸¬è©¦é€šé: {test_name} (æˆåŠŸç‡: {success_rate:.1%})")
            else:
                self.test_results["failed_tests"] += 1
                test_result = "FAILED"
                logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {test_name} (æˆåŠŸç‡: {success_rate:.1%})")
            
            self.test_results["test_details"].append({
                "test_name": test_name,
                "result": test_result,
                "success_rate": success_rate,
                "details": f"æˆåŠŸæ¨è–¦: {success_count}/{total_requests}"
            })
            
            return test_result == "PASSED"
            
        except Exception as e:
            self.test_results["failed_tests"] += 1
            logger.error(f"âŒ æ¸¬è©¦ç•°å¸¸: {test_name} - {e}")
            
            self.test_results["test_details"].append({
                "test_name": test_name,
                "result": "ERROR",
                "error": str(e)
            })
            
            return False
    
    async def test_smart_routing_decision(self):
        """æ¸¬è©¦ 2: æ™ºæ…§è·¯ç”±å¼•æ“æ±ºç­–"""
        
        test_name = "æ™ºæ…§è·¯ç”±å¼•æ“æ±ºç­–"
        self.test_results["total_tests"] += 1
        
        logger.info(f"ğŸ§ª åŸ·è¡Œæ¸¬è©¦: {test_name}")
        
        try:
            # æ¨¡æ“¬å°ˆå®¶éŸ¿æ‡‰
            from core.aicore3 import ExpertResponse
            
            mock_expert_responses = [
                ExpertResponse(
                    expert_id="testing_expert",
                    expert_name="Testing Expert",
                    expert_type="testing",
                    analysis="éœ€è¦åŸ·è¡Œæ¸¬è©¦",
                    recommendations=["ä½¿ç”¨ Test Flow MCP"],
                    tool_suggestions=[{
                        "tool_name": "test_flow_mcp",
                        "confidence": 0.95,
                        "reasoning": "æ¸¬è©¦å°ˆå®¶æ¨è–¦"
                    }],
                    confidence=0.9
                )
            ]
            
            # æ¸¬è©¦ä¸åŒé¡å‹çš„è«‹æ±‚
            test_scenarios = [
                {"capability": "testing", "expected_tool": "test_flow_mcp"},
                {"capability": "data_retrieval", "expected_tool": "smartinvention_adapter_mcp"},
                {"capability": "requirement_analysis", "expected_tool": "manus_adapter_mcp"}
            ]
            
            success_count = 0
            total_scenarios = len(test_scenarios)
            
            for i, scenario in enumerate(test_scenarios):
                capability = scenario["capability"]
                expected_tool = scenario["expected_tool"]
                
                logger.info(f"  æ¸¬è©¦å ´æ™¯ {i+1}/{total_scenarios}: {capability}")
                
                # åŸ·è¡Œè·¯ç”±æ±ºç­–
                routing_decision = await self.expert_routing_integrator.route_with_expert_recommendations(
                    mock_expert_responses if capability == "testing" else [],
                    {"type": capability},
                    capability
                )
                
                # æª¢æŸ¥è·¯ç”±çµæœ
                if routing_decision.target_tool == expected_tool:
                    success_count += 1
                    logger.info(f"    âœ… æ­£ç¢ºè·¯ç”±åˆ°: {routing_decision.target_tool}")
                else:
                    logger.warning(f"    âŒ è·¯ç”±éŒ¯èª¤: æœŸæœ› {expected_tool}, å¯¦éš› {routing_decision.target_tool}")
            
            # è¨ˆç®—æˆåŠŸç‡
            success_rate = success_count / total_scenarios
            
            if success_rate >= 0.8:
                self.test_results["passed_tests"] += 1
                test_result = "PASSED"
                logger.info(f"âœ… æ¸¬è©¦é€šé: {test_name} (æˆåŠŸç‡: {success_rate:.1%})")
            else:
                self.test_results["failed_tests"] += 1
                test_result = "FAILED"
                logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {test_name} (æˆåŠŸç‡: {success_rate:.1%})")
            
            self.test_results["test_details"].append({
                "test_name": test_name,
                "result": test_result,
                "success_rate": success_rate,
                "details": f"æ­£ç¢ºè·¯ç”±: {success_count}/{total_scenarios}"
            })
            
            return test_result == "PASSED"
            
        except Exception as e:
            self.test_results["failed_tests"] += 1
            logger.error(f"âŒ æ¸¬è©¦ç•°å¸¸: {test_name} - {e}")
            
            self.test_results["test_details"].append({
                "test_name": test_name,
                "result": "ERROR",
                "error": str(e)
            })
            
            return False
    
    async def test_smartinvention_integration(self):
        """æ¸¬è©¦ 3: Smartinvention MCP æ•´åˆ"""
        
        test_name = "Smartinvention MCP æ•´åˆ"
        self.test_results["total_tests"] += 1
        
        logger.info(f"ğŸ§ª åŸ·è¡Œæ¸¬è©¦: {test_name}")
        
        try:
            # æ¨¡æ“¬éœ€è¦ Smartinvention æ•¸æ“šçš„è«‹æ±‚
            test_requests = [
                "åˆ†æè·¨ä»»å‹™é—œè¯æ€§",
                "ç²å–ç›¸é—œä»»å‹™æ•¸æ“š",
                "æª¢ç´¢æª”æ¡ˆä¿¡æ¯",
                "åŸ·è¡Œæ•¸æ“šåŒæ­¥"
            ]
            
            success_count = 0
            total_requests = len(test_requests)
            
            for i, request_content in enumerate(test_requests):
                logger.info(f"  æ¸¬è©¦è«‹æ±‚ {i+1}/{total_requests}: {request_content}")
                
                # æª¢æŸ¥ Smartinvention MCP æ˜¯å¦å¯ç”¨
                smartinvention_endpoints = self.smart_routing_engine.tool_endpoints.get("smartinvention_adapter_mcp", [])
                
                if smartinvention_endpoints:
                    endpoint = smartinvention_endpoints[0]
                    if endpoint.health == ToolHealth.HEALTHY:
                        success_count += 1
                        logger.info(f"    âœ… Smartinvention MCP å¯ç”¨: {endpoint.endpoint_url}")
                    else:
                        logger.warning(f"    âŒ Smartinvention MCP ä¸å¥åº·: {endpoint.health}")
                else:
                    logger.warning(f"    âŒ Smartinvention MCP æœªè¨»å†Š")
            
            # è¨ˆç®—æˆåŠŸç‡
            success_rate = success_count / total_requests
            
            if success_rate >= 0.8:
                self.test_results["passed_tests"] += 1
                test_result = "PASSED"
                logger.info(f"âœ… æ¸¬è©¦é€šé: {test_name} (æˆåŠŸç‡: {success_rate:.1%})")
            else:
                self.test_results["failed_tests"] += 1
                test_result = "FAILED"
                logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {test_name} (æˆåŠŸç‡: {success_rate:.1%})")
            
            self.test_results["test_details"].append({
                "test_name": test_name,
                "result": test_result,
                "success_rate": success_rate,
                "details": f"å¯ç”¨æª¢æŸ¥: {success_count}/{total_requests}"
            })
            
            return test_result == "PASSED"
            
        except Exception as e:
            self.test_results["failed_tests"] += 1
            logger.error(f"âŒ æ¸¬è©¦ç•°å¸¸: {test_name} - {e}")
            
            self.test_results["test_details"].append({
                "test_name": test_name,
                "result": "ERROR",
                "error": str(e)
            })
            
            return False
    
    async def test_performance_stability(self):
        """æ¸¬è©¦ 4: æ€§èƒ½å’Œç©©å®šæ€§"""
        
        test_name = "æ€§èƒ½å’Œç©©å®šæ€§"
        self.test_results["total_tests"] += 1
        
        logger.info(f"ğŸ§ª åŸ·è¡Œæ¸¬è©¦: {test_name}")
        
        try:
            # æ€§èƒ½æ¸¬è©¦åƒæ•¸
            num_requests = 10
            max_response_time = 2.0  # 2ç§’
            
            response_times = []
            success_count = 0
            
            for i in range(num_requests):
                start_time = time.time()
                
                try:
                    # åŸ·è¡Œå°ˆå®¶å»ºè­°ç²å–
                    expert_responses = await self.expert_registry.get_expert_recommendations(
                        f"æ¸¬è©¦è«‹æ±‚ {i+1}", {"type": "testing"}
                    )
                    
                    # åŸ·è¡Œè·¯ç”±æ±ºç­–
                    if expert_responses:
                        routing_decision = await self.expert_routing_integrator.route_with_expert_recommendations(
                            expert_responses, {"type": "testing"}, "testing"
                        )
                        
                        if routing_decision:
                            success_count += 1
                    
                    response_time = time.time() - start_time
                    response_times.append(response_time)
                    
                    logger.info(f"  è«‹æ±‚ {i+1}/{num_requests}: {response_time:.3f}s")
                    
                except Exception as e:
                    logger.warning(f"  è«‹æ±‚ {i+1} å¤±æ•—: {e}")
                    response_times.append(max_response_time + 1)  # æ¨™è¨˜ç‚ºè¶…æ™‚
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            avg_response_time = sum(response_times) / len(response_times)
            max_response_time_actual = max(response_times)
            success_rate = success_count / num_requests
            
            # æª¢æŸ¥æ€§èƒ½æ¨™æº–
            performance_ok = avg_response_time <= max_response_time and success_rate >= 0.9
            
            if performance_ok:
                self.test_results["passed_tests"] += 1
                test_result = "PASSED"
                logger.info(f"âœ… æ¸¬è©¦é€šé: {test_name}")
            else:
                self.test_results["failed_tests"] += 1
                test_result = "FAILED"
                logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {test_name}")
            
            self.test_results["test_details"].append({
                "test_name": test_name,
                "result": test_result,
                "avg_response_time": avg_response_time,
                "max_response_time": max_response_time_actual,
                "success_rate": success_rate,
                "details": f"å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_response_time:.3f}s, æˆåŠŸç‡: {success_rate:.1%}"
            })
            
            return test_result == "PASSED"
            
        except Exception as e:
            self.test_results["failed_tests"] += 1
            logger.error(f"âŒ æ¸¬è©¦ç•°å¸¸: {test_name} - {e}")
            
            self.test_results["test_details"].append({
                "test_name": test_name,
                "result": "ERROR",
                "error": str(e)
            })
            
            return False
    
    async def run_all_tests(self):
        """é‹è¡Œæ‰€æœ‰æ¸¬è©¦"""
        
        logger.info("ğŸš€ é–‹å§‹ AICore ç«¯åˆ°ç«¯æ•´åˆæ¸¬è©¦")
        
        # è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
        if not await self.setup_test_environment():
            logger.error("âŒ æ¸¬è©¦ç’°å¢ƒè¨­ç½®å¤±æ•—ï¼Œçµ‚æ­¢æ¸¬è©¦")
            return False
        
        # åŸ·è¡Œæ¸¬è©¦
        tests = [
            self.test_testing_expert_recommendation,
            self.test_smart_routing_decision,
            self.test_smartinvention_integration,
            self.test_performance_stability
        ]
        
        for test_func in tests:
            await test_func()
            await asyncio.sleep(0.5)  # æ¸¬è©¦é–“éš”
        
        # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
        await self.generate_test_report()
        
        # è¿”å›ç¸½é«”çµæœ
        return self.test_results["failed_tests"] == 0
    
    async def generate_test_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        
        logger.info("ğŸ“Š ç”Ÿæˆæ¸¬è©¦å ±å‘Š...")
        
        # è¨ˆç®—çµ±è¨ˆ
        total_tests = self.test_results["total_tests"]
        passed_tests = self.test_results["passed_tests"]
        failed_tests = self.test_results["failed_tests"]
        pass_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        # ç”Ÿæˆå ±å‘Š
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "pass_rate": pass_rate,
                "overall_result": "PASSED" if failed_tests == 0 else "FAILED"
            },
            "test_details": self.test_results["test_details"],
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_environment": {
                "expert_count": len(self.expert_registry.experts) if self.expert_registry else 0,
                "mcp_endpoints": len(self.smart_routing_engine.tool_endpoints) if self.smart_routing_engine else 0
            }
        }
        
        # ä¿å­˜å ±å‘Š
        report_file = f"/home/ubuntu/aicore_e2e_test_report_{int(time.time())}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°ç¸½çµ
        print(f"\nğŸ“Š AICore ç«¯åˆ°ç«¯æ¸¬è©¦ç¸½çµ:")
        print(f"   ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print(f"   é€šéæ¸¬è©¦: {passed_tests}")
        print(f"   å¤±æ•—æ¸¬è©¦: {failed_tests}")
        print(f"   é€šéç‡: {pass_rate:.1%}")
        print(f"   ç¸½é«”çµæœ: {'âœ… é€šé' if failed_tests == 0 else 'âŒ å¤±æ•—'}")
        print(f"   æ¸¬è©¦å ±å‘Š: {report_file}")
        
        for detail in self.test_results["test_details"]:
            result_icon = "âœ…" if detail["result"] == "PASSED" else "âŒ"
            print(f"   {result_icon} {detail['test_name']}: {detail['result']}")
        
        logger.info(f"âœ… æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {report_file}")

async def main():
    """ä¸»å‡½æ•¸"""
    
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # å‰µå»ºæ¸¬è©¦å¥—ä»¶
    test_suite = AICorE2ETestSuite()
    
    # é‹è¡Œæ¸¬è©¦
    success = await test_suite.run_all_tests()
    
    # è¿”å›çµæœ
    if success:
        print(f"\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼AICore æ™ºæ…§è·¯ç”±å„ªåŒ–åŠŸèƒ½æ­£å¸¸")
        return 0
    else:
        print(f"\nâš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿è©¦")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())

