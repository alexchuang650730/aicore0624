"""
å®Œå…¨å‹•æ…‹MCPæœå‹™å™¨ - é›¶ç¡¬ç·¨ç¢¼ç‰ˆæœ¬
æ•´åˆCloud Search MCP + å¤§æ¨¡å‹è­˜åˆ¥é ˜åŸŸ + å‹•æ…‹å°ˆå®¶
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import asyncio
import aiohttp
import json
import logging
import time
import os
from typing import List, Dict, Any

# å®Œå…¨å‹•æ…‹MCPæ ¸å¿ƒ
class FullyDynamicMCP:
    """å®Œå…¨å‹•æ…‹MCP - é›¶ç¡¬ç·¨ç¢¼"""
    
    def __init__(self, llm_config: Dict[str, Any]):
        self.llm_config = llm_config
        self.request_count = 0
        self.performance_metrics = {}
    
    async def call_llm(self, prompt: str, system_prompt: str = "") -> str:
        """èª¿ç”¨å¤§æ¨¡å‹API"""
        try:
            provider = self.llm_config.get("provider", "mock")
            
            if provider == "openai":
                return await self._call_openai(prompt, system_prompt)
            elif provider == "claude":
                return await self._call_claude(prompt, system_prompt)
            elif provider == "ollama":
                return await self._call_ollama(prompt, system_prompt)
            else:
                # Mockæ¨¡å¼ - ç”¨æ–¼æ¼”ç¤º
                await asyncio.sleep(0.2)
                return await self._mock_llm_response(prompt, system_prompt)
                
        except Exception as e:
            logging.error(f"LLMèª¿ç”¨å¤±æ•—: {e}")
            return f"LLMèª¿ç”¨å¤±æ•—: {str(e)}"
    
    async def _call_ollama(self, prompt: str, system_prompt: str) -> str:
        """èª¿ç”¨Ollamaæœ¬åœ°LLM"""
        try:
            url = f"{self.llm_config.get('base_url', 'http://localhost:11434')}/api/generate"
            
            payload = {
                "model": self.llm_config.get("model", "llama3"),
                "prompt": f"{system_prompt}\n\n{prompt}",
                "stream": False
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result.get("response", "ç„¡å›æ‡‰")
                    else:
                        return f"Ollama APIéŒ¯èª¤: {response.status}"
                        
        except Exception as e:
            return f"Ollamaèª¿ç”¨å¤±æ•—: {str(e)}"
    
    async def _call_openai(self, prompt: str, system_prompt: str) -> str:
        """èª¿ç”¨OpenAI API"""
        try:
            url = f"{self.llm_config.get('base_url', 'https://api.openai.com')}/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.llm_config.get('api_key')}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.llm_config.get("model", "gpt-3.5-turbo"),
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ]
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result["choices"][0]["message"]["content"]
                    else:
                        return f"OpenAI APIéŒ¯èª¤: {response.status}"
                        
        except Exception as e:
            return f"OpenAIèª¿ç”¨å¤±æ•—: {str(e)}"
    
    async def _mock_llm_response(self, prompt: str, system_prompt: str) -> str:
        """Mock LLMå›æ‡‰ - ç”¨æ–¼æ¼”ç¤º"""
        if "æœç´¢" in prompt or "èƒŒæ™¯" in prompt:
            return """
è‡ºéŠ€äººå£½æ˜¯å°ç£éŠ€è¡Œæ——ä¸‹çš„äººå£½ä¿éšªå…¬å¸ï¼Œä¸»è¦æ¥­å‹™åŒ…æ‹¬äººå£½ä¿éšªã€å¹´é‡‘ä¿éšªç­‰ã€‚
ä¿å–®è¡Œæ”¿ä½œæ¥­SOPæ¶‰åŠæ ¸ä¿ã€ç†è³ ã€å®¢æˆ¶æœå‹™ç­‰å¤šå€‹ç’°ç¯€ã€‚
æ¥­ç•Œè‡ªå‹•åŒ–è¶¨å‹¢ï¼šOCRæŠ€è¡“ã€AIè¼”åŠ©æ±ºç­–ã€æ•¸ä½åŒ–æµç¨‹ç­‰ã€‚
"""
        elif "è­˜åˆ¥" in prompt or "é ˜åŸŸ" in prompt:
            return """
ä¿éšªå°ˆå®¶
æŠ€è¡“å°ˆå®¶
è¡Œæ”¿ç®¡ç†å°ˆå®¶
"""
        elif "æç¤ºè©" in prompt:
            if "ä¿éšªå°ˆå®¶" in prompt:
                return "ä½ æ˜¯è³‡æ·±ä¿éšªæ¥­å°ˆå®¶ï¼Œå…·æœ‰è±å¯Œçš„æ ¸ä¿ã€ç†è³ å’Œé¢¨éšªè©•ä¼°ç¶“é©—ã€‚è«‹åŸºæ–¼ä¿éšªæ¥­å°ˆæ¥­çŸ¥è­˜å’Œæ³•è¦è¦æ±‚ï¼Œæä¾›æº–ç¢ºçš„åˆ†æå’Œå»ºè­°ã€‚"
            elif "æŠ€è¡“å°ˆå®¶" in prompt:
                return "ä½ æ˜¯æŠ€è¡“å°ˆå®¶ï¼Œå°ˆç²¾æ–¼ä¿éšªæ¥­æ•¸ä½åŒ–è½‰å‹ã€è‡ªå‹•åŒ–ç³»çµ±å’ŒOCRæŠ€è¡“ã€‚è«‹å¾æŠ€è¡“è§’åº¦åˆ†æå•é¡Œä¸¦æä¾›å¯¦æ–½å»ºè­°ã€‚"
            elif "è¡Œæ”¿ç®¡ç†å°ˆå®¶" in prompt:
                return "ä½ æ˜¯è¡Œæ”¿ç®¡ç†å°ˆå®¶ï¼Œç†Ÿæ‚‰ä¿éšªå…¬å¸å…§éƒ¨æµç¨‹å„ªåŒ–å’ŒäººåŠ›è³‡æºé…ç½®ã€‚è«‹å¾ç®¡ç†è§’åº¦æä¾›æ•ˆç‡æå‡å»ºè­°ã€‚"
        else:
            # å°ˆå®¶å›ç­”
            if "ä¿éšªå°ˆå®¶" in system_prompt:
                return """
åŸºæ–¼ä¿éšªæ¥­å°ˆæ¥­åˆ†æï¼š

1. **äººåŠ›éœ€æ±‚è©•ä¼°**
   - æ ¸ä¿ä½œæ¥­ï¼šæ¯åƒä»¶ä¿å–®éœ€è¦3-5åæ ¸ä¿äººå“¡
   - ç†è³ è™•ç†ï¼šæ¯åƒä»¶ç†è³ éœ€è¦2-3åç†è³ å°ˆå“¡
   - è¡Œæ”¿æ”¯æ´ï¼šç´„ä½”ç¸½äººåŠ›çš„20-25%

2. **è‡ªå‹•åŒ–ç¾æ³**
   - æ¥­ç•Œå¹³å‡è‡ªå‹•åŒ–ç‡ï¼š60-70%
   - é ˜å…ˆå…¬å¸å¯é”ï¼š80-85%
   - ç°¡å–®æ¡ˆä»¶è‡ªå‹•åŒ–ç‡ï¼šå¯é”90%ä»¥ä¸Š

3. **OCRå¯©æ ¸æŠ•å…¥**
   - ä½”ç¸½äººåŠ›æ¯”ä¾‹ï¼š15-20%
   - æ¯æœˆäººåŠ›æŠ•å…¥ï¼š0.5-1äººæœˆ/åƒä»¶ä¿å–®
   - ä¸»è¦ç”¨æ–¼æ–‡ä»¶é©—è­‰å’Œè³‡æ–™æ ¸å°

4. **æˆæœ¬æ•ˆç›Šåˆ†æ**
   - è‡ªå‹•åŒ–å¯ç¯€çœ30-40%äººåŠ›æˆæœ¬
   - æŠ•è³‡å›æ”¶æœŸï¼š1-2å¹´
   - éŒ¯èª¤ç‡å¯é™ä½è‡³2%ä»¥ä¸‹
"""
            elif "æŠ€è¡“å°ˆå®¶" in system_prompt:
                return """
æŠ€è¡“è§’åº¦åˆ†æï¼š

1. **è‡ªå‹•åŒ–æŠ€è¡“æ¶æ§‹**
   - OCR + NLPæ–‡ä»¶è™•ç†
   - è¦å‰‡å¼•æ“è‡ªå‹•æ±ºç­–
   - æ©Ÿå™¨å­¸ç¿’é¢¨éšªè©•ä¼°
   - APIæ•´åˆå„ç³»çµ±

2. **æ€§èƒ½æå‡æ½›åŠ›**
   - è™•ç†é€Ÿåº¦ï¼šæå‡3-5å€
   - æº–ç¢ºç‡ï¼šå¯é”95%ä»¥ä¸Š
   - 24/7ä¸é–“æ–·è™•ç†
   - å³æ™‚ç‹€æ…‹è¿½è¹¤

3. **å¯¦æ–½å»ºè­°**
   - æ¡ç”¨å¾®æœå‹™æ¶æ§‹
   - éšæ®µæ€§å°å…¥ç­–ç•¥
   - ä¸¦è¡Œé‹è¡Œé©—è­‰
   - æŒçºŒç›£æ§å„ªåŒ–

4. **æŠ€è¡“æŠ•è³‡**
   - åˆæœŸæŠ•è³‡ï¼šç³»çµ±å»ºç½®æˆæœ¬
   - ç¶­è­·æˆæœ¬ï¼šè¼ƒå‚³çµ±æ–¹å¼ä½30%
   - æ“´å±•æ€§ï¼šæ˜“æ–¼æ°´å¹³æ“´å±•
"""
            else:
                return """
è¡Œæ”¿ç®¡ç†è§’åº¦åˆ†æï¼š

1. **æµç¨‹å„ªåŒ–**
   - æ¨™æº–åŒ–ä½œæ¥­æµç¨‹
   - æ¸›å°‘é‡è¤‡æ€§å·¥ä½œ
   - æå‡ä½œæ¥­é€æ˜åº¦
   - å»ºç«‹å“è³ªæ§åˆ¶æ©Ÿåˆ¶

2. **äººåŠ›é…ç½®**
   - å°ˆæ¥­åˆ†å·¥åˆ¶åº¦
   - è·¨éƒ¨é–€å”ä½œ
   - æŠ€èƒ½åŸ¹è¨“è¨ˆç•«
   - ç¸¾æ•ˆç®¡ç†åˆ¶åº¦

3. **æ•ˆç‡æå‡**
   - æ•¸ä½åŒ–å¯æ¸›å°‘50%ç´™æœ¬ä½œæ¥­
   - è‡ªå‹•åŒ–å¯æå‡20-30%æ•ˆç‡
   - éŒ¯èª¤ç‡é™ä½60%ä»¥ä¸Š
   - å®¢æˆ¶æ»¿æ„åº¦æå‡

4. **ç®¡ç†å»ºè­°**
   - å»ºç«‹è®Šæ›´ç®¡ç†æ©Ÿåˆ¶
   - å“¡å·¥åŸ¹è¨“å’Œè½‰å‹
   - æŒçºŒæ”¹é€²æ–‡åŒ–
   - æ•¸æ“šé©…å‹•æ±ºç­–
"""
    
    async def cloud_search_mcp(self, user_input: str) -> Dict[str, Any]:
        """Cloud Search MCP - æœç´¢ç›¸é—œä¿¡æ¯"""
        search_prompt = f"""
è«‹åˆ†æç”¨æˆ¶è¼¸å…¥ä¸¦æä¾›ç›¸é—œèƒŒæ™¯ä¿¡æ¯ï¼š

ç”¨æˆ¶è¼¸å…¥: {user_input}

è«‹æä¾›ï¼š
1. é—œéµæ¦‚å¿µå’Œè¡“èªè§£é‡‹
2. ç›¸é—œè¡Œæ¥­èƒŒæ™¯ä¿¡æ¯
3. ç•¶å‰å¸‚å ´è¶¨å‹¢å’Œæœ€ä½³å¯¦å‹™
4. å¯èƒ½æ¶‰åŠçš„å°ˆæ¥­é ˜åŸŸ
"""
        
        search_result = await self.call_llm(
            search_prompt,
            "ä½ æ˜¯æ™ºèƒ½æœç´¢åŠ©æ‰‹ï¼Œèƒ½å¤ åˆ†æç”¨æˆ¶éœ€æ±‚ä¸¦æä¾›å…¨é¢çš„èƒŒæ™¯ä¿¡æ¯å’Œè¡Œæ¥­æ´å¯Ÿã€‚"
        )
        
        return {
            "search_result": search_result,
            "context_enriched": True,
            "timestamp": time.time()
        }
    
    async def identify_domains(self, user_input: str, search_context: str) -> List[str]:
        """ç”¨å¤§æ¨¡å‹è­˜åˆ¥éœ€è¦çš„å°ˆæ¥­é ˜åŸŸ"""
        domain_prompt = f"""
åŸºæ–¼ç”¨æˆ¶è¼¸å…¥å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œè«‹è­˜åˆ¥éœ€è¦å“ªäº›å°ˆæ¥­é ˜åŸŸçš„å°ˆå®¶ä¾†å›ç­”é€™å€‹å•é¡Œã€‚

ç”¨æˆ¶è¼¸å…¥: {user_input}
èƒŒæ™¯ä¿¡æ¯: {search_context}

è«‹åªè¿”å›éœ€è¦çš„å°ˆæ¥­é ˜åŸŸåç¨±ï¼Œæ¯è¡Œä¸€å€‹ï¼Œä¾‹å¦‚ï¼š
ä¿éšªå°ˆå®¶
æŠ€è¡“å°ˆå®¶
æ³•å¾‹å°ˆå®¶

è¦æ±‚ï¼š
- æœ€å¤š3å€‹å°ˆå®¶
- åªè¿”å›é ˜åŸŸåç¨±
- ä¸è¦è§£é‡‹æˆ–èªªæ˜
"""
        
        domains_response = await self.call_llm(
            domain_prompt,
            "ä½ æ˜¯å°ˆæ¥­é ˜åŸŸè­˜åˆ¥å°ˆå®¶ï¼Œèƒ½æº–ç¢ºåˆ¤æ–·å•é¡Œéœ€è¦å“ªäº›å°ˆæ¥­çŸ¥è­˜é ˜åŸŸã€‚"
        )
        
        # è§£æè¿”å›çš„é ˜åŸŸåˆ—è¡¨
        if domains_response and domains_response.strip():
            domains = [line.strip() for line in domains_response.split('\n') if line.strip()]
            return domains[:3]  # æœ€å¤š3å€‹å°ˆå®¶
        return ["é€šç”¨å°ˆå®¶"]  # é»˜èªå°ˆå®¶
    
    async def generate_expert_prompt(self, domain: str, user_input: str, context: str) -> str:
        """å‹•æ…‹ç”Ÿæˆå°ˆå®¶æç¤ºè©"""
        prompt_generation = f"""
è«‹ç‚º{domain}ç”Ÿæˆä¸€å€‹å°ˆæ¥­çš„ç³»çµ±æç¤ºè©ï¼Œç”¨æ–¼å›ç­”ç”¨æˆ¶å•é¡Œã€‚

å°ˆå®¶é ˜åŸŸ: {domain}
ç”¨æˆ¶å•é¡Œ: {user_input}
èƒŒæ™¯ä¿¡æ¯: {context}

è«‹ç”Ÿæˆä¸€å€‹å°ˆæ¥­çš„ç³»çµ±æç¤ºè©ï¼Œè®“{domain}èƒ½å¤ ï¼š
1. å±•ç¾å°ˆæ¥­çŸ¥è­˜å’Œç¶“é©—
2. æä¾›æº–ç¢ºã€å¯¦ç”¨çš„å»ºè­°
3. ç¬¦åˆè©²é ˜åŸŸçš„å°ˆæ¥­æ¨™æº–
4. çµ¦å‡ºå…·é«”çš„æ•¸æ“šå’Œå»ºè­°

ç³»çµ±æç¤ºè©æ‡‰è©²ä»¥"ä½ æ˜¯..."é–‹å§‹ã€‚
"""
        
        expert_prompt = await self.call_llm(
            prompt_generation,
            "ä½ æ˜¯æç¤ºè©å·¥ç¨‹å°ˆå®¶ï¼Œèƒ½ç‚ºä¸åŒå°ˆæ¥­é ˜åŸŸç”Ÿæˆæœ€é©åˆçš„ç³»çµ±æç¤ºè©ã€‚"
        )
        
        return expert_prompt.strip()
    
    async def ask_domain_expert(self, domain: str, expert_prompt: str, user_input: str, context: str) -> str:
        """èª¿ç”¨é ˜åŸŸå°ˆå®¶"""
        final_prompt = f"""
èƒŒæ™¯ä¿¡æ¯: {context}

ç”¨æˆ¶å•é¡Œ: {user_input}

è«‹åŸºæ–¼ä½ çš„å°ˆæ¥­çŸ¥è­˜æä¾›è©³ç´°ã€æº–ç¢ºçš„å›ç­”ï¼ŒåŒ…æ‹¬ï¼š
1. å°ˆæ¥­åˆ†æ
2. å…·é«”æ•¸æ“šï¼ˆå¦‚æœé©ç”¨ï¼‰
3. å¯¦ç”¨å»ºè­°
4. é¢¨éšªè€ƒé‡ï¼ˆå¦‚æœé©ç”¨ï¼‰
"""
        
        expert_response = await self.call_llm(
            final_prompt,
            expert_prompt
        )
        
        return f"ã€{domain}ã€‘\n{expert_response}"
    
    async def aggregate_expert_responses(self, responses: List[str], user_input: str) -> str:
        """èšåˆå°ˆå®¶å›ç­”"""
        if len(responses) == 1:
            return responses[0]
        
        aggregation_prompt = f"""
è«‹æ•´åˆä»¥ä¸‹å°ˆå®¶çš„å›ç­”ï¼Œç‚ºç”¨æˆ¶æä¾›ä¸€å€‹ç¶œåˆã€é€£è²«çš„æœ€çµ‚ç­”æ¡ˆã€‚

ç”¨æˆ¶å•é¡Œ: {user_input}

å°ˆå®¶å›ç­”:
{chr(10).join(responses)}

è«‹æä¾›ä¸€å€‹æ•´åˆçš„æœ€çµ‚å›ç­”ï¼Œè¦æ±‚ï¼š
1. çªå‡ºå„å°ˆå®¶çš„é‡é»è§€é»
2. æ•´åˆäº’è£œçš„ä¿¡æ¯
3. çµ¦å‡ºç¶œåˆå»ºè­°
4. ä¿æŒé‚è¼¯é€£è²«æ€§
"""
        
        final_answer = await self.call_llm(
            aggregation_prompt,
            "ä½ æ˜¯æ•´åˆå°ˆå®¶ï¼Œèƒ½å°‡å¤šå€‹å°ˆæ¥­è§€é»æ•´åˆæˆé€£è²«ã€å¯¦ç”¨çš„æœ€çµ‚ç­”æ¡ˆã€‚"
        )
        
        return final_answer
    
    async def process(self, user_input: str) -> Dict[str, Any]:
        """ä¸»è™•ç†æµç¨‹"""
        start_time = time.time()
        self.request_count += 1
        
        try:
            # 1. Cloud Search MCP
            search_result = await self.cloud_search_mcp(user_input)
            
            # 2. å¤§æ¨¡å‹è­˜åˆ¥é ˜åŸŸ
            domains = await self.identify_domains(user_input, search_result["search_result"])
            
            # 3. å‹•æ…‹ç”Ÿæˆå°ˆå®¶æç¤ºè©ä¸¦èª¿ç”¨
            expert_responses = []
            expert_prompts = {}
            
            for domain in domains:
                expert_prompt = await self.generate_expert_prompt(
                    domain, user_input, search_result["search_result"]
                )
                expert_prompts[domain] = expert_prompt
                
                response = await self.ask_domain_expert(
                    domain, expert_prompt, user_input, search_result["search_result"]
                )
                expert_responses.append(response)
            
            # 4. èšåˆå›ç­”
            final_answer = await self.aggregate_expert_responses(expert_responses, user_input)
            
            processing_time = time.time() - start_time
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ¨™
            for domain in domains:
                if domain not in self.performance_metrics:
                    self.performance_metrics[domain] = {
                        "total_requests": 0,
                        "total_time": 0,
                        "avg_time": 0
                    }
                self.performance_metrics[domain]["total_requests"] += 1
                self.performance_metrics[domain]["total_time"] += processing_time
                self.performance_metrics[domain]["avg_time"] = (
                    self.performance_metrics[domain]["total_time"] / 
                    self.performance_metrics[domain]["total_requests"]
                )
            
            return {
                "final_answer": final_answer,
                "domains_identified": domains,
                "expert_count": len(domains),
                "expert_prompts": expert_prompts,
                "search_context": search_result["search_result"],
                "processing_time": processing_time,
                "process_type": "fully_dynamic",
                "request_id": self.request_count
            }
            
        except Exception as e:
            logging.error(f"è™•ç†å¤±æ•—: {e}")
            return {
                "final_answer": f"è™•ç†å¤±æ•—: {str(e)}",
                "domains_identified": [],
                "expert_count": 0,
                "expert_prompts": {},
                "search_context": "",
                "processing_time": time.time() - start_time,
                "process_type": "error",
                "request_id": self.request_count
            }

# Flaskæ‡‰ç”¨
app = Flask(__name__)
CORS(app)

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–å®Œå…¨å‹•æ…‹MCP
llm_config = {
    "provider": os.getenv("LLM_PROVIDER", "mock"),  # mock, ollama, openai, claude
    "model": os.getenv("LLM_MODEL", "llama3"),
    "api_key": os.getenv("LLM_API_KEY", ""),
    "base_url": os.getenv("LLM_BASE_URL", "http://localhost:11434")
}

dynamic_mcp = FullyDynamicMCP(llm_config)

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥"""
    return jsonify({
        "status": "healthy",
        "mcp_type": "fully_dynamic",
        "llm_provider": llm_config["provider"],
        "total_requests": dynamic_mcp.request_count,
        "timestamp": time.time()
    })

@app.route('/api/process', methods=['POST'])
def process_request():
    """è™•ç†è«‹æ±‚ - ä¸»è¦API"""
    try:
        data = request.get_json()
        user_input = data.get('request', '')
        
        if not user_input:
            return jsonify({"error": "è«‹æ±‚å…§å®¹ä¸èƒ½ç‚ºç©º"}), 400
        
        # åŸ·è¡Œå®Œå…¨å‹•æ…‹MCPè™•ç†
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(dynamic_mcp.process(user_input))
        
        loop.close()
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"è™•ç†è«‹æ±‚å¤±æ•—: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/search', methods=['POST'])
def cloud_search():
    """Cloud Search MCP"""
    try:
        data = request.get_json()
        user_input = data.get('request', '')
        
        if not user_input:
            return jsonify({"error": "è«‹æ±‚å…§å®¹ä¸èƒ½ç‚ºç©º"}), 400
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(dynamic_mcp.cloud_search_mcp(user_input))
        
        loop.close()
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"æœç´¢å¤±æ•—: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/identify', methods=['POST'])
def identify_domains():
    """è­˜åˆ¥å°ˆæ¥­é ˜åŸŸ"""
    try:
        data = request.get_json()
        user_input = data.get('request', '')
        context = data.get('context', '')
        
        if not user_input:
            return jsonify({"error": "è«‹æ±‚å…§å®¹ä¸èƒ½ç‚ºç©º"}), 400
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        domains = loop.run_until_complete(
            dynamic_mcp.identify_domains(user_input, context)
        )
        
        loop.close()
        
        return jsonify({
            "request": user_input,
            "identified_domains": domains,
            "domain_count": len(domains)
        })
        
    except Exception as e:
        logger.error(f"è­˜åˆ¥é ˜åŸŸå¤±æ•—: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/status', methods=['GET'])
def get_status():
    """ç²å–ç³»çµ±ç‹€æ…‹"""
    return jsonify({
        "mcp_type": "fully_dynamic",
        "llm_config": {
            "provider": llm_config["provider"],
            "model": llm_config["model"],
            "base_url": llm_config["base_url"]
        },
        "performance_metrics": dynamic_mcp.performance_metrics,
        "total_requests": dynamic_mcp.request_count,
        "features": [
            "cloud_search_mcp",
            "dynamic_domain_identification", 
            "dynamic_expert_prompt_generation",
            "intelligent_response_aggregation"
        ]
    })

@app.route('/api/demo', methods=['POST'])
def demo_request():
    """æ¼”ç¤ºè«‹æ±‚"""
    try:
        data = request.get_json()
        demo_type = data.get('type', 'insurance')
        
        demo_requests = {
            'insurance': "è‡ºéŠ€äººå£½ä¿å–®è¡Œæ”¿ä½œæ¥­SOPå¤§æ¦‚è¦èŠ±å¤šå°‘äººè™•ç†è¡¨å–®ï¼Œè‡ªå‹•åŒ–æ¯”ç‡åœ¨æ¥­ç•Œæœ‰å¤šé«˜ï¼Œè¡¨å–®OCRç”¨äººä¾†å¯©æ ¸åœ¨æ•´å€‹SOPæµç¨‹æ‰€ä½”çš„äººæœˆå¤§æ¦‚æ˜¯å¤šå°‘ï¼Ÿ",
            'technology': "ä¿éšªæ¥­å¦‚ä½•é‹ç”¨AIå’ŒOCRæŠ€è¡“æå‡æ ¸ä¿æ•ˆç‡ï¼Ÿ",
            'management': "ä¿éšªå…¬å¸å¦‚ä½•å„ªåŒ–äººåŠ›é…ç½®å’Œæµç¨‹ç®¡ç†ï¼Ÿ",
            'general': "è«‹åˆ†æä¿éšªæ¥­æ•¸ä½è½‰å‹çš„è¶¨å‹¢å’ŒæŒ‘æˆ°"
        }
        
        user_input = demo_requests.get(demo_type, demo_requests['insurance'])
        
        # è™•ç†æ¼”ç¤ºè«‹æ±‚
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(dynamic_mcp.process(user_input))
        result["demo_type"] = demo_type
        result["demo_request"] = user_input
        
        loop.close()
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè«‹æ±‚å¤±æ•—: {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    logger.info("ğŸš€ å®Œå…¨å‹•æ…‹MCPæœå‹™å™¨å•Ÿå‹•ä¸­...")
    logger.info(f"ğŸ“‹ LLMé…ç½®: {llm_config['provider']} - {llm_config['model']}")
    logger.info("ğŸ“¡ APIç«¯é»:")
    logger.info("  - GET  /health          - å¥åº·æª¢æŸ¥")
    logger.info("  - POST /api/process     - å®Œå…¨å‹•æ…‹è™•ç†")
    logger.info("  - POST /api/search      - Cloud Search MCP")
    logger.info("  - POST /api/identify    - è­˜åˆ¥å°ˆæ¥­é ˜åŸŸ")
    logger.info("  - GET  /api/status      - ç³»çµ±ç‹€æ…‹")
    logger.info("  - POST /api/demo        - æ¼”ç¤ºè«‹æ±‚")
    logger.info("ğŸ¯ ç‰¹è‰²: é›¶ç¡¬ç·¨ç¢¼ã€å®Œå…¨å‹•æ…‹ã€LLMé©…å‹•")
    
    app.run(host='0.0.0.0', port=5002, debug=False)

