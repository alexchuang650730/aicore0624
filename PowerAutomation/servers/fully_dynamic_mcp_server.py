"""
å®Œå…¨å‹•æ…‹MCPæœå‹™å™¨ - é›¶ç¡¬ç·¨ç¢¼ç‰ˆæœ¬
æ•´åˆCloud Search MCP + å¤§æ¨¡å‹è­˜åˆ¥é ˜åŸŸ + å‹•æ…‹å°ˆå®¶ + Webç®¡ç†ç•Œé¢
Updated: ä½¿ç”¨ç¨ç«‹çš„Cloud Search MCPçµ„ä»¶å’ŒWebç®¡ç†ç•Œé¢
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import asyncio
import aiohttp
import json
import logging
import time
import os
import sys
from typing import List, Dict, Any

# æ·»åŠ çµ„ä»¶è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'components'))

# å°å…¥Cloud Search MCPçµ„ä»¶å’ŒWebç®¡ç†ç•Œé¢
from cloud_search_mcp import CloudSearchMCP, create_cloud_search_mcp
from web_management_interface import WebManagementInterface, create_web_management_interface

# å®Œå…¨å‹•æ…‹MCPæ ¸å¿ƒ
class FullyDynamicMCP:
    """å®Œå…¨å‹•æ…‹MCP - é›¶ç¡¬ç·¨ç¢¼"""
    
    def __init__(self, llm_config: Dict[str, Any]):
        self.llm_config = llm_config
        self.request_count = 0
        self.performance_metrics = {}
        
        # åˆå§‹åŒ–Cloud Search MCPçµ„ä»¶
        self.cloud_search_mcp = None
        
    async def initialize(self):
        """åˆå§‹åŒ–MCPçµ„ä»¶"""
        try:
            # å‰µå»ºCloud Search MCPçµ„ä»¶
            self.cloud_search_mcp = await create_cloud_search_mcp(self.llm_config)
            logging.info("Cloud Search MCPçµ„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logging.error(f"Cloud Search MCPçµ„ä»¶åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    async def call_llm(self, prompt: str, system_prompt: str = "") -> str:
        """èª¿ç”¨å¤§æ¨¡å‹API"""
        try:
            provider = self.llm_config.get("provider", "mock")
            
            if provider == "openai":
                return await self._call_openai(prompt, system_prompt)
            elif provider == "claude":
                return await self._call_claude(prompt, system_prompt)
            elif provider == "ollama":
                return await self._call_ollama(prompt, system_prompt)
            else:
                # Mockæ¨¡å¼ - ç”¨æ–¼æ¼”ç¤º
                await asyncio.sleep(0.2)
                return await self._mock_llm_response(prompt, system_prompt)
                
        except Exception as e:
            logging.error(f"LLMèª¿ç”¨å¤±æ•—: {e}")
            return f"LLMèª¿ç”¨å¤±æ•—: {str(e)}"
    
    async def _call_ollama(self, prompt: str, system_prompt: str) -> str:
        """èª¿ç”¨Ollamaæœ¬åœ°LLM"""
        try:
            url = f"{self.llm_config.get('base_url', 'http://localhost:11434')}/api/generate"
            
            payload = {
                "model": self.llm_config.get("model", "llama3"),
                "prompt": f"{system_prompt}\n\n{prompt}",
                "stream": False
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result.get("response", "ç„¡å›æ‡‰")
                    else:
                        return f"Ollama APIéŒ¯èª¤: {response.status}"
                        
        except Exception as e:
            return f"Ollamaèª¿ç”¨å¤±æ•—: {str(e)}"
    
    async def _call_openai(self, prompt: str, system_prompt: str) -> str:
        """èª¿ç”¨OpenAI API"""
        try:
            url = f"{self.llm_config.get('base_url', 'https://api.openai.com')}/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.llm_config.get('api_key')}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.llm_config.get("model", "gpt-3.5-turbo"),
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ]
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result["choices"][0]["message"]["content"]
                    else:
                        return f"OpenAI APIéŒ¯èª¤: {response.status}"
                        
        except Exception as e:
            return f"OpenAIèª¿ç”¨å¤±æ•—: {str(e)}"
    
    async def _call_claude(self, prompt: str, system_prompt: str) -> str:
        """èª¿ç”¨Claude API"""
        # Claude APIå¯¦ç¾
        return await self._mock_llm_response(prompt, system_prompt)
    
    async def _mock_llm_response(self, prompt: str, system_prompt: str) -> str:
        """Mock LLMå›æ‡‰ - ç”¨æ–¼æ¼”ç¤º"""
        if "æœç´¢" in prompt or "èƒŒæ™¯" in prompt:
            return """
è‡ºéŠ€äººå£½æ˜¯å°ç£éŠ€è¡Œæ——ä¸‹çš„äººå£½ä¿éšªå…¬å¸ï¼Œä¸»è¦æ¥­å‹™åŒ…æ‹¬äººå£½ä¿éšªã€å¹´é‡‘ä¿éšªç­‰ã€‚
ä¿å–®è¡Œæ”¿ä½œæ¥­SOPæ¶‰åŠæ ¸ä¿ã€ç†è³ ã€å®¢æˆ¶æœå‹™ç­‰å¤šå€‹ç’°ç¯€ã€‚

**é—œéµæµç¨‹åŒ…æ‹¬ï¼š**
1. **æ ¸ä¿æµç¨‹**
   - é¢¨éšªè©•ä¼°å’Œä¿è²»è¨ˆç®—
   - é†«ç™‚æª¢æŸ¥å’Œè²¡å‹™å¯©æ ¸
   - ä¿å–®æ¢æ¬¾ç¢ºèª

2. **ç†è³ æµç¨‹**
   - ç†è³ ç”³è«‹å—ç†
   - æ¡ˆä»¶èª¿æŸ¥å’Œå¯©æ ¸
   - ç†è³ é‡‘çµ¦ä»˜

3. **å®¢æˆ¶æœå‹™**
   - ä¿å–®è®Šæ›´æœå‹™
   - çºŒæœŸä¿è²»æ”¶å–
   - å®¢æˆ¶è«®è©¢è™•ç†

4. **ç®¡ç†å»ºè­°**
   - å»ºç«‹è®Šæ›´ç®¡ç†æ©Ÿåˆ¶
   - å“¡å·¥åŸ¹è¨“å’Œè½‰å‹
   - æŒçºŒæ”¹é€²æ–‡åŒ–
   - æ•¸æ“šé©…å‹•æ±ºç­–
"""
        elif "é ˜åŸŸ" in prompt or "å°ˆå®¶" in prompt:
            return """ä¿éšªå°ˆå®¶
æŠ€è¡“å°ˆå®¶
æ³•å¾‹å°ˆå®¶"""
        elif "ä¿éšªå°ˆå®¶" in prompt:
            return """
ä½œç‚ºä¿éšªå°ˆå®¶ï¼Œæˆ‘å»ºè­°ï¼š

**æ ¸ä¿å„ªåŒ–**ï¼š
- å»ºç«‹æ¨™æº–åŒ–æ ¸ä¿æµç¨‹
- å°å…¥AIè¼”åŠ©é¢¨éšªè©•ä¼°
- ç°¡åŒ–ä½é¢¨éšªæ¡ˆä»¶å¯©æ ¸

**ç†è³ æ”¹å–„**ï¼š
- æ•¸ä½åŒ–ç†è³ ç”³è«‹æµç¨‹
- å»ºç«‹å¿«é€Ÿç†è³ é€šé“
- åŠ å¼·ç†è³ æ¡ˆä»¶è¿½è¹¤

**å®¢æˆ¶é«”é©—**ï¼š
- æä¾›24/7ç·šä¸Šæœå‹™
- å»ºç«‹å®¢æˆ¶è‡ªåŠ©å¹³å°
- å„ªåŒ–ä¿å–®ç®¡ç†ç³»çµ±
"""
        elif "æŠ€è¡“å°ˆå®¶" in prompt:
            return """
å¾æŠ€è¡“è§’åº¦å»ºè­°ï¼š

**ç³»çµ±æ•´åˆ**ï¼š
- å»ºç«‹çµ±ä¸€çš„ä¿å–®ç®¡ç†å¹³å°
- æ•´åˆæ ¸ä¿ã€ç†è³ ã€å®¢æœç³»çµ±
- å¯¦ç¾æ•¸æ“šå³æ™‚åŒæ­¥

**è‡ªå‹•åŒ–æ”¹å–„**ï¼š
- å°å…¥RPAè‡ªå‹•åŒ–æµç¨‹
- å»ºç«‹æ™ºèƒ½å®¢æœæ©Ÿå™¨äºº
- å¯¦ç¾æ–‡ä»¶è‡ªå‹•è­˜åˆ¥

**æ•¸æ“šåˆ†æ**ï¼š
- å»ºç«‹å•†æ¥­æ™ºèƒ½å¹³å°
- å¯¦ç¾é æ¸¬æ€§åˆ†æ
- å„ªåŒ–é¢¨éšªæ¨¡å‹
"""
        elif "æ³•å¾‹å°ˆå®¶" in prompt:
            return """
æ³•å¾‹åˆè¦å»ºè­°ï¼š

**æ³•è¦éµå¾ª**ï¼š
- ç¢ºä¿ç¬¦åˆä¿éšªæ³•è¦è¦æ±‚
- å»ºç«‹åˆè¦ç›£æ§æ©Ÿåˆ¶
- å®šæœŸé€²è¡Œæ³•è¦æ›´æ–°

**é¢¨éšªç®¡æ§**ï¼š
- å»ºç«‹æ³•å¾‹é¢¨éšªè©•ä¼°
- å®Œå–„å…§æ§åˆ¶åº¦
- åŠ å¼·å“¡å·¥åˆè¦åŸ¹è¨“

**å®¢æˆ¶æ¬Šç›Š**ï¼š
- ä¿éšœå®¢æˆ¶çŸ¥æƒ…æ¬Š
- å»ºç«‹ç”³è¨´è™•ç†æ©Ÿåˆ¶
- ç¢ºä¿è³‡æ–™éš±ç§ä¿è­·
"""
        elif "æ•´åˆ" in prompt:
            return """
ç¶œåˆå„å°ˆå®¶å»ºè­°ï¼Œè‡ºéŠ€äººå£½ä¿å–®è¡Œæ”¿ä½œæ¥­SOPå„ªåŒ–æ–¹æ¡ˆï¼š

**çŸ­æœŸç›®æ¨™ï¼ˆ3-6å€‹æœˆï¼‰**ï¼š
1. å»ºç«‹æ¨™æº–åŒ–ä½œæ¥­æµç¨‹
2. å°å…¥åŸºç¤è‡ªå‹•åŒ–å·¥å…·
3. å®Œå–„å“¡å·¥åŸ¹è¨“é«”ç³»

**ä¸­æœŸç›®æ¨™ï¼ˆ6-12å€‹æœˆï¼‰**ï¼š
1. å»ºç«‹çµ±ä¸€æ•¸ä½å¹³å°
2. å¯¦ç¾æ ¸å¿ƒæµç¨‹è‡ªå‹•åŒ–
3. å»ºç«‹æ•¸æ“šåˆ†æèƒ½åŠ›

**é•·æœŸç›®æ¨™ï¼ˆ1-2å¹´ï¼‰**ï¼š
1. å¯¦ç¾å…¨é¢æ•¸ä½è½‰å‹
2. å»ºç«‹æ™ºèƒ½æ±ºç­–ç³»çµ±
3. é”æˆè¡Œæ¥­é ˜å…ˆæ°´æº–

**å¯¦æ–½å»ºè­°**ï¼š
- æ¡ç”¨æ•æ·é–‹ç™¼æ–¹æ³•
- å»ºç«‹è·¨éƒ¨é–€å”ä½œæ©Ÿåˆ¶
- æŒçºŒç›£æ§å’Œå„ªåŒ–
- ç¢ºä¿åˆè¦å’Œé¢¨éšªæ§åˆ¶
"""
        else:
            return "æ ¹æ“šæ‚¨çš„å•é¡Œï¼Œæˆ‘æä¾›äº†å°ˆæ¥­çš„åˆ†æå’Œå»ºè­°ã€‚"
    
    async def identify_domains(self, user_input: str, search_context: str) -> List[str]:
        """ç”¨å¤§æ¨¡å‹è­˜åˆ¥éœ€è¦çš„å°ˆæ¥­é ˜åŸŸ"""
        domain_prompt = f"""
åŸºæ–¼ç”¨æˆ¶è¼¸å…¥å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œè«‹è­˜åˆ¥éœ€è¦å“ªäº›å°ˆæ¥­é ˜åŸŸçš„å°ˆå®¶ä¾†å›ç­”é€™å€‹å•é¡Œã€‚

ç”¨æˆ¶è¼¸å…¥: {user_input}
èƒŒæ™¯ä¿¡æ¯: {search_context}

è«‹åªè¿”å›éœ€è¦çš„å°ˆæ¥­é ˜åŸŸåç¨±ï¼Œæ¯è¡Œä¸€å€‹ï¼Œä¾‹å¦‚ï¼š
ä¿éšªå°ˆå®¶
æŠ€è¡“å°ˆå®¶
æ³•å¾‹å°ˆå®¶

è¦æ±‚ï¼š
- æœ€å¤š3å€‹å°ˆå®¶
- åªè¿”å›é ˜åŸŸåç¨±
- ä¸è¦è§£é‡‹æˆ–èªªæ˜
"""
        
        domains_response = await self.call_llm(
            domain_prompt,
            "ä½ æ˜¯å°ˆæ¥­é ˜åŸŸè­˜åˆ¥å°ˆå®¶ï¼Œèƒ½æº–ç¢ºåˆ¤æ–·å•é¡Œéœ€è¦å“ªäº›å°ˆæ¥­çŸ¥è­˜é ˜åŸŸã€‚"
        )
        
        # è§£æè¿”å›çš„é ˜åŸŸåˆ—è¡¨
        if domains_response and domains_response.strip():
            domains = [line.strip() for line in domains_response.split('\n') if line.strip()]
            return domains[:3]  # æœ€å¤š3å€‹å°ˆå®¶
        
        return ["é€šç”¨å°ˆå®¶"]  # é»˜èªå°ˆå®¶
    
    async def generate_expert_prompt(self, domain: str, user_input: str, search_context: str) -> str:
        """å‹•æ…‹ç”Ÿæˆå°ˆå®¶æç¤ºè©"""
        prompt_generation = f"""
è«‹ç‚º{domain}ç”Ÿæˆä¸€å€‹å°ˆæ¥­çš„æç¤ºè©ï¼Œç”¨æ–¼å›ç­”ç”¨æˆ¶å•é¡Œã€‚

ç”¨æˆ¶å•é¡Œ: {user_input}
èƒŒæ™¯ä¿¡æ¯: {search_context}

ç”Ÿæˆçš„æç¤ºè©æ‡‰è©²ï¼š
1. é«”ç¾{domain}çš„å°ˆæ¥­ç‰¹è‰²
2. é‡å°å…·é«”å•é¡Œæä¾›å¯¦ç”¨å»ºè­°
3. çµæ§‹æ¸…æ™°ï¼Œé‚è¼¯åš´è¬¹
4. åŒ…å«å…·é«”çš„è¡Œå‹•å»ºè­°

è«‹ç›´æ¥è¿”å›æç¤ºè©å…§å®¹ï¼Œä¸è¦é¡å¤–èªªæ˜ã€‚
"""
        
        return await self.call_llm(
            prompt_generation,
            f"ä½ æ˜¯æç¤ºè©å·¥ç¨‹å°ˆå®¶ï¼Œèƒ½ç‚º{domain}ç”Ÿæˆé«˜è³ªé‡çš„å°ˆæ¥­æç¤ºè©ã€‚"
        )
    
    async def ask_domain_expert(self, domain: str, expert_prompt: str, user_input: str, search_context: str) -> str:
        """å‘ç‰¹å®šé ˜åŸŸå°ˆå®¶æå•"""
        full_prompt = f"""
{expert_prompt}

åŸºæ–¼ä»¥ä¸‹ä¿¡æ¯å›ç­”å•é¡Œï¼š
ç”¨æˆ¶å•é¡Œ: {user_input}
èƒŒæ™¯ä¿¡æ¯: {search_context}

è«‹æä¾›å°ˆæ¥­ã€å¯¦ç”¨çš„å»ºè­°ã€‚
"""
        
        return await self.call_llm(
            full_prompt,
            f"ä½ æ˜¯{domain}ï¼Œå…·æœ‰è±å¯Œçš„å°ˆæ¥­çŸ¥è­˜å’Œå¯¦è¸ç¶“é©—ã€‚"
        )
    
    async def aggregate_expert_responses(self, expert_responses: List[str], user_input: str) -> str:
        """èšåˆå°ˆå®¶å›ç­”"""
        aggregation_prompt = f"""
è«‹æ•´åˆä»¥ä¸‹å°ˆå®¶çš„å›ç­”ï¼Œå½¢æˆä¸€å€‹é€£è²«ã€å…¨é¢çš„æœ€çµ‚ç­”æ¡ˆï¼š

ç”¨æˆ¶å•é¡Œ: {user_input}

å°ˆå®¶å›ç­”ï¼š
{chr(10).join([f"å°ˆå®¶{i+1}: {response}" for i, response in enumerate(expert_responses)])}

è«‹æä¾›ï¼š
1. ç¶œåˆåˆ†æå’Œå»ºè­°
2. å…·é«”çš„å¯¦æ–½æ­¥é©Ÿ
3. æ³¨æ„äº‹é …å’Œé¢¨éšªæé†’
"""
        
        final_answer = await self.call_llm(
            aggregation_prompt,
            "ä½ æ˜¯æ•´åˆå°ˆå®¶ï¼Œèƒ½å°‡å¤šå€‹å°ˆæ¥­è§€é»æ•´åˆæˆé€£è²«ã€å¯¦ç”¨çš„æœ€çµ‚ç­”æ¡ˆã€‚"
        )
        
        return final_answer
    
    async def process(self, user_input: str) -> Dict[str, Any]:
        """ä¸»è™•ç†æµç¨‹ - ä½¿ç”¨æ–°çš„Cloud Search MCPçµ„ä»¶"""
        start_time = time.time()
        self.request_count += 1
        
        try:
            # æª¢æŸ¥Cloud Search MCPæ˜¯å¦å·²åˆå§‹åŒ–
            if not self.cloud_search_mcp:
                await self.initialize()
            
            # 1. ä½¿ç”¨Cloud Search MCPçµ„ä»¶é€²è¡Œæœç´¢å’Œåˆ†æ
            search_result = await self.cloud_search_mcp.search_and_analyze(user_input)
            
            # 2. å¾æœç´¢çµæœä¸­ç²å–è­˜åˆ¥çš„é ˜åŸŸï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨å¤§æ¨¡å‹è­˜åˆ¥
            domains = search_result.domains_identified
            if not domains:
                domains = await self.identify_domains(user_input, search_result.result)
            
            # 3. å‹•æ…‹ç”Ÿæˆå°ˆå®¶æç¤ºè©ä¸¦èª¿ç”¨
            expert_responses = []
            expert_prompts = {}
            
            for domain in domains:
                expert_prompt = await self.generate_expert_prompt(
                    domain, user_input, search_result.result
                )
                expert_prompts[domain] = expert_prompt
                
                response = await self.ask_domain_expert(
                    domain, expert_prompt, user_input, search_result.result
                )
                expert_responses.append(response)
            
            # 4. èšåˆå›ç­”
            final_answer = await self.aggregate_expert_responses(expert_responses, user_input)
            
            processing_time = time.time() - start_time
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ¨™
            for domain in domains:
                if domain not in self.performance_metrics:
                    self.performance_metrics[domain] = {
                        "total_requests": 0,
                        "total_time": 0,
                        "avg_time": 0
                    }
                self.performance_metrics[domain]["total_requests"] += 1
                self.performance_metrics[domain]["total_time"] += processing_time
                self.performance_metrics[domain]["avg_time"] = (
                    self.performance_metrics[domain]["total_time"] / 
                    self.performance_metrics[domain]["total_requests"]
                )
            
            return {
                "final_answer": final_answer,
                "domains_identified": domains,
                "expert_count": len(domains),
                "expert_prompts": expert_prompts,
                "search_context": search_result.result,
                "search_confidence": search_result.confidence_score,
                "search_metadata": search_result.metadata,
                "processing_time": processing_time,
                "process_type": "fully_dynamic_with_cloud_search_mcp",
                "request_count": self.request_count,
                "cloud_search_mcp_version": self.cloud_search_mcp.version if self.cloud_search_mcp else "unknown"
            }
            
        except Exception as e:
            logging.error(f"è™•ç†å¤±æ•—: {e}")
            return {
                "error": str(e),
                "processing_time": time.time() - start_time,
                "request_count": self.request_count
            }
    
    def get_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        cloud_search_metrics = {}
        if self.cloud_search_mcp:
            cloud_search_metrics = self.cloud_search_mcp.get_metrics()
        
        return {
            "system": "FullyDynamicMCP",
            "version": "2.0.0",
            "request_count": self.request_count,
            "performance_metrics": self.performance_metrics,
            "cloud_search_mcp": cloud_search_metrics,
            "llm_provider": self.llm_config.get("provider", "mock"),
            "status": "active",
            "timestamp": time.time()
        }

# Flaskæ‡‰ç”¨åˆå§‹åŒ–
app = Flask(__name__)
CORS(app)

# åˆå§‹åŒ–Webç®¡ç†ç•Œé¢
web_interface = create_web_management_interface(app)

# å…¨å±€MCPå¯¦ä¾‹
_mcp_instance = None

async def get_mcp_instance():
    """ç²å–MCPå¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _mcp_instance
    if _mcp_instance is None:
        # é»˜èªé…ç½®
        llm_config = {
            "provider": "mock",  # å¯é¸: openai, claude, ollama, mock
            "model": "gpt-3.5-turbo",
            "api_key": "",
            "base_url": ""
        }
        
        _mcp_instance = FullyDynamicMCP(llm_config)
        await _mcp_instance.initialize()
    
    return _mcp_instance

@app.route('/process', methods=['POST'])
def process_request():
    """è™•ç†ç”¨æˆ¶è«‹æ±‚"""
    start_time = time.time()
    try:
        data = request.get_json()
        user_input = data.get('input', '')
        
        if not user_input:
            return jsonify({"error": "ç¼ºå°‘è¼¸å…¥å…§å®¹"}), 400
        
        # ä½¿ç”¨asyncioé‹è¡Œç•°æ­¥è™•ç†
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            mcp = loop.run_until_complete(get_mcp_instance())
            result = loop.run_until_complete(mcp.process(user_input))
            
            # è¨˜éŒ„è«‹æ±‚åˆ°Webç•Œé¢
            response_time = (time.time() - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
            web_interface.record_request(response_time)
            
            return jsonify(result)
        finally:
            loop.close()
            
    except Exception as e:
        logging.error(f"è«‹æ±‚è™•ç†å¤±æ•—: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/status', methods=['GET'])
def get_status():
    """ç²å–ç³»çµ±ç‹€æ…‹"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            mcp = loop.run_until_complete(get_mcp_instance())
            status = mcp.get_status()
            return jsonify(status)
        finally:
            loop.close()
            
    except Exception as e:
        logging.error(f"ç‹€æ…‹ç²å–å¤±æ•—: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            mcp = loop.run_until_complete(get_mcp_instance())
            if mcp.cloud_search_mcp:
                health = loop.run_until_complete(mcp.cloud_search_mcp.health_check())
                return jsonify(health)
            else:
                return jsonify({"healthy": False, "error": "Cloud Search MCPæœªåˆå§‹åŒ–"})
        finally:
            loop.close()
            
    except Exception as e:
        logging.error(f"å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
        return jsonify({"healthy": False, "error": str(e)}), 500

if __name__ == '__main__':
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸš€ å•Ÿå‹•å®Œå…¨å‹•æ…‹MCPæœå‹™å™¨ v2.1")
    print("ğŸ“¦ æ•´åˆCloud Search MCPçµ„ä»¶")
    print("ğŸŒ æ•´åˆWebç®¡ç†ç•Œé¢")
    print("ğŸ”— æ”¯æŒå¤šç¨®LLMæä¾›å•†")
    print("ğŸ“Š å…§å»ºæ€§èƒ½ç›£æ§å’Œå¥åº·æª¢æŸ¥")
    print("ğŸ’» Webç•Œé¢: http://localhost:8099")
    
    app.run(host='0.0.0.0', port=8099, debug=True)

