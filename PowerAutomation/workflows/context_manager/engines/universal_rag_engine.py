#!/usr/bin/env python3
"""
Universal RAG Engine
é€šç”¨ RAG å¼•æ“

ç‚ºæ•´å€‹ AICore ç³»çµ±æä¾›çµ±ä¸€çš„æª¢ç´¢å¢å¼·ç”Ÿæˆèƒ½åŠ›
æ”¯æŒå¤šç¨®æ•¸æ“šé¡å‹ï¼šä»£ç¢¼ã€æ–‡æª”ã€å°è©±ã€é …ç›®çŸ¥è­˜ç­‰
"""

import asyncio
import json
import logging
import time
import hashlib
import sqlite3
import numpy as np
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from enum import Enum
from collections import defaultdict

# å˜—è©¦å°å…¥å‘é‡æ•¸æ“šåº«
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    logging.warning("ChromaDB not available, using fallback vector storage")

# å˜—è©¦å°å…¥åµŒå…¥æ¨¡å‹
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logging.warning("SentenceTransformers not available, using fallback embeddings")

logger = logging.getLogger(__name__)

class DataType(Enum):
    """æ•¸æ“šé¡å‹"""
    CODE = "code"
    DOCUMENTATION = "documentation"
    CONVERSATION = "conversation"
    PROJECT_KNOWLEDGE = "project_knowledge"
    GLOBAL_KNOWLEDGE = "global_knowledge"
    ERROR_LOG = "error_log"
    BEST_PRACTICE = "best_practice"
    DESIGN_PATTERN = "design_pattern"

class SearchMode(Enum):
    """æœç´¢æ¨¡å¼"""
    SEMANTIC = "semantic"          # èªç¾©æœç´¢
    KEYWORD = "keyword"            # é—œéµè©æœç´¢
    HYBRID = "hybrid"              # æ··åˆæœç´¢
    CONTEXTUAL = "contextual"      # ä¸Šä¸‹æ–‡æœç´¢

@dataclass
class Document:
    """æ–‡æª”å°è±¡"""
    doc_id: str
    content: str
    data_type: DataType
    metadata: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[List[float]] = None
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    relevance_score: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "doc_id": self.doc_id,
            "content": self.content,
            "data_type": self.data_type.value,
            "metadata": self.metadata,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "access_count": self.access_count,
            "relevance_score": self.relevance_score
        }

@dataclass
class SearchResult:
    """æœç´¢çµæœ"""
    document: Document
    similarity_score: float
    rank: int
    search_metadata: Dict[str, Any] = field(default_factory=dict)

class EmbeddingEngine:
    """åµŒå…¥å¼•æ“"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.model_name = self.config.get("model_name", "all-MiniLM-L6-v2")
        self.model = None
        self.embedding_dim = 384  # é»˜èªç¶­åº¦
        self.initialized = False
        
    async def initialize(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                self.model = SentenceTransformer(self.model_name)
                self.embedding_dim = self.model.get_sentence_embedding_dimension()
                logger.info(f"Initialized SentenceTransformer model: {self.model_name}")
            else:
                logger.warning("Using fallback embedding engine")
            
            self.initialized = True
            
        except Exception as e:
            logger.error(f"Failed to initialize embedding engine: {e}")
            self.initialized = True  # å…è¨±ä½¿ç”¨å›é€€æ¨¡å¼
    
    async def encode(self, texts: Union[str, List[str]]) -> Union[List[float], List[List[float]]]:
        """ç·¨ç¢¼æ–‡æœ¬ç‚ºå‘é‡"""
        if not self.initialized:
            await self.initialize()
        
        if isinstance(texts, str):
            texts = [texts]
        
        try:
            if self.model:
                embeddings = self.model.encode(texts)
                return embeddings.tolist() if len(texts) > 1 else embeddings[0].tolist()
            else:
                # å›é€€åˆ°ç°¡å–®çš„å“ˆå¸ŒåµŒå…¥
                return [self._hash_embedding(text) for text in texts]
                
        except Exception as e:
            logger.error(f"Error encoding texts: {e}")
            return [self._hash_embedding(text) for text in texts]
    
    def _hash_embedding(self, text: str) -> List[float]:
        """åŸºæ–¼å“ˆå¸Œçš„ç°¡å–®åµŒå…¥"""
        # å‰µå»ºç¢ºå®šæ€§çš„å½éš¨æ©Ÿå‘é‡
        hash_obj = hashlib.md5(text.encode())
        hash_bytes = hash_obj.digest()
        
        # è½‰æ›ç‚ºæµ®é»æ•¸å‘é‡
        embedding = []
        for i in range(0, len(hash_bytes), 4):
            chunk = hash_bytes[i:i+4]
            if len(chunk) == 4:
                value = int.from_bytes(chunk, byteorder='big') / (2**32)
                embedding.append(value)
        
        # å¡«å……åˆ°ç›®æ¨™ç¶­åº¦
        while len(embedding) < self.embedding_dim:
            embedding.extend(embedding[:min(len(embedding), self.embedding_dim - len(embedding))])
        
        return embedding[:self.embedding_dim]

class VectorStore:
    """å‘é‡å­˜å„²"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.storage_path = self.config.get("storage_path", "./vector_store")
        self.collection_name = self.config.get("collection_name", "universal_rag")
        
        self.chroma_client = None
        self.collection = None
        self.fallback_storage = {}  # å›é€€å­˜å„²
        self.initialized = False
        
    async def initialize(self):
        """åˆå§‹åŒ–å‘é‡å­˜å„²"""
        try:
            if CHROMADB_AVAILABLE:
                # åˆå§‹åŒ– ChromaDB
                self.chroma_client = chromadb.PersistentClient(
                    path=self.storage_path,
                    settings=Settings(anonymized_telemetry=False)
                )
                
                self.collection = self.chroma_client.get_or_create_collection(
                    name=self.collection_name,
                    metadata={"description": "Universal RAG collection for AICore"}
                )
                
                logger.info(f"Initialized ChromaDB collection: {self.collection_name}")
            else:
                logger.warning("Using fallback vector storage")
            
            self.initialized = True
            
        except Exception as e:
            logger.error(f"Failed to initialize vector store: {e}")
            self.initialized = True  # å…è¨±ä½¿ç”¨å›é€€æ¨¡å¼
    
    async def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–‡æª”"""
        if not self.initialized:
            await self.initialize()
        
        try:
            if self.collection:
                # ä½¿ç”¨ ChromaDB
                ids = [doc.doc_id for doc in documents]
                embeddings = [doc.embedding for doc in documents if doc.embedding]
                metadatas = [doc.to_dict() for doc in documents]
                documents_text = [doc.content for doc in documents]
                
                if embeddings and len(embeddings) == len(documents):
                    self.collection.add(
                        ids=ids,
                        embeddings=embeddings,
                        metadatas=metadatas,
                        documents=documents_text
                    )
                else:
                    # è®“ ChromaDB è‡ªå‹•ç”ŸæˆåµŒå…¥
                    self.collection.add(
                        ids=ids,
                        metadatas=metadatas,
                        documents=documents_text
                    )
            else:
                # ä½¿ç”¨å›é€€å­˜å„²
                for doc in documents:
                    self.fallback_storage[doc.doc_id] = doc
            
            logger.info(f"Added {len(documents)} documents to vector store")
            
        except Exception as e:
            logger.error(f"Error adding documents: {e}")
            # å›é€€åˆ°å…§å­˜å­˜å„²
            for doc in documents:
                self.fallback_storage[doc.doc_id] = doc
    
    async def search(self, query_embedding: List[float], limit: int = 10, 
                    filters: Dict[str, Any] = None) -> List[Tuple[str, float]]:
        """æœç´¢ç›¸ä¼¼æ–‡æª”"""
        try:
            if self.collection:
                # ä½¿ç”¨ ChromaDB æœç´¢
                where_clause = {}
                if filters:
                    for key, value in filters.items():
                        where_clause[key] = value
                
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=limit,
                    where=where_clause if where_clause else None
                )
                
                # è§£æçµæœ
                search_results = []
                if results['ids'] and results['distances']:
                    for doc_id, distance in zip(results['ids'][0], results['distances'][0]):
                        similarity = 1.0 - distance  # è½‰æ›è·é›¢ç‚ºç›¸ä¼¼åº¦
                        search_results.append((doc_id, similarity))
                
                return search_results
            else:
                # ä½¿ç”¨å›é€€æœç´¢
                return await self._fallback_search(query_embedding, limit, filters)
                
        except Exception as e:
            logger.error(f"Error searching documents: {e}")
            return await self._fallback_search(query_embedding, limit, filters)
    
    async def _fallback_search(self, query_embedding: List[float], limit: int, 
                              filters: Dict[str, Any] = None) -> List[Tuple[str, float]]:
        """å›é€€æœç´¢å¯¦ç¾"""
        results = []
        
        for doc_id, doc in self.fallback_storage.items():
            # æ‡‰ç”¨éæ¿¾å™¨
            if filters:
                match = True
                for key, value in filters.items():
                    if key == "data_type" and doc.data_type.value != value:
                        match = False
                        break
                    elif key in doc.metadata and doc.metadata[key] != value:
                        match = False
                        break
                
                if not match:
                    continue
            
            # è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
            if doc.embedding:
                similarity = self._cosine_similarity(query_embedding, doc.embedding)
            else:
                similarity = 0.5  # é»˜èªç›¸ä¼¼åº¦
            
            results.append((doc_id, similarity))
        
        # æ’åºä¸¦è¿”å›å‰ N å€‹çµæœ
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:limit]
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        try:
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            magnitude1 = sum(a * a for a in vec1) ** 0.5
            magnitude2 = sum(a * a for a in vec2) ** 0.5
            
            if magnitude1 == 0 or magnitude2 == 0:
                return 0.0
            
            return dot_product / (magnitude1 * magnitude2)
        except:
            return 0.0
    
    async def get_document(self, doc_id: str) -> Optional[Document]:
        """ç²å–æ–‡æª”"""
        try:
            if self.collection:
                results = self.collection.get(ids=[doc_id])
                if results['ids']:
                    metadata = results['metadatas'][0]
                    content = results['documents'][0]
                    
                    # é‡å»º Document å°è±¡
                    doc = Document(
                        doc_id=doc_id,
                        content=content,
                        data_type=DataType(metadata.get('data_type', 'documentation')),
                        metadata=metadata.get('metadata', {}),
                        created_at=datetime.fromisoformat(metadata.get('created_at', datetime.now().isoformat())),
                        updated_at=datetime.fromisoformat(metadata.get('updated_at', datetime.now().isoformat())),
                        access_count=metadata.get('access_count', 0),
                        relevance_score=metadata.get('relevance_score', 0.0)
                    )
                    return doc
            else:
                return self.fallback_storage.get(doc_id)
                
        except Exception as e:
            logger.error(f"Error getting document {doc_id}: {e}")
            return self.fallback_storage.get(doc_id)
        
        return None
    
    async def delete_document(self, doc_id: str) -> bool:
        """åˆªé™¤æ–‡æª”"""
        try:
            if self.collection:
                self.collection.delete(ids=[doc_id])
            
            if doc_id in self.fallback_storage:
                del self.fallback_storage[doc_id]
            
            return True
            
        except Exception as e:
            logger.error(f"Error deleting document {doc_id}: {e}")
            return False
    
    async def update_document(self, document: Document):
        """æ›´æ–°æ–‡æª”"""
        await self.delete_document(document.doc_id)
        await self.add_documents([document])

class UniversalRAGEngine:
    """é€šç”¨ RAG å¼•æ“"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.name = "Universal RAG Engine"
        self.version = "1.0.0"
        
        # åˆå§‹åŒ–å­çµ„ä»¶
        self.embedding_engine = EmbeddingEngine(self.config.get("embedding", {}))
        self.vector_store = VectorStore(self.config.get("vector_store", {}))
        
        # é…ç½®åƒæ•¸
        self.max_chunk_size = self.config.get("max_chunk_size", 2000)
        self.chunk_overlap = self.config.get("chunk_overlap", 200)
        self.default_search_limit = self.config.get("default_search_limit", 10)
        self.similarity_threshold = self.config.get("similarity_threshold", 0.3)
        
        # ç‹€æ…‹ç®¡ç†
        self.initialized = False
        self.status = "initializing"
        
        # æ€§èƒ½çµ±è¨ˆ
        self.stats = {
            "total_documents": 0,
            "total_searches": 0,
            "successful_searches": 0,
            "average_search_time": 0.0,
            "data_type_distribution": {dt.value: 0 for dt in DataType}
        }
        
        logger.info(f"Initializing {self.name} v{self.version}")
    
    async def initialize(self):
        """åˆå§‹åŒ– RAG å¼•æ“"""
        try:
            logger.info("Initializing Universal RAG Engine components...")
            
            # åˆå§‹åŒ–åµŒå…¥å¼•æ“
            await self.embedding_engine.initialize()
            logger.info("âœ… Embedding Engine initialized")
            
            # åˆå§‹åŒ–å‘é‡å­˜å„²
            await self.vector_store.initialize()
            logger.info("âœ… Vector Store initialized")
            
            self.initialized = True
            self.status = "ready"
            logger.info(f"ğŸ‰ {self.name} initialization completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize {self.name}: {e}")
            self.status = "error"
            return False
    
    async def add_document(self, content: str, data_type: DataType, 
                          metadata: Dict[str, Any] = None) -> str:
        """æ·»åŠ å–®å€‹æ–‡æª”"""
        doc_id = f"{data_type.value}_{hashlib.md5(content.encode()).hexdigest()[:12]}"
        
        # ç”ŸæˆåµŒå…¥
        embedding = await self.embedding_engine.encode(content)
        
        # å‰µå»ºæ–‡æª”å°è±¡
        document = Document(
            doc_id=doc_id,
            content=content,
            data_type=data_type,
            metadata=metadata or {},
            embedding=embedding
        )
        
        # æ·»åŠ åˆ°å‘é‡å­˜å„²
        await self.vector_store.add_documents([document])
        
        # æ›´æ–°çµ±è¨ˆ
        self.stats["total_documents"] += 1
        self.stats["data_type_distribution"][data_type.value] += 1
        
        logger.debug(f"Added document {doc_id} of type {data_type.value}")
        return doc_id
    
    async def add_documents_batch(self, documents_data: List[Tuple[str, DataType, Dict[str, Any]]]) -> List[str]:
        """æ‰¹é‡æ·»åŠ æ–‡æª”"""
        documents = []
        doc_ids = []
        
        # æº–å‚™æ–‡æª”å…§å®¹
        contents = [content for content, _, _ in documents_data]
        
        # æ‰¹é‡ç”ŸæˆåµŒå…¥
        embeddings = await self.embedding_engine.encode(contents)
        if not isinstance(embeddings[0], list):
            embeddings = [embeddings]
        
        # å‰µå»ºæ–‡æª”å°è±¡
        for i, (content, data_type, metadata) in enumerate(documents_data):
            doc_id = f"{data_type.value}_{hashlib.md5(content.encode()).hexdigest()[:12]}"
            
            document = Document(
                doc_id=doc_id,
                content=content,
                data_type=data_type,
                metadata=metadata or {},
                embedding=embeddings[i] if i < len(embeddings) else None
            )
            
            documents.append(document)
            doc_ids.append(doc_id)
        
        # æ‰¹é‡æ·»åŠ åˆ°å‘é‡å­˜å„²
        await self.vector_store.add_documents(documents)
        
        # æ›´æ–°çµ±è¨ˆ
        self.stats["total_documents"] += len(documents)
        for _, data_type, _ in documents_data:
            self.stats["data_type_distribution"][data_type.value] += 1
        
        logger.info(f"Added {len(documents)} documents in batch")
        return doc_ids
    
    async def search(self, query: str, data_types: List[DataType] = None, 
                    limit: int = None, mode: SearchMode = SearchMode.SEMANTIC,
                    filters: Dict[str, Any] = None) -> List[SearchResult]:
        """æœç´¢æ–‡æª”"""
        if not self.initialized:
            raise RuntimeError("RAG Engine not initialized")
        
        start_time = time.time()
        self.stats["total_searches"] += 1
        
        try:
            limit = limit or self.default_search_limit
            
            # ç”ŸæˆæŸ¥è©¢åµŒå…¥
            query_embedding = await self.embedding_engine.encode(query)
            
            # æº–å‚™éæ¿¾å™¨
            search_filters = filters or {}
            if data_types:
                # å¦‚æœæŒ‡å®šäº†æ•¸æ“šé¡å‹ï¼Œéœ€è¦åˆ†åˆ¥æœç´¢æ¯ç¨®é¡å‹
                all_results = []
                for data_type in data_types:
                    type_filters = {**search_filters, "data_type": data_type.value}
                    type_results = await self.vector_store.search(
                        query_embedding, limit, type_filters
                    )
                    all_results.extend(type_results)
                
                # åˆä½µä¸¦é‡æ–°æ’åº
                all_results.sort(key=lambda x: x[1], reverse=True)
                search_results = all_results[:limit]
            else:
                search_results = await self.vector_store.search(
                    query_embedding, limit, search_filters
                )
            
            # æ§‹å»ºçµæœå°è±¡
            results = []
            for rank, (doc_id, similarity) in enumerate(search_results):
                if similarity >= self.similarity_threshold:
                    document = await self.vector_store.get_document(doc_id)
                    if document:
                        # æ›´æ–°è¨ªå•è¨ˆæ•¸
                        document.access_count += 1
                        document.relevance_score = similarity
                        await self.vector_store.update_document(document)
                        
                        result = SearchResult(
                            document=document,
                            similarity_score=similarity,
                            rank=rank + 1,
                            search_metadata={
                                "query": query,
                                "search_mode": mode.value,
                                "search_time": time.time() - start_time
                            }
                        )
                        results.append(result)
            
            # æ›´æ–°çµ±è¨ˆ
            self.stats["successful_searches"] += 1
            search_time = time.time() - start_time
            self.stats["average_search_time"] = (
                (self.stats["average_search_time"] * (self.stats["successful_searches"] - 1) + search_time)
                / self.stats["successful_searches"]
            )
            
            logger.info(f"Search completed: found {len(results)} results in {search_time:.2f}s")
            return results
            
        except Exception as e:
            logger.error(f"Error during search: {e}")
            return []
    
    async def search_code_knowledge(self, query: str, limit: int = 5) -> List[SearchResult]:
        """æœç´¢ä»£ç¢¼ç›¸é—œçŸ¥è­˜"""
        return await self.search(
            query=query,
            data_types=[DataType.CODE, DataType.BEST_PRACTICE, DataType.DESIGN_PATTERN],
            limit=limit,
            mode=SearchMode.SEMANTIC
        )
    
    async def search_project_experience(self, query: str, limit: int = 3) -> List[SearchResult]:
        """æœç´¢é …ç›®ç¶“é©—"""
        return await self.search(
            query=query,
            data_types=[DataType.PROJECT_KNOWLEDGE, DataType.DOCUMENTATION],
            limit=limit,
            mode=SearchMode.CONTEXTUAL
        )
    
    async def search_test_knowledge(self, query: str, limit: int = 5) -> List[SearchResult]:
        """æœç´¢æ¸¬è©¦ç›¸é—œçŸ¥è­˜"""
        return await self.search(
            query=query,
            data_types=[DataType.CODE, DataType.BEST_PRACTICE],
            limit=limit,
            filters={"category": "testing"}
        )
    
    async def get_global_knowledge(self, query: str, limit: int = 3) -> List[SearchResult]:
        """ç²å–å…¨å±€çŸ¥è­˜"""
        return await self.search(
            query=query,
            data_types=[DataType.GLOBAL_KNOWLEDGE, DataType.BEST_PRACTICE],
            limit=limit,
            mode=SearchMode.HYBRID
        )
    
    async def add_code_snippet(self, code: str, language: str, description: str = "",
                             project_path: str = None) -> str:
        """æ·»åŠ ä»£ç¢¼ç‰‡æ®µ"""
        metadata = {
            "language": language,
            "description": description,
            "category": "code_snippet"
        }
        
        if project_path:
            metadata["project_path"] = project_path
        
        return await self.add_document(code, DataType.CODE, metadata)
    
    async def add_documentation(self, content: str, doc_type: str = "general",
                               source: str = None) -> str:
        """æ·»åŠ æ–‡æª”"""
        metadata = {
            "doc_type": doc_type,
            "category": "documentation"
        }
        
        if source:
            metadata["source"] = source
        
        return await self.add_document(content, DataType.DOCUMENTATION, metadata)
    
    async def add_conversation_context(self, conversation: str, session_id: str,
                                     participants: List[str] = None) -> str:
        """æ·»åŠ å°è©±ä¸Šä¸‹æ–‡"""
        metadata = {
            "session_id": session_id,
            "participants": participants or [],
            "category": "conversation"
        }
        
        return await self.add_document(conversation, DataType.CONVERSATION, metadata)
    
    async def add_project_knowledge(self, knowledge: str, project_name: str,
                                   knowledge_type: str = "general") -> str:
        """æ·»åŠ é …ç›®çŸ¥è­˜"""
        metadata = {
            "project_name": project_name,
            "knowledge_type": knowledge_type,
            "category": "project"
        }
        
        return await self.add_document(knowledge, DataType.PROJECT_KNOWLEDGE, metadata)
    
    async def cleanup_old_documents(self, max_age_days: int = 30, 
                                   min_access_count: int = 1) -> int:
        """æ¸…ç†èˆŠæ–‡æª”"""
        # é€™è£¡éœ€è¦å¯¦ç¾æ¸…ç†é‚è¼¯
        # ç”±æ–¼å‘é‡å­˜å„²çš„é™åˆ¶ï¼Œé€™æ˜¯ä¸€å€‹ç°¡åŒ–å¯¦ç¾
        logger.info(f"Cleanup requested for documents older than {max_age_days} days")
        return 0  # è¿”å›æ¸…ç†çš„æ–‡æª”æ•¸é‡
    
    def get_status(self) -> Dict[str, Any]:
        """ç²å–å¼•æ“ç‹€æ…‹"""
        return {
            "name": self.name,
            "version": self.version,
            "status": self.status,
            "initialized": self.initialized,
            "stats": self.stats,
            "config": {
                "max_chunk_size": self.max_chunk_size,
                "similarity_threshold": self.similarity_threshold,
                "embedding_model": self.embedding_engine.model_name
            },
            "components": {
                "embedding_engine": {
                    "initialized": self.embedding_engine.initialized,
                    "model_available": self.embedding_engine.model is not None,
                    "embedding_dim": self.embedding_engine.embedding_dim
                },
                "vector_store": {
                    "initialized": self.vector_store.initialized,
                    "chromadb_available": CHROMADB_AVAILABLE,
                    "fallback_docs": len(self.vector_store.fallback_storage)
                }
            }
        }
    
    async def shutdown(self):
        """é—œé–‰å¼•æ“"""
        logger.info("Shutting down Universal RAG Engine...")
        
        # æ¸…ç†è³‡æº
        if hasattr(self.embedding_engine, 'model') and self.embedding_engine.model:
            del self.embedding_engine.model
        
        if self.vector_store.chroma_client:
            # ChromaDB æœƒè‡ªå‹•æŒä¹…åŒ–
            pass
        
        self.status = "shutdown"
        logger.info("Universal RAG Engine shut down")

# å·¥å» å‡½æ•¸
async def create_universal_rag_engine(config: Dict[str, Any] = None) -> UniversalRAGEngine:
    """å‰µå»ºä¸¦åˆå§‹åŒ–é€šç”¨ RAG å¼•æ“"""
    engine = UniversalRAGEngine(config)
    await engine.initialize()
    return engine

if __name__ == "__main__":
    # æ¸¬è©¦ä»£ç¢¼
    async def test_rag_engine():
        config = {
            "embedding": {"model_name": "all-MiniLM-L6-v2"},
            "vector_store": {"storage_path": "./test_vector_store"},
            "max_chunk_size": 1000,
            "similarity_threshold": 0.3
        }
        
        engine = await create_universal_rag_engine(config)
        
        # æ·»åŠ æ¸¬è©¦æ–‡æª”
        await engine.add_code_snippet(
            "def hello_world():\n    print('Hello, World!')",
            "python",
            "Simple hello world function"
        )
        
        await engine.add_documentation(
            "This is a comprehensive guide to Python programming.",
            "tutorial",
            "python_guide"
        )
        
        # æœç´¢æ¸¬è©¦
        results = await engine.search("python function", limit=5)
        print(f"Found {len(results)} results")
        
        for result in results:
            print(f"- {result.document.data_type.value}: {result.similarity_score:.3f}")
            print(f"  {result.document.content[:100]}...")
        
        # ç²å–ç‹€æ…‹
        status = engine.get_status()
        print(f"Engine status: {status['status']}")
        print(f"Total documents: {status['stats']['total_documents']}")
        
        # é—œé–‰å¼•æ“
        await engine.shutdown()
    
    asyncio.run(test_rag_engine())

