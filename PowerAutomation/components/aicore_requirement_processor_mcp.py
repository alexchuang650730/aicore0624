#!/usr/bin/env python3
"""
AICore éœ€æ±‚è™•ç†å™¨ (AICore Requirement Processor)
å°‡ç”¨æˆ¶éœ€æ±‚è‡ªå‹•è¼¸å…¥åˆ° AICore 3.0 ç³»çµ±ä¸­ï¼Œèª¿ç”¨ smartinvention MCP å’Œç›¸é—œå°ˆå®¶ä¾†è§£æ±ºå•é¡Œ
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field, asdict
from pathlib import Path
import re

# å°å…¥ AICore 3.0 çµ„ä»¶
from core.aicore3 import AICore3, UserRequest, ProcessingResult
from components.enhanced_smartinvention_mcp_v2 import EnhancedSmartinventionMCP

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RequirementAnalysisRequest:
    """éœ€æ±‚åˆ†æè«‹æ±‚"""
    requirement_id: str
    requirement_text: str
    analysis_scope: str = "full"  # full, basic, cross_task
    target_entity: Optional[str] = None
    context: Dict[str, Any] = field(default_factory=dict)
    timestamp: float = field(default_factory=time.time)

@dataclass
class RequirementItem:
    """éœ€æ±‚é …ç›®"""
    requirement_id: str
    title: str
    description: str
    priority: str
    source_tasks: List[str]
    technical_complexity: str
    estimated_hours: int
    category: str
    dependencies: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ManusAction:
    """Manus è¡Œå‹•é …ç›®"""
    action_id: str
    action_type: str
    description: str
    related_tasks: List[str]
    execution_status: str
    priority: str
    estimated_effort: str
    prerequisites: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class FileReference:
    """æª”æ¡ˆåƒè€ƒ"""
    file_path: str
    file_type: str
    relevance_score: float
    cross_task_relations: List[str]
    description: str
    size_bytes: Optional[int] = None
    last_modified: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class CrossTaskAnalysis:
    """è·¨ä»»å‹™åˆ†æ"""
    related_task_count: int
    shared_requirements: List[str]
    dependency_chain: str
    impact_assessment: str
    coordination_needs: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class RequirementAnalysisResult:
    """éœ€æ±‚åˆ†æçµæœ"""
    requirement_id: str
    analysis_timestamp: str
    requirements_list: List[RequirementItem]
    manus_actions: List[ManusAction]
    file_references: List[FileReference]
    cross_task_analysis: CrossTaskAnalysis
    expert_insights: Dict[str, Any]
    processing_metrics: Dict[str, Any]
    metadata: Dict[str, Any] = field(default_factory=dict)

class RequirementParser:
    """éœ€æ±‚è§£æå™¨"""
    
    def __init__(self):
        self.requirement_patterns = {
            'req_id': r'REQ[_-]?(\d+)',
            'target_entity': r'(REQ[_-]?\d+)',
            'analysis_keywords': ['åˆ—å‡º', 'åˆ†æ', 'æ˜ç¢ºéœ€æ±‚', 'manus action', 'æª”æ¡ˆåˆ—è¡¨'],
            'cross_task_keywords': ['è·¨ä»»å‹™', 'åŒä¸€å€‹éœ€æ±‚', 'å¤šä»»å‹™']
        }
    
    def parse_requirement(self, requirement_text: str) -> Dict[str, Any]:
        """è§£æéœ€æ±‚æ–‡æœ¬"""
        logger.info(f"ğŸ” è§£æéœ€æ±‚æ–‡æœ¬: {requirement_text[:100]}...")
        
        # æå–éœ€æ±‚ID
        req_id_match = re.search(self.requirement_patterns['req_id'], requirement_text)
        target_entity = req_id_match.group(0) if req_id_match else None
        
        # æå–ç›®æ¨™å¯¦é«”
        entity_match = re.search(self.requirement_patterns['target_entity'], requirement_text)
        target_entity = entity_match.group(0) if entity_match else target_entity
        
        # åˆ†æéœ€æ±‚é¡å‹
        requirement_type = self._classify_requirement_type(requirement_text)
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦è·¨ä»»å‹™åˆ†æ
        cross_task_analysis = any(keyword in requirement_text for keyword in self.requirement_patterns['cross_task_keywords'])
        
        # æå–è¼¸å‡ºæ ¼å¼è¦æ±‚
        output_format = self._extract_output_format(requirement_text)
        
        # ç¢ºå®šåˆ†æç¯„åœ
        analysis_scope = "full" if cross_task_analysis else "basic"
        
        result = {
            "requirement_type": requirement_type,
            "target_entity": target_entity,
            "analysis_scope": analysis_scope,
            "output_format": output_format,
            "cross_task_analysis": cross_task_analysis,
            "data_sources": ["smartinvention_mcp"],
            "expert_domains": self._determine_expert_domains(requirement_text)
        }
        
        logger.info(f"âœ… éœ€æ±‚è§£æå®Œæˆ: {result}")
        return result
    
    def _classify_requirement_type(self, text: str) -> str:
        """åˆ†é¡éœ€æ±‚é¡å‹"""
        if any(keyword in text for keyword in self.requirement_patterns['analysis_keywords']):
            return "requirement_analysis"
        return "general_inquiry"
    
    def _extract_output_format(self, text: str) -> List[str]:
        """æå–è¼¸å‡ºæ ¼å¼è¦æ±‚"""
        formats = []
        if "æ˜ç¢ºéœ€æ±‚" in text or "éœ€æ±‚åˆ—è¡¨" in text:
            formats.append("requirements_list")
        if "manus action" in text:
            formats.append("manus_actions")
        if "æª”æ¡ˆåˆ—è¡¨" in text:
            formats.append("file_list")
        return formats or ["requirements_list"]
    
    def _determine_expert_domains(self, text: str) -> List[str]:
        """ç¢ºå®šéœ€è¦çš„å°ˆå®¶é ˜åŸŸ"""
        domains = ["requirement_analysis"]  # åŸºç¤éœ€æ±‚åˆ†æå°ˆå®¶
        
        if "ç•Œé¢" in text or "UI" in text or "UX" in text or "è¨­è¨ˆ" in text:
            domains.append("ui_ux_design")
        
        if "æª”æ¡ˆ" in text or "è·¨ä»»å‹™" in text:
            domains.append("data_analysis")
        
        if "æŠ€è¡“" in text or "å¯¦ç¾" in text:
            domains.append("technical_architecture")
        
        return domains

class ExpertCoordinator:
    """å°ˆå®¶å”èª¿å™¨"""
    
    def __init__(self, aicore: AICore3):
        self.aicore = aicore
        self.expert_configs = {
            "requirement_analysis": {
                "name": "éœ€æ±‚åˆ†æå°ˆå®¶",
                "skills": ["éœ€æ±‚å·¥ç¨‹", "æ¥­å‹™åˆ†æ", "è·¨ä»»å‹™é—œè¯åˆ†æ"],
                "scenario_type": "REQUIREMENT_ANALYSIS"
            },
            "ui_ux_design": {
                "name": "UI/UXè¨­è¨ˆå°ˆå®¶", 
                "skills": ["ç•Œé¢è¨­è¨ˆ", "ç”¨æˆ¶é«”é©—", "å‰ç«¯æŠ€è¡“"],
                "scenario_type": "UI_UX_DESIGN"
            },
            "data_analysis": {
                "name": "æ•¸æ“šåˆ†æå°ˆå®¶",
                "skills": ["æ•¸æ“šæŒ–æ˜", "é—œè¯åˆ†æ", "æª”æ¡ˆç³»çµ±åˆ†æ"],
                "scenario_type": "DATA_ANALYSIS"
            },
            "technical_architecture": {
                "name": "æŠ€è¡“æ¶æ§‹å°ˆå®¶",
                "skills": ["ç³»çµ±è¨­è¨ˆ", "æŠ€è¡“å¯¦ç¾", "æ¶æ§‹è¦åŠƒ"],
                "scenario_type": "TECHNICAL_ARCHITECTURE"
            }
        }
    
    async def coordinate_experts(self, parsed_requirement: Dict[str, Any], smartinvention_data: Dict[str, Any]) -> Dict[str, Any]:
        """å”èª¿å°ˆå®¶åˆ†æ"""
        logger.info(f"ğŸ¤ é–‹å§‹å°ˆå®¶å”èª¿ï¼Œéœ€è¦å°ˆå®¶: {parsed_requirement['expert_domains']}")
        
        expert_results = {}
        
        for domain in parsed_requirement['expert_domains']:
            if domain in self.expert_configs:
                logger.info(f"ğŸ§  èª¿ç”¨ {domain} å°ˆå®¶")
                expert_result = await self._invoke_expert(domain, parsed_requirement, smartinvention_data)
                expert_results[domain] = expert_result
        
        # èšåˆå°ˆå®¶çµæœ
        aggregated_result = await self._aggregate_expert_results(expert_results, parsed_requirement)
        
        logger.info(f"âœ… å°ˆå®¶å”èª¿å®Œæˆï¼Œå…± {len(expert_results)} å€‹å°ˆå®¶åƒèˆ‡")
        return aggregated_result
    
    async def _invoke_expert(self, domain: str, parsed_requirement: Dict[str, Any], smartinvention_data: Dict[str, Any]) -> Dict[str, Any]:
        """èª¿ç”¨ç‰¹å®šé ˜åŸŸå°ˆå®¶"""
        config = self.expert_configs[domain]
        
        # æ§‹å»ºå°ˆå®¶è«‹æ±‚
        expert_request = {
            "expert_domain": domain,
            "expert_name": config["name"],
            "scenario_type": config["scenario_type"],
            "skills": config["skills"],
            "task_data": smartinvention_data,
            "requirement_context": parsed_requirement,
            "analysis_objectives": self._get_analysis_objectives(domain, parsed_requirement)
        }
        
        # æ¨¡æ“¬å°ˆå®¶åˆ†æï¼ˆåœ¨å¯¦éš›å¯¦ç¾ä¸­æœƒèª¿ç”¨ AICore çš„å‹•æ…‹å°ˆå®¶ç³»çµ±ï¼‰
        if domain == "requirement_analysis":
            return await self._requirement_analysis_expert(expert_request)
        elif domain == "ui_ux_design":
            return await self._ui_ux_design_expert(expert_request)
        elif domain == "data_analysis":
            return await self._data_analysis_expert(expert_request)
        elif domain == "technical_architecture":
            return await self._technical_architecture_expert(expert_request)
        else:
            return {"error": f"Unknown expert domain: {domain}"}
    
    def _get_analysis_objectives(self, domain: str, parsed_requirement: Dict[str, Any]) -> List[str]:
        """ç²å–åˆ†æç›®æ¨™"""
        base_objectives = {
            "requirement_analysis": [
                "è­˜åˆ¥æ˜ç¢ºéœ€æ±‚",
                "åˆ†æéœ€æ±‚å„ªå…ˆç´š",
                "è©•ä¼°éœ€æ±‚ä¾è³´é—œä¿‚"
            ],
            "ui_ux_design": [
                "åˆ†æç•Œé¢è¨­è¨ˆéœ€æ±‚",
                "è©•ä¼°ç”¨æˆ¶é«”é©—å½±éŸ¿",
                "æä¾›è¨­è¨ˆå»ºè­°"
            ],
            "data_analysis": [
                "åˆ†æè·¨ä»»å‹™é—œè¯",
                "è­˜åˆ¥ç›¸é—œæª”æ¡ˆ",
                "ç”Ÿæˆä¾è³´é—œä¿‚åœ–"
            ],
            "technical_architecture": [
                "è©•ä¼°æŠ€è¡“å¯è¡Œæ€§",
                "åˆ†æå¯¦ç¾è¤‡é›œåº¦",
                "æä¾›æ¶æ§‹å»ºè­°"
            ]
        }
        
        objectives = base_objectives.get(domain, [])
        
        # æ ¹æ“šéœ€æ±‚æ·»åŠ ç‰¹å®šç›®æ¨™
        if parsed_requirement.get("cross_task_analysis"):
            objectives.append("åŸ·è¡Œè·¨ä»»å‹™åˆ†æ")
        
        return objectives
    
    async def _requirement_analysis_expert(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """éœ€æ±‚åˆ†æå°ˆå®¶"""
        task_data = request["task_data"]
        target_entity = request["requirement_context"].get("target_entity", "REQ_001")
        
        # åˆ†æä»»å‹™æ•¸æ“šä¸­çš„éœ€æ±‚
        requirements = []
        manus_actions = []
        
        # å¾ä»»å‹™æ•¸æ“šä¸­æå–éœ€æ±‚
        for task_id, task_info in task_data.get("tasks", {}).items():
            if self._is_relevant_to_target(task_info, target_entity):
                req = self._extract_requirement_from_task(task_info, task_id)
                if req:
                    requirements.append(req)
                
                actions = self._extract_actions_from_task(task_info, task_id)
                manus_actions.extend(actions)
        
        return {
            "expert_type": "requirement_analysis",
            "analysis_result": {
                "identified_requirements": requirements,
                "manus_actions": manus_actions,
                "priority_assessment": self._assess_priorities(requirements),
                "dependency_analysis": self._analyze_dependencies(requirements)
            },
            "confidence": 0.85,
            "processing_time": 2.5
        }
    
    async def _ui_ux_design_expert(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """UI/UXè¨­è¨ˆå°ˆå®¶"""
        requirements = request.get("analysis_result", {}).get("identified_requirements", [])
        
        ui_analysis = {
            "design_complexity": "ä¸­ç­‰",
            "user_impact": "é«˜",
            "implementation_recommendations": [
                "æ¡ç”¨éŸ¿æ‡‰å¼è¨­è¨ˆ",
                "ç¢ºä¿è·¨ç€è¦½å™¨å…¼å®¹æ€§",
                "å¯¦æ–½ç”¨æˆ¶æ¸¬è©¦"
            ],
            "design_patterns": [
                "å°èˆªæ¬„æ•´åˆæ¨¡å¼",
                "åŠŸèƒ½é·ç§»æ¨¡å¼"
            ]
        }
        
        return {
            "expert_type": "ui_ux_design",
            "analysis_result": ui_analysis,
            "confidence": 0.80,
            "processing_time": 1.8
        }
    
    async def _data_analysis_expert(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """æ•¸æ“šåˆ†æå°ˆå®¶"""
        task_data = request["task_data"]
        
        # åˆ†ææª”æ¡ˆé—œè¯
        file_analysis = self._analyze_file_relationships(task_data)
        cross_task_analysis = self._analyze_cross_task_relationships(task_data)
        
        return {
            "expert_type": "data_analysis",
            "analysis_result": {
                "file_relationships": file_analysis,
                "cross_task_relationships": cross_task_analysis,
                "data_dependencies": self._identify_data_dependencies(task_data)
            },
            "confidence": 0.90,
            "processing_time": 3.2
        }
    
    async def _technical_architecture_expert(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """æŠ€è¡“æ¶æ§‹å°ˆå®¶"""
        requirements = request.get("analysis_result", {}).get("identified_requirements", [])
        
        architecture_analysis = {
            "technical_feasibility": "é«˜",
            "implementation_complexity": "ä¸­ç­‰",
            "resource_requirements": {
                "development_time": "40-60å°æ™‚",
                "team_size": "2-3äºº",
                "technologies": ["HTML/CSS", "JavaScript", "å‰ç«¯æ¡†æ¶"]
            },
            "risk_assessment": [
                "ç”¨æˆ¶é©æ‡‰æ€§é¢¨éšª: ä½",
                "æŠ€è¡“å¯¦ç¾é¢¨éšª: ä½",
                "é›†æˆé¢¨éšª: ä¸­"
            ]
        }
        
        return {
            "expert_type": "technical_architecture",
            "analysis_result": architecture_analysis,
            "confidence": 0.88,
            "processing_time": 2.1
        }
    
    def _is_relevant_to_target(self, task_info: Dict[str, Any], target_entity: str) -> bool:
        """åˆ¤æ–·ä»»å‹™æ˜¯å¦èˆ‡ç›®æ¨™å¯¦é«”ç›¸é—œ"""
        # æª¢æŸ¥ä»»å‹™æè¿°ä¸­æ˜¯å¦åŒ…å«UI/UXç›¸é—œé—œéµè©
        description = task_info.get("description", "").lower()
        ui_keywords = ["ui", "ux", "ç•Œé¢", "è¨­è¨ˆ", "å°èˆª", "ç”¨æˆ¶é«”é©—"]
        
        return any(keyword in description for keyword in ui_keywords)
    
    def _extract_requirement_from_task(self, task_info: Dict[str, Any], task_id: str) -> Optional[Dict[str, Any]]:
        """å¾ä»»å‹™ä¸­æå–éœ€æ±‚"""
        if not self._is_relevant_to_target(task_info, "REQ_001"):
            return None
        
        description = task_info.get("description", "")
        
        # ç”Ÿæˆéœ€æ±‚ID
        req_id = f"REQ_001_{task_id}_UI"
        
        # åˆ†æå„ªå…ˆç´š
        priority = "é«˜" if "å°èˆª" in description or "æ™ºæ…§ä¸‹è¼‰" in description else "ä¸­"
        
        # è©•ä¼°è¤‡é›œåº¦
        complexity = "ä¸­ç­‰" if len(description) > 50 else "ä½"
        
        return {
            "requirement_id": req_id,
            "title": f"{task_info.get('task_name', 'æœªçŸ¥ä»»å‹™')} UIéœ€æ±‚",
            "description": description,
            "priority": priority,
            "source_tasks": [task_id],
            "technical_complexity": complexity,
            "estimated_hours": 40 if priority == "é«˜" else 20,
            "category": "UI/UXè¨­è¨ˆ"
        }
    
    def _extract_actions_from_task(self, task_info: Dict[str, Any], task_id: str) -> List[Dict[str, Any]]:
        """å¾ä»»å‹™ä¸­æå–è¡Œå‹•é …ç›®"""
        actions = []
        description = task_info.get("description", "")
        
        if "å°èˆª" in description:
            actions.append({
                "action_id": f"ACTION_{task_id}_NAV",
                "action_type": "å°èˆªå„ªåŒ–",
                "description": "å„ªåŒ–å°èˆªæ¬„åŠŸèƒ½",
                "related_tasks": [task_id],
                "execution_status": "å¾…åŸ·è¡Œ",
                "priority": "é«˜",
                "estimated_effort": "2-3å¤©"
            })
        
        if "è¨­è¨ˆ" in description:
            actions.append({
                "action_id": f"ACTION_{task_id}_DESIGN",
                "action_type": "ç•Œé¢è¨­è¨ˆ",
                "description": "ç•Œé¢è¨­è¨ˆå„ªåŒ–",
                "related_tasks": [task_id],
                "execution_status": "å¾…åŸ·è¡Œ",
                "priority": "ä¸­",
                "estimated_effort": "1-2å¤©"
            })
        
        return actions
    
    def _assess_priorities(self, requirements: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è©•ä¼°å„ªå…ˆç´š"""
        high_priority = [req for req in requirements if req.get("priority") == "é«˜"]
        medium_priority = [req for req in requirements if req.get("priority") == "ä¸­"]
        low_priority = [req for req in requirements if req.get("priority") == "ä½"]
        
        return {
            "high_priority_count": len(high_priority),
            "medium_priority_count": len(medium_priority),
            "low_priority_count": len(low_priority),
            "priority_distribution": {
                "é«˜": len(high_priority),
                "ä¸­": len(medium_priority),
                "ä½": len(low_priority)
            }
        }
    
    def _analyze_dependencies(self, requirements: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æä¾è³´é—œä¿‚"""
        # ç°¡åŒ–çš„ä¾è³´åˆ†æ
        dependencies = []
        for req in requirements:
            if "å°èˆª" in req.get("description", ""):
                dependencies.append({
                    "requirement": req["requirement_id"],
                    "depends_on": ["åŸºç¤UIæ¡†æ¶", "ç”¨æˆ¶èªè­‰ç³»çµ±"],
                    "dependency_type": "æŠ€è¡“ä¾è³´"
                })
        
        return {
            "total_dependencies": len(dependencies),
            "dependency_details": dependencies,
            "critical_path": self._identify_critical_path(dependencies)
        }
    
    def _identify_critical_path(self, dependencies: List[Dict[str, Any]]) -> List[str]:
        """è­˜åˆ¥é—œéµè·¯å¾‘"""
        # ç°¡åŒ–çš„é—œéµè·¯å¾‘åˆ†æ
        return ["åŸºç¤UIæ¡†æ¶", "å°èˆªæ¬„é‡æ§‹", "åŠŸèƒ½é·ç§»", "ç”¨æˆ¶æ¸¬è©¦"]
    
    def _analyze_file_relationships(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææª”æ¡ˆé—œä¿‚"""
        file_relationships = {}
        
        for task_id, task_info in task_data.get("tasks", {}).items():
            files = task_info.get("files", [])
            for file_info in files:
                file_path = file_info.get("file_path", "")
                if "task_info.json" in file_path:
                    file_relationships[file_path] = {
                        "type": "ä»»å‹™å…ƒæ•¸æ“š",
                        "relevance_score": 0.95,
                        "related_tasks": [task_id],
                        "cross_references": []
                    }
        
        return file_relationships
    
    def _analyze_cross_task_relationships(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè·¨ä»»å‹™é—œä¿‚"""
        ui_related_tasks = []
        
        for task_id, task_info in task_data.get("tasks", {}).items():
            if self._is_relevant_to_target(task_info, "REQ_001"):
                ui_related_tasks.append(task_id)
        
        return {
            "related_task_count": len(ui_related_tasks),
            "related_tasks": ui_related_tasks,
            "shared_requirements": ["UIå„ªåŒ–", "ç”¨æˆ¶é«”é©—æå‡"],
            "dependency_chain": " â†’ ".join(ui_related_tasks[:3]) if ui_related_tasks else "ç„¡",
            "coordination_complexity": "ä¸­ç­‰" if len(ui_related_tasks) > 2 else "ä½"
        }
    
    def _identify_data_dependencies(self, task_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è­˜åˆ¥æ•¸æ“šä¾è³´"""
        dependencies = []
        
        # åˆ†æä»»å‹™é–“çš„æ•¸æ“šä¾è³´
        for task_id, task_info in task_data.get("tasks", {}).items():
            if self._is_relevant_to_target(task_info, "REQ_001"):
                dependencies.append({
                    "source_task": task_id,
                    "dependency_type": "æ•¸æ“šå…±äº«",
                    "shared_resources": ["ç”¨æˆ¶ç•Œé¢é…ç½®", "å°èˆªè¨­å®š"],
                    "impact_level": "ä¸­"
                })
        
        return dependencies
    
    async def _aggregate_expert_results(self, expert_results: Dict[str, Any], parsed_requirement: Dict[str, Any]) -> Dict[str, Any]:
        """èšåˆå°ˆå®¶çµæœ"""
        logger.info("ğŸ”„ èšåˆå°ˆå®¶åˆ†æçµæœ")
        
        # èšåˆéœ€æ±‚åˆ—è¡¨
        all_requirements = []
        all_actions = []
        
        for domain, result in expert_results.items():
            analysis = result.get("analysis_result", {})
            
            if "identified_requirements" in analysis:
                all_requirements.extend(analysis["identified_requirements"])
            
            if "manus_actions" in analysis:
                all_actions.extend(analysis["manus_actions"])
        
        # èšåˆæª”æ¡ˆåˆ†æ
        file_analysis = {}
        cross_task_analysis = {}
        
        if "data_analysis" in expert_results:
            data_result = expert_results["data_analysis"]["analysis_result"]
            file_analysis = data_result.get("file_relationships", {})
            cross_task_analysis = data_result.get("cross_task_relationships", {})
        
        # èšåˆå°ˆå®¶æ´å¯Ÿ
        expert_insights = {}
        for domain, result in expert_results.items():
            expert_insights[domain] = {
                "confidence": result.get("confidence", 0.0),
                "processing_time": result.get("processing_time", 0.0),
                "key_findings": result.get("analysis_result", {})
            }
        
        return {
            "requirements": all_requirements,
            "actions": all_actions,
            "file_analysis": file_analysis,
            "cross_task_analysis": cross_task_analysis,
            "expert_insights": expert_insights,
            "aggregation_metadata": {
                "expert_count": len(expert_results),
                "aggregation_time": time.time(),
                "confidence_average": sum(r.get("confidence", 0) for r in expert_results.values()) / len(expert_results)
            }
        }

class AICoreRequirementProcessor:
    """AICore éœ€æ±‚è™•ç†å™¨ä¸»é¡"""
    
    def __init__(self):
        self.aicore = None
        self.smartinvention_mcp = None
        self.requirement_parser = RequirementParser()
        self.expert_coordinator = None
        self.processing_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "average_processing_time": 0.0
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–è™•ç†å™¨"""
        logger.info("ğŸš€ åˆå§‹åŒ– AICore éœ€æ±‚è™•ç†å™¨")
        
        try:
            # åˆå§‹åŒ– AICore 3.0
            self.aicore = AICore3()
            await self.aicore.initialize()
            
            # åˆå§‹åŒ– Enhanced Smartinvention MCP
            self.smartinvention_mcp = EnhancedSmartinventionMCP()
            await self.smartinvention_mcp.initialize()
            
            # åˆå§‹åŒ–å°ˆå®¶å”èª¿å™¨
            self.expert_coordinator = ExpertCoordinator(self.aicore)
            
            logger.info("âœ… AICore éœ€æ±‚è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    async def process_requirement(self, requirement_text: str, context: Dict[str, Any] = None) -> RequirementAnalysisResult:
        """è™•ç†ç”¨æˆ¶éœ€æ±‚"""
        start_time = time.time()
        request_id = f"REQ_PROC_{int(start_time)}"
        
        logger.info(f"ğŸ¯ é–‹å§‹è™•ç†éœ€æ±‚: {request_id}")
        logger.info(f"ğŸ“ éœ€æ±‚å…§å®¹: {requirement_text}")
        
        try:
            # éšæ®µ1: éœ€æ±‚è§£æ
            logger.info("ğŸ” éšæ®µ1: éœ€æ±‚è§£æ")
            parsed_requirement = self.requirement_parser.parse_requirement(requirement_text)
            
            # éšæ®µ2: æ•¸æ“šç²å–
            logger.info("ğŸ“Š éšæ®µ2: å¾ Smartinvention MCP ç²å–æ•¸æ“š")
            smartinvention_data = await self._acquire_smartinvention_data(parsed_requirement)
            
            # éšæ®µ3: å°ˆå®¶å”ä½œåˆ†æ
            logger.info("ğŸ§  éšæ®µ3: å°ˆå®¶å”ä½œåˆ†æ")
            expert_analysis = await self.expert_coordinator.coordinate_experts(parsed_requirement, smartinvention_data)
            
            # éšæ®µ4: çµæœæ ¼å¼åŒ–
            logger.info("ğŸ“‹ éšæ®µ4: çµæœæ ¼å¼åŒ–")
            formatted_result = await self._format_analysis_result(
                request_id, parsed_requirement, expert_analysis, smartinvention_data
            )
            
            # æ›´æ–°çµ±è¨ˆ
            processing_time = time.time() - start_time
            self._update_processing_stats(True, processing_time)
            
            logger.info(f"âœ… éœ€æ±‚è™•ç†å®Œæˆ: {request_id}, è€—æ™‚: {processing_time:.2f}ç§’")
            return formatted_result
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._update_processing_stats(False, processing_time)
            logger.error(f"âŒ éœ€æ±‚è™•ç†å¤±æ•—: {request_id}, éŒ¯èª¤: {e}")
            raise
    
    async def analyze_req_001(self, analysis_scope: str = "full") -> RequirementAnalysisResult:
        """å°ˆé–€åˆ†æ REQ_001 çš„æ–¹æ³•"""
        requirement_text = "é¦–å…ˆå…ˆé‡å° REQ_001: ç”¨æˆ¶ç•Œé¢è¨­è¨ˆéœ€æ±‚ åˆ—å‡ºæˆ‘çš„æ˜ç¢ºéœ€æ±‚ åŠmanus action åŒ…å«ç›¸é—œçš„æª”æ¡ˆåˆ—è¡¨ æ³¨æ„åŒä¸€å€‹éœ€æ±‚å¯èƒ½è·¨ä»»å‹™"
        
        context = {
            "target_requirement": "REQ_001",
            "analysis_scope": analysis_scope,
            "focus_areas": ["ui_design", "cross_task_analysis", "file_mapping"]
        }
        
        return await self.process_requirement(requirement_text, context)
    
    async def _acquire_smartinvention_data(self, parsed_requirement: Dict[str, Any]) -> Dict[str, Any]:
        """å¾ Smartinvention MCP ç²å–æ•¸æ“š"""
        logger.info("ğŸ“¡ é€£æ¥ Smartinvention MCP ç²å–æ•¸æ“š")
        
        try:
            # ç²å–æ‰€æœ‰ä»»å‹™
            all_tasks = await self.smartinvention_mcp.get_all_tasks()
            
            # æœå°‹UIç›¸é—œä»»å‹™
            ui_tasks = await self.smartinvention_mcp.search_tasks("UI")
            design_tasks = await self.smartinvention_mcp.search_tasks("è¨­è¨ˆ")
            interface_tasks = await self.smartinvention_mcp.search_tasks("ç•Œé¢")
            
            # åˆä½µæœå°‹çµæœ
            relevant_tasks = {}
            for task_list in [ui_tasks, design_tasks, interface_tasks]:
                if isinstance(task_list, list):
                    for task in task_list:
                        if hasattr(task, 'task_id'):
                            relevant_tasks[task.task_id] = task
            
            # ç²å–è©³ç´°æ•¸æ“š
            detailed_data = {}
            for task_id, task in relevant_tasks.items():
                try:
                    # ç²å–å°è©±æ­·å²
                    conversations = await self.smartinvention_mcp.get_task_conversations(task_id)
                    
                    # ç²å–æª”æ¡ˆåˆ—è¡¨
                    files = await self.smartinvention_mcp.get_task_files(task_id)
                    
                    # ç²å–éœ€æ±‚åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    requirements = None
                    try:
                        requirements = await self.smartinvention_mcp.analyze_task_requirements(task_id)
                    except:
                        pass  # éœ€æ±‚åˆ†æå¯èƒ½ä¸å¯ç”¨
                    
                    detailed_data[task_id] = {
                        "task_info": task,
                        "conversations": conversations,
                        "files": files,
                        "requirements": requirements,
                        "description": getattr(task, 'description', ''),
                        "task_name": getattr(task, 'task_name', f'Task {task_id}')
                    }
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ç²å–ä»»å‹™ {task_id} è©³ç´°æ•¸æ“šå¤±æ•—: {e}")
                    continue
            
            result = {
                "all_tasks_count": len(all_tasks) if all_tasks else 0,
                "relevant_tasks_count": len(detailed_data),
                "tasks": detailed_data,
                "search_results": {
                    "ui_tasks": len(ui_tasks) if ui_tasks else 0,
                    "design_tasks": len(design_tasks) if design_tasks else 0,
                    "interface_tasks": len(interface_tasks) if interface_tasks else 0
                },
                "acquisition_timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"âœ… æ•¸æ“šç²å–å®Œæˆ: {len(detailed_data)} å€‹ç›¸é—œä»»å‹™")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Smartinvention MCP æ•¸æ“šç²å–å¤±æ•—: {e}")
            # è¿”å›æ¨¡æ“¬æ•¸æ“šä»¥ç¢ºä¿æµç¨‹ç¹¼çºŒ
            return self._get_mock_smartinvention_data()
    
    def _get_mock_smartinvention_data(self) -> Dict[str, Any]:
        """ç²å–æ¨¡æ“¬æ•¸æ“šï¼ˆç•¶çœŸå¯¦æ•¸æ“šä¸å¯ç”¨æ™‚ï¼‰"""
        logger.info("ğŸ”„ ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š")
        
        return {
            "all_tasks_count": 11,
            "relevant_tasks_count": 3,
            "tasks": {
                "TASK_001": {
                    "task_info": {
                        "task_id": "TASK_001",
                        "task_name": "UI/UXè¨­è¨ˆä»»å‹™ç¾¤çµ„",
                        "description": "å¦‚ä½•å°‡æ™ºæ…§ä¸‹è¼‰ç§»è‡³å°èˆªæ¬„å¹¶ç§»é™¤åŸåŠŸèƒ½"
                    },
                    "conversations": [],
                    "files": [
                        {
                            "file_path": "/home/ec2-user/smartinvention_mcp/tasks/TASK_001/metadata/task_info.json",
                            "file_type": "ä»»å‹™å…ƒæ•¸æ“š"
                        }
                    ],
                    "requirements": None
                },
                "TASK_003": {
                    "task_info": {
                        "task_id": "TASK_003",
                        "task_name": "MCPæ¶æ§‹ç¾¤çµ„",
                        "description": "ç«¯é›²å”åŒç³»çµ±çš„ç•Œé¢è¨­è¨ˆå„ªåŒ–"
                    },
                    "conversations": [],
                    "files": [],
                    "requirements": None
                },
                "TASK_006": {
                    "task_info": {
                        "task_id": "TASK_006",
                        "task_name": "ä¼æ¥­è§£æ±ºæ–¹æ¡ˆç¾¤çµ„",
                        "description": "ä¼æ¥­ç´šç•Œé¢è¨­è¨ˆéœ€æ±‚"
                    },
                    "conversations": [],
                    "files": [],
                    "requirements": None
                }
            },
            "search_results": {
                "ui_tasks": 3,
                "design_tasks": 2,
                "interface_tasks": 1
            },
            "acquisition_timestamp": datetime.now().isoformat()
        }
    
    async def _format_analysis_result(self, request_id: str, parsed_requirement: Dict[str, Any], 
                                    expert_analysis: Dict[str, Any], smartinvention_data: Dict[str, Any]) -> RequirementAnalysisResult:
        """æ ¼å¼åŒ–åˆ†æçµæœ"""
        logger.info("ğŸ“‹ æ ¼å¼åŒ–åˆ†æçµæœ")
        
        # è½‰æ›éœ€æ±‚åˆ—è¡¨
        requirements_list = []
        for req_data in expert_analysis.get("requirements", []):
            requirement = RequirementItem(
                requirement_id=req_data.get("requirement_id", ""),
                title=req_data.get("title", ""),
                description=req_data.get("description", ""),
                priority=req_data.get("priority", "ä¸­"),
                source_tasks=req_data.get("source_tasks", []),
                technical_complexity=req_data.get("technical_complexity", "ä¸­ç­‰"),
                estimated_hours=req_data.get("estimated_hours", 20),
                category=req_data.get("category", "ä¸€èˆ¬"),
                dependencies=req_data.get("dependencies", []),
                metadata=req_data.get("metadata", {})
            )
            requirements_list.append(requirement)
        
        # è½‰æ›è¡Œå‹•é …ç›®
        manus_actions = []
        for action_data in expert_analysis.get("actions", []):
            action = ManusAction(
                action_id=action_data.get("action_id", ""),
                action_type=action_data.get("action_type", ""),
                description=action_data.get("description", ""),
                related_tasks=action_data.get("related_tasks", []),
                execution_status=action_data.get("execution_status", "å¾…åŸ·è¡Œ"),
                priority=action_data.get("priority", "ä¸­"),
                estimated_effort=action_data.get("estimated_effort", ""),
                prerequisites=action_data.get("prerequisites", []),
                metadata=action_data.get("metadata", {})
            )
            manus_actions.append(action)
        
        # è½‰æ›æª”æ¡ˆåƒè€ƒ
        file_references = []
        file_analysis = expert_analysis.get("file_analysis", {})
        for file_path, file_data in file_analysis.items():
            file_ref = FileReference(
                file_path=file_path,
                file_type=file_data.get("type", "æœªçŸ¥"),
                relevance_score=file_data.get("relevance_score", 0.5),
                cross_task_relations=file_data.get("related_tasks", []),
                description=f"{file_data.get('type', 'æª”æ¡ˆ')} - {file_path}",
                metadata=file_data
            )
            file_references.append(file_ref)
        
        # è½‰æ›è·¨ä»»å‹™åˆ†æ
        cross_task_data = expert_analysis.get("cross_task_analysis", {})
        cross_task_analysis = CrossTaskAnalysis(
            related_task_count=cross_task_data.get("related_task_count", 0),
            shared_requirements=cross_task_data.get("shared_requirements", []),
            dependency_chain=cross_task_data.get("dependency_chain", ""),
            impact_assessment=cross_task_data.get("coordination_complexity", "ä½"),
            coordination_needs=cross_task_data.get("coordination_needs", []),
            metadata=cross_task_data
        )
        
        # è™•ç†æŒ‡æ¨™
        processing_metrics = {
            "total_tasks_analyzed": smartinvention_data.get("relevant_tasks_count", 0),
            "requirements_identified": len(requirements_list),
            "actions_generated": len(manus_actions),
            "files_analyzed": len(file_references),
            "expert_confidence_average": expert_analysis.get("aggregation_metadata", {}).get("confidence_average", 0.0),
            "processing_stages_completed": 4
        }
        
        # å‰µå»ºæœ€çµ‚çµæœ
        result = RequirementAnalysisResult(
            requirement_id=parsed_requirement.get("target_entity", "REQ_001"),
            analysis_timestamp=datetime.now().isoformat(),
            requirements_list=requirements_list,
            manus_actions=manus_actions,
            file_references=file_references,
            cross_task_analysis=cross_task_analysis,
            expert_insights=expert_analysis.get("expert_insights", {}),
            processing_metrics=processing_metrics,
            metadata={
                "request_id": request_id,
                "parsed_requirement": parsed_requirement,
                "smartinvention_data_summary": {
                    "total_tasks": smartinvention_data.get("all_tasks_count", 0),
                    "relevant_tasks": smartinvention_data.get("relevant_tasks_count", 0)
                }
            }
        )
        
        logger.info(f"âœ… çµæœæ ¼å¼åŒ–å®Œæˆ: {len(requirements_list)} éœ€æ±‚, {len(manus_actions)} è¡Œå‹•, {len(file_references)} æª”æ¡ˆ")
        return result
    
    def _update_processing_stats(self, success: bool, processing_time: float):
        """æ›´æ–°è™•ç†çµ±è¨ˆ"""
        self.processing_stats["total_requests"] += 1
        if success:
            self.processing_stats["successful_requests"] += 1
        
        # æ›´æ–°å¹³å‡è™•ç†æ™‚é–“
        total_time = self.processing_stats["average_processing_time"] * (self.processing_stats["total_requests"] - 1)
        self.processing_stats["average_processing_time"] = (total_time + processing_time) / self.processing_stats["total_requests"]
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ç²å–è™•ç†çµ±è¨ˆ"""
        success_rate = 0.0
        if self.processing_stats["total_requests"] > 0:
            success_rate = self.processing_stats["successful_requests"] / self.processing_stats["total_requests"]
        
        return {
            **self.processing_stats,
            "success_rate": success_rate
        }
    
    async def export_result_to_json(self, result: RequirementAnalysisResult, file_path: str):
        """å°å‡ºçµæœåˆ°JSONæª”æ¡ˆ"""
        try:
            result_dict = asdict(result)
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… çµæœå·²å°å‡ºåˆ°: {file_path}")
        except Exception as e:
            logger.error(f"âŒ å°å‡ºçµæœå¤±æ•—: {e}")
            raise

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•¸ç¤ºä¾‹"""
    processor = AICoreRequirementProcessor()
    
    try:
        # åˆå§‹åŒ–
        await processor.initialize()
        
        # è™•ç† REQ_001 éœ€æ±‚
        result = await processor.analyze_req_001("full")
        
        # è¼¸å‡ºçµæœæ‘˜è¦
        print(f"\nğŸ¯ REQ_001 åˆ†æçµæœæ‘˜è¦:")
        print(f"ğŸ“‹ è­˜åˆ¥éœ€æ±‚æ•¸é‡: {len(result.requirements_list)}")
        print(f"ğŸš€ Manus è¡Œå‹•æ•¸é‡: {len(result.manus_actions)}")
        print(f"ğŸ“ ç›¸é—œæª”æ¡ˆæ•¸é‡: {len(result.file_references)}")
        print(f"ğŸ”— è·¨ä»»å‹™é—œè¯: {result.cross_task_analysis.related_task_count} å€‹ä»»å‹™")
        
        # å°å‡ºçµæœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"/home/ubuntu/req_001_analysis_result_{timestamp}.json"
        await processor.export_result_to_json(result, output_file)
        
        # é¡¯ç¤ºè™•ç†çµ±è¨ˆ
        stats = processor.get_processing_stats()
        print(f"\nğŸ“Š è™•ç†çµ±è¨ˆ:")
        print(f"ç¸½è«‹æ±‚æ•¸: {stats['total_requests']}")
        print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"å¹³å‡è™•ç†æ™‚é–“: {stats['average_processing_time']:.2f}ç§’")
        
    except Exception as e:
        logger.error(f"âŒ ä¸»ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

