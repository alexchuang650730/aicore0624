"""
Enhanced Test Flow MCP v5.0 - é›†æˆå¢å¼·å ±å‘Šç”Ÿæˆå™¨
æ•´åˆCode Fix Adapterå’Œå¢å¼·çš„æ‘˜è¦å ±å‘Šç”ŸæˆåŠŸèƒ½

æ–°å¢åŠŸèƒ½ï¼š
- AIåŠ©æ‰‹ç´šåˆ¥çš„è©³ç´°æ‘˜è¦å ±å‘Š
- å¯¦æ™‚æ§åˆ¶å°è¼¸å‡º
- çµæ§‹åŒ–å ±å‘Šæ ¼å¼
- æ™ºèƒ½æ–‡ä»¶ç®¡ç†

ç‰ˆæœ¬: 5.0.0
å‰µå»ºæ—¥æœŸ: 2025-06-23
åŠŸèƒ½: ç³»çµ±å•é¡Œä¿®æ­£ã€è§£æ±ºæ–¹æ¡ˆç”Ÿæˆã€å¢å¼·å ±å‘Šè¼¸å‡º
"""

import asyncio
import json
import logging
import time
import sys
import os
from typing import Dict, Any, Optional, List, Union
from datetime import datetime
from enum import Enum
from dataclasses import dataclass
import aiohttp
import re

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘ä»¥å°å…¥å¢å¼·å ±å‘Šç”Ÿæˆå™¨
sys.path.append('/home/ubuntu')
from enhanced_summary_report_generator import (
    EnhancedSummaryReportGenerator,
    create_enhanced_test_report
)

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UserMode(Enum):
    """ç”¨æˆ¶æ¨¡å¼"""
    DEVELOPER = "developer"
    USER = "user"

class ProcessingStage(Enum):
    """è™•ç†éšæ®µï¼ˆé–‹ç™¼è€…æ¨¡å¼ï¼‰"""
    REQUIREMENT_SYNC = "requirement_sync"      # éœ€æ±‚åŒæ­¥å¼•æ“
    COMPARISON_ANALYSIS = "comparison_analysis" # æ¯”è¼ƒåˆ†æå¼•æ“  
    EVALUATION_REPORT = "evaluation_report"    # è©•ä¼°å ±å‘Šç”Ÿæˆå™¨
    CODE_FIX = "code_fix"                     # Code Fix Adapter
    COMPLETED = "completed"

class FixStrategy(Enum):
    """ä¿®å¾©ç­–ç•¥"""
    CONSERVATIVE = "conservative"
    AGGRESSIVE = "aggressive"
    INTELLIGENT = "intelligent"
    KILOCODE_FALLBACK = "kilocode_fallback"

@dataclass
class DeveloperRequest:
    """é–‹ç™¼è€…æ¨¡å¼è«‹æ±‚"""
    requirement: str
    mode: UserMode
    manus_context: Optional[Dict[str, Any]] = None
    fix_strategy: FixStrategy = FixStrategy.INTELLIGENT
    priority: str = "medium"

@dataclass
class ProcessingResult:
    """è™•ç†çµæœ"""
    stage: ProcessingStage
    success: bool
    data: Dict[str, Any]
    confidence_score: float
    recommendations: List[str]
    execution_time: float = 0.0
    next_stage: Optional[ProcessingStage] = None

class RequirementSyncEngine:
    """éœ€æ±‚åŒæ­¥å¼•æ“"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.manus_api_base = config.get("manus_api.base_url", "https://manus.chat")
    
    async def sync_requirement(self, request: DeveloperRequest) -> ProcessingResult:
        """åŒæ­¥éœ€æ±‚åˆ°Manusç³»çµ±"""
        
        start_time = time.time()
        self.logger.info("é–‹å§‹éœ€æ±‚åŒæ­¥å¼•æ“è™•ç†")
        
        try:
            # è§£æéœ€æ±‚
            parsed_requirement = self._parse_requirement(request.requirement)
            
            # èˆ‡Manusç³»çµ±åŒæ­¥
            manus_sync_result = await self._sync_with_manus(parsed_requirement, request.manus_context)
            
            # ç”ŸæˆåŒæ­¥å ±å‘Š
            sync_data = {
                "original_requirement": request.requirement,
                "parsed_requirement": parsed_requirement,
                "manus_sync_status": manus_sync_result.get("status", "unknown"),
                "manus_feedback": manus_sync_result.get("feedback", {}),
                "sync_timestamp": datetime.now().isoformat(),
                "requirement_id": manus_sync_result.get("requirement_id", f"manus_req_{int(time.time())}")
            }
            
            confidence = manus_sync_result.get("confidence", 0.8)
            execution_time = time.time() - start_time
            
            return ProcessingResult(
                stage=ProcessingStage.REQUIREMENT_SYNC,
                success=True,
                data=sync_data,
                confidence_score=confidence,
                recommendations=["éœ€æ±‚å·²æˆåŠŸåŒæ­¥åˆ°Manusç³»çµ±"],
                execution_time=execution_time,
                next_stage=ProcessingStage.COMPARISON_ANALYSIS
            )
            
        except Exception as e:
            self.logger.error(f"éœ€æ±‚åŒæ­¥å¤±æ•—: {e}")
            execution_time = time.time() - start_time
            
            return ProcessingResult(
                stage=ProcessingStage.REQUIREMENT_SYNC,
                success=False,
                data={"error": str(e)},
                confidence_score=0.0,
                recommendations=["éœ€æ±‚åŒæ­¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²çµ¡é€£æ¥å’Œé…ç½®"],
                execution_time=execution_time
            )
    
    def _parse_requirement(self, requirement: str) -> Dict[str, Any]:
        """è§£æéœ€æ±‚"""
        
        # ç°¡åŒ–çš„éœ€æ±‚è§£æé‚è¼¯
        req_type = "creation" if any(word in requirement for word in ["å‰µå»º", "å»ºç«‹", "ç”Ÿæˆ"]) else \
                  "fixing" if any(word in requirement for word in ["ä¿®å¾©", "ä¿®æ­£", "è§£æ±º"]) else \
                  "testing"
        
        priority = "high" if any(word in requirement for word in ["ç·Šæ€¥", "é‡è¦", "é—œéµ"]) else \
                  "low" if any(word in requirement for word in ["ç°¡å–®", "åŸºæœ¬", "è¼•å¾®"]) else \
                  "medium"
        
        complexity = "high" if len(requirement) > 100 or any(word in requirement for word in ["è¤‡é›œ", "å®Œæ•´", "å…¨é¢"]) else \
                    "low" if len(requirement) < 30 else \
                    "medium"
        
        return {
            "type": req_type,
            "priority": priority,
            "complexity": complexity,
            "keywords": self._extract_keywords(requirement),
            "estimated_effort": self._estimate_effort(complexity, req_type)
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–é—œéµè©"""
        
        keywords = []
        
        # æŠ€è¡“é—œéµè©
        tech_keywords = ['api', 'ui', 'database', 'mcp', 'workflow', 'test', 'fix']
        for keyword in tech_keywords:
            if keyword in text.lower():
                keywords.append(keyword)
        
        # å‹•ä½œé—œéµè©
        action_keywords = ['å‰µå»º', 'ä¿®å¾©', 'æ¸¬è©¦', 'åˆ†æ', 'å„ªåŒ–', 'éƒ¨ç½²']
        for keyword in action_keywords:
            if keyword in text:
                keywords.append(keyword)
        
        return keywords
    
    def _estimate_effort(self, complexity: str, req_type: str) -> str:
        """ä¼°ç®—å·¥ä½œé‡"""
        
        effort_matrix = {
            ("low", "testing"): "1-2å°æ™‚",
            ("low", "fixing"): "30åˆ†é˜-1å°æ™‚", 
            ("low", "creation"): "2-4å°æ™‚",
            ("medium", "testing"): "4-8å°æ™‚",
            ("medium", "fixing"): "2-4å°æ™‚",
            ("medium", "creation"): "1-2å¤©",
            ("high", "testing"): "1-2å¤©",
            ("high", "fixing"): "4-8å°æ™‚",
            ("high", "creation"): "3-5å¤©"
        }
        
        return effort_matrix.get((complexity, req_type), "å¾…è©•ä¼°")
    
    async def _sync_with_manus(self, parsed_requirement: Dict[str, Any], 
                              manus_context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """èˆ‡Manusç³»çµ±åŒæ­¥"""
        
        try:
            # æ¨¡æ“¬Manus APIèª¿ç”¨
            sync_payload = {
                "requirement": parsed_requirement,
                "context": manus_context or {},
                "timestamp": datetime.now().isoformat()
            }
            
            # æ¨¡æ“¬APIéŸ¿æ‡‰
            await asyncio.sleep(0.1)  # æ¨¡æ“¬ç¶²çµ¡å»¶é²
            
            return {
                "status": "synced",
                "requirement_id": f"manus_req_{int(time.time())}",
                "confidence": 0.85,
                "feedback": {
                    "manus_understanding": "éœ€æ±‚å·²ç†è§£ä¸¦è¨˜éŒ„",
                    "suggested_approach": "å»ºè­°ä½¿ç”¨å¢é‡ä¿®å¾©ç­–ç•¥",
                    "estimated_complexity": parsed_requirement.get("complexity", "medium")
                }
            }
            
        except Exception as e:
            return {
                "status": "failed",
                "error": str(e),
                "confidence": 0.0
            }

class ComparisonAnalysisEngine:
    """æ¯”è¼ƒåˆ†æå¼•æ“"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.comparison_threshold = config.get("comparison.threshold", 0.7)
    
    async def analyze_comparison(self, sync_result: ProcessingResult) -> ProcessingResult:
        """åŸ·è¡Œæ¯”è¼ƒåˆ†æ"""
        
        start_time = time.time()
        self.logger.info("é–‹å§‹æ¯”è¼ƒåˆ†æå¼•æ“è™•ç†")
        
        try:
            requirement_data = sync_result.data
            requirement_id = requirement_data.get("requirement_id", "unknown")
            
            # ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹
            current_state = await self._get_current_system_state()
            
            # ç²å–Manusæ¨™æº–
            manus_standards = await self._get_manus_standards(requirement_data.get("parsed_requirement", {}))
            
            # åŸ·è¡Œæ¯”è¼ƒåˆ†æ
            comparison_result = self._perform_comparison(current_state, manus_standards)
            
            # ç”Ÿæˆå·®è·åˆ†æ
            gap_analysis = self._analyze_gaps(comparison_result)
            
            analysis_data = {
                "requirement_id": requirement_id,
                "current_state": current_state,
                "manus_standards": manus_standards,
                "comparison_result": comparison_result,
                "gap_analysis": gap_analysis,
                "analysis_timestamp": datetime.now().isoformat()
            }
            
            execution_time = time.time() - start_time
            
            return ProcessingResult(
                stage=ProcessingStage.COMPARISON_ANALYSIS,
                success=True,
                data=analysis_data,
                confidence_score=comparison_result.get("confidence", 0.75),
                recommendations=[
                    "å­˜åœ¨æ”¹é€²ç©ºé–“ï¼Œå»ºè­°é€æ­¥å„ªåŒ–",
                    f"é‡é»é—œæ³¨ï¼š{gap_analysis.get('priority_areas', ['ç¶­æŒç¾ç‹€'])[0]}"
                ],
                execution_time=execution_time,
                next_stage=ProcessingStage.EVALUATION_REPORT
            )
            
        except Exception as e:
            self.logger.error(f"æ¯”è¼ƒåˆ†æå¤±æ•—: {e}")
            execution_time = time.time() - start_time
            
            return ProcessingResult(
                stage=ProcessingStage.COMPARISON_ANALYSIS,
                success=False,
                data={"error": str(e)},
                confidence_score=0.0,
                recommendations=["æ¯”è¼ƒåˆ†æå¤±æ•—ï¼Œè«‹æª¢æŸ¥ç³»çµ±ç‹€æ…‹"],
                execution_time=execution_time
            )
    
    async def _get_current_system_state(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹"""
        
        # æ¨¡æ“¬ç³»çµ±ç‹€æ…‹æª¢æŸ¥
        await asyncio.sleep(0.05)
        
        return {
            "mcp_components": {
                "test_flow_mcp": {"status": "active", "version": "5.0.0"},
                "manus_integration_mcp": {"status": "active", "version": "1.0.0"},
                "code_fix_adapter": {"status": "integrated", "version": "2.0.0"}
            },
            "system_health": {
                "cpu_usage": 15.2,
                "memory_usage": 245,
                "response_time": 120
            },
            "recent_issues": [
                {"type": "performance", "severity": "low", "count": 2},
                {"type": "compatibility", "severity": "medium", "count": 1}
            ],
            "capabilities": [
                "workflow_testing", "code_fixing", "manus_integration",
                "requirement_analysis", "automated_repair"
            ]
        }
    
    async def _get_manus_standards(self, requirement: Dict[str, Any]) -> Dict[str, Any]:
        """ç²å–Manusæ¨™æº–"""
        
        # æ¨¡æ“¬Manusæ¨™æº–ç²å–
        await asyncio.sleep(0.05)
        
        return {
            "best_practices": [
                "éµå¾ªè¨­è¨ˆæ¨¡å¼",
                "å¯¦æ–½æ¨¡å¡ŠåŒ–æ¶æ§‹", 
                "ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶"
            ],
            "performance_benchmarks": {
                "development_speed": "å¿«é€Ÿ",
                "code_reusability": "> 70%",
                "bug_rate": "< 2%"
            }
        }
    
    def _perform_comparison(self, current_state: Dict[str, Any], 
                          manus_standards: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œæ¯”è¼ƒåˆ†æ"""
        
        # æ€§èƒ½æ¯”è¼ƒ
        performance_score = self._compare_performance(current_state.get("system_health", {}))
        
        # èƒ½åŠ›æ¯”è¼ƒ
        capability_score = self._compare_capabilities(
            current_state.get("capabilities", []),
            manus_standards.get("best_practices", [])
        )
        
        # è³ªé‡æ¯”è¼ƒ
        quality_score = self._compare_quality(current_state, manus_standards)
        
        overall_score = (performance_score + capability_score + quality_score) / 3
        
        return {
            "performance_comparison": {
                "score": performance_score,
                "details": {
                    "response_time": {
                        "current": f"{current_state.get('system_health', {}).get('response_time', 0)}ms",
                        "standard": "< 200ms",
                        "status": "good" if current_state.get('system_health', {}).get('response_time', 0) < 200 else "needs_improvement"
                    },
                    "resource_usage": {
                        "cpu": f"{current_state.get('system_health', {}).get('cpu_usage', 0)}%",
                        "memory": f"{current_state.get('system_health', {}).get('memory_usage', 0)}MB",
                        "status": "good"
                    }
                }
            },
            "capability_comparison": {
                "score": capability_score,
                "details": {
                    "available_capabilities": current_state.get("capabilities", []),
                    "required_practices": manus_standards.get("best_practices", []),
                    "coverage": f"{len(current_state.get('capabilities', []))}/{len(manus_standards.get('best_practices', []))} practices covered"
                }
            },
            "quality_comparison": {
                "score": quality_score,
                "details": {
                    "code_quality": "Bç´š (æ¨™æº–: Aç´š)",
                    "documentation": "éƒ¨åˆ†å®Œæ•´ (æ¨™æº–: å®Œæ•´)",
                    "maintainability": "ä¸­ç­‰ (æ¨™æº–: é«˜)"
                }
            },
            "overall_score": overall_score,
            "confidence": 0.75
        }
    
    def _compare_performance(self, system_health: Dict[str, Any]) -> float:
        """æ¯”è¼ƒæ€§èƒ½"""
        response_time = system_health.get("response_time", 200)
        cpu_usage = system_health.get("cpu_usage", 50)
        
        # ç°¡å–®çš„è©•åˆ†é‚è¼¯
        time_score = 1.0 if response_time < 150 else 0.8 if response_time < 200 else 0.6
        cpu_score = 1.0 if cpu_usage < 20 else 0.8 if cpu_usage < 50 else 0.6
        
        return (time_score + cpu_score) / 2
    
    def _compare_capabilities(self, current_capabilities: List[str], 
                            required_practices: List[str]) -> float:
        """æ¯”è¼ƒèƒ½åŠ›"""
        if not required_practices:
            return 0.85
        
        # ç°¡å–®çš„è¦†è“‹ç‡è¨ˆç®—
        coverage = len(current_capabilities) / max(len(required_practices), 1)
        return min(coverage, 1.0) * 0.85
    
    def _compare_quality(self, current_state: Dict[str, Any], 
                        manus_standards: Dict[str, Any]) -> float:
        """æ¯”è¼ƒè³ªé‡"""
        # åŸºæ–¼å•é¡Œæ•¸é‡çš„ç°¡å–®è©•åˆ†
        issues = current_state.get("recent_issues", [])
        issue_count = sum(issue.get("count", 0) for issue in issues)
        
        if issue_count == 0:
            return 0.9
        elif issue_count <= 3:
            return 0.75
        else:
            return 0.6
    
    def _analyze_gaps(self, comparison_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå·®è·"""
        
        overall_score = comparison_result.get("overall_score", 0.5)
        
        if overall_score >= 0.9:
            gap_level = "low"
            priority_areas = ["æŒçºŒå„ªåŒ–"]
        elif overall_score >= 0.7:
            gap_level = "medium"
            priority_areas = ["ç¶­æŒç¾ç‹€"]
        else:
            gap_level = "high"
            priority_areas = ["ç·Šæ€¥æ”¹é€²"]
        
        improvement_potential = f"{(1 - overall_score) * 100:.1f}%"
        
        return {
            "gap_level": gap_level,
            "priority_areas": priority_areas,
            "improvement_potential": improvement_potential,
            "recommended_actions": ["é‡é»æ”¹é€²", "å¢é‡å‡ç´š", "æ€§èƒ½èª¿å„ª"]
        }

class EvaluationReportGenerator:
    """è©•ä¼°å ±å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    async def generate_report(self, analysis_result: ProcessingResult) -> ProcessingResult:
        """ç”Ÿæˆè©•ä¼°å ±å‘Š"""
        
        start_time = time.time()
        self.logger.info("é–‹å§‹è©•ä¼°å ±å‘Šç”Ÿæˆ")
        
        try:
            analysis_data = analysis_result.data
            requirement_id = analysis_data.get("requirement_id", "unknown")
            comparison_result = analysis_data.get("comparison_result", {})
            gap_analysis = analysis_data.get("gap_analysis", {})
            
            # ç”Ÿæˆè©•ä¼°å ±å‘Š
            evaluation_report = self._create_evaluation_report(comparison_result, gap_analysis)
            
            # ç”Ÿæˆä¿®å¾©å»ºè­°
            fix_recommendations = self._generate_fix_recommendations(gap_analysis, comparison_result)
            
            # ç”Ÿæˆå¯¦æ–½è¨ˆåŠƒ
            implementation_plan = self._create_implementation_plan(fix_recommendations)
            
            report_data = {
                "requirement_id": requirement_id,
                "evaluation_report": evaluation_report,
                "fix_recommendations": fix_recommendations,
                "implementation_plan": implementation_plan,
                "report_timestamp": datetime.now().isoformat(),
                "ready_for_code_fix": len(fix_recommendations) > 0
            }
            
            execution_time = time.time() - start_time
            
            return ProcessingResult(
                stage=ProcessingStage.EVALUATION_REPORT,
                success=True,
                data=report_data,
                confidence_score=0.9,
                recommendations=[
                    "ç³»çµ±ç‹€æ…‹è‰¯å¥½ï¼Œå¯é€²è¡Œå„ªåŒ–",
                    "è©•ä¼°å ±å‘Šå·²ç”Ÿæˆï¼Œæº–å‚™é€²å…¥ä»£ç¢¼ä¿®å¾©éšæ®µ",
                    "å»ºè­°æŒ‰ç…§å¯¦æ–½è¨ˆåŠƒé€æ­¥åŸ·è¡Œ",
                    "æŒçºŒç›£æ§ä¿®å¾©é€²åº¦å’Œæ•ˆæœ"
                ],
                execution_time=execution_time,
                next_stage=ProcessingStage.CODE_FIX
            )
            
        except Exception as e:
            self.logger.error(f"è©•ä¼°å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            execution_time = time.time() - start_time
            
            return ProcessingResult(
                stage=ProcessingStage.EVALUATION_REPORT,
                success=False,
                data={"error": str(e)},
                confidence_score=0.0,
                recommendations=["è©•ä¼°å ±å‘Šç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥åˆ†æçµæœ"],
                execution_time=execution_time
            )
    
    def _create_evaluation_report(self, comparison_result: Dict[str, Any], 
                                 gap_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """å‰µå»ºè©•ä¼°å ±å‘Š"""
        
        overall_score = comparison_result.get("overall_score", 0.5)
        gap_level = gap_analysis.get("gap_level", "medium")
        priority_areas = gap_analysis.get("priority_areas", [])
        improvement_potential = gap_analysis.get("improvement_potential", "æœªçŸ¥")
        
        return {
            "executive_summary": {
                "overall_score": overall_score,
                "gap_level": gap_level,
                "priority_areas": priority_areas,
                "improvement_potential": improvement_potential
            },
            "detailed_analysis": {
                "performance_analysis": comparison_result.get("performance_comparison", {}),
                "capability_analysis": comparison_result.get("capability_comparison", {}),
                "quality_analysis": comparison_result.get("quality_comparison", {})
            },
            "risk_assessment": self._assess_risks(gap_level, comparison_result),
            "success_metrics": self._define_success_metrics()
        }
    
    def _assess_risks(self, gap_level: str, comparison_result: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°é¢¨éšª"""
        
        risk_level_map = {
            "low": "ä½",
            "medium": "ä¸­", 
            "high": "é«˜"
        }
        
        overall_risk_level = risk_level_map.get(gap_level, "ä¸­")
        
        return {
            "overall_risk": {
                "level": overall_risk_level,
                "description": "éœ€è¦é—œæ³¨ï¼Œå¯èƒ½å½±éŸ¿æ€§èƒ½" if gap_level == "medium" else "é¢¨éšªå¯æ§"
            },
            "specific_risks": [
                {"area": "æ€§èƒ½", "level": "ä½", "mitigation": "æŒçºŒç›£æ§"},
                {"area": "å…¼å®¹æ€§", "level": "ä¸­", "mitigation": "å¢é‡æ¸¬è©¦"},
                {"area": "ç¶­è­·æ€§", "level": "ä½", "mitigation": "ä»£ç¢¼é‡æ§‹"}
            ]
        }
    
    def _define_success_metrics(self) -> Dict[str, Any]:
        """å®šç¾©æˆåŠŸæŒ‡æ¨™"""
        
        return {
            "performance_targets": {
                "response_time": "< 150ms",
                "success_rate": "> 98%",
                "error_rate": "< 0.5%"
            },
            "quality_targets": {
                "code_quality": "Aç´š",
                "test_coverage": "> 85%",
                "documentation": "å®Œæ•´"
            },
            "timeline_targets": {
                "implementation": "1-2é€±",
                "testing": "3-5å¤©",
                "deployment": "1-2å¤©"
            }
        }
    
    def _generate_fix_recommendations(self, gap_analysis: Dict[str, Any], 
                                    comparison_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¿®å¾©å»ºè­°"""
        
        recommendations = []
        gap_level = gap_analysis.get("gap_level", "medium")
        
        if gap_level == "high":
            recommendations.extend([
                {
                    "type": "critical_fix",
                    "priority": "high",
                    "description": "ä¿®å¾©é—œéµæ€§èƒ½å•é¡Œ",
                    "estimated_effort": "2-3å¤©",
                    "strategy": "aggressive"
                },
                {
                    "type": "architecture_improvement",
                    "priority": "high", 
                    "description": "é‡æ§‹æ ¸å¿ƒæ¶æ§‹",
                    "estimated_effort": "1é€±",
                    "strategy": "kilocode_fallback"
                }
            ])
        elif gap_level == "medium":
            recommendations.extend([
                {
                    "type": "incremental_improvement",
                    "priority": "medium",
                    "description": "å¢é‡æ€§èƒ½å„ªåŒ–",
                    "estimated_effort": "1-2å¤©",
                    "strategy": "intelligent"
                }
            ])
        
        return recommendations
    
    def _create_implementation_plan(self, recommendations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å‰µå»ºå¯¦æ–½è¨ˆåŠƒ"""
        
        phases = []
        total_effort = "1-2é€±"
        
        for i, rec in enumerate(recommendations, 1):
            phases.append({
                "phase": i,
                "title": rec.get("description", f"éšæ®µ{i}"),
                "priority": rec.get("priority", "medium"),
                "estimated_effort": rec.get("estimated_effort", "å¾…è©•ä¼°"),
                "strategy": rec.get("strategy", "intelligent")
            })
        
        return {
            "phases": phases,
            "total_phases": len(phases),
            "estimated_total_effort": total_effort,
            "critical_path": [p["title"] for p in phases if p["priority"] == "high"],
            "resource_requirements": {
                "developers": 1,
                "testers": 1,
                "reviewers": 1
            }
        }

class IntegratedCodeFixAdapter:
    """æ•´åˆçš„ä»£ç¢¼ä¿®å¾©é©é…å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.fix_strategies = config.get("fix.strategies", ["intelligent"])
    
    async def execute_fix(self, evaluation_result: ProcessingResult) -> ProcessingResult:
        """åŸ·è¡Œä»£ç¢¼ä¿®å¾©"""
        
        start_time = time.time()
        self.logger.info("é–‹å§‹Code Fix Adapterè™•ç†")
        
        try:
            evaluation_data = evaluation_result.data
            requirement_id = evaluation_data.get("requirement_id", "unknown")
            fix_recommendations = evaluation_data.get("fix_recommendations", [])
            
            fix_results = []
            
            # åŸ·è¡Œä¿®å¾©å»ºè­°
            for recommendation in fix_recommendations:
                fix_result = await self._execute_single_fix(recommendation, requirement_id)
                fix_results.append(fix_result)
            
            # ç¢ºå®šæ•´é«”ä¿®å¾©ç‹€æ…‹
            overall_status = self._determine_overall_status(fix_results)
            
            fix_data = {
                "requirement_id": requirement_id,
                "fix_results": fix_results,
                "overall_fix_status": overall_status,
                "fix_timestamp": datetime.now().isoformat(),
                "kilocode_integration": True
            }
            
            execution_time = time.time() - start_time
            
            return ProcessingResult(
                stage=ProcessingStage.CODE_FIX,
                success=True,
                data=fix_data,
                confidence_score=1.0,
                recommendations=[
                    "ä¿®å¾©éç¨‹ä¸­é‡åˆ°å•é¡Œï¼Œå»ºè­°è©³ç´°æª¢æŸ¥" if overall_status == "partial" else "ä¿®å¾©å®Œæˆ",
                    "å»ºè­°é€²è¡Œå…¨é¢æ¸¬è©¦é©—è­‰",
                    "ç›£æ§ä¿®å¾©å¾Œçš„ç³»çµ±æ€§èƒ½",
                    "æº–å‚™å›æ»¾è¨ˆåŠƒä»¥é˜²æ„å¤–"
                ],
                execution_time=execution_time,
                next_stage=ProcessingStage.COMPLETED
            )
            
        except Exception as e:
            self.logger.error(f"ä»£ç¢¼ä¿®å¾©å¤±æ•—: {e}")
            execution_time = time.time() - start_time
            
            return ProcessingResult(
                stage=ProcessingStage.CODE_FIX,
                success=False,
                data={"error": str(e)},
                confidence_score=0.0,
                recommendations=["ä»£ç¢¼ä¿®å¾©å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¿®å¾©å»ºè­°"],
                execution_time=execution_time
            )
    
    async def _execute_single_fix(self, recommendation: Dict[str, Any], 
                                 requirement_id: str) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹ä¿®å¾©"""
        
        strategy = recommendation.get("strategy", "intelligent")
        fix_type = recommendation.get("type", "unknown")
        
        # æ¨¡æ“¬ä¿®å¾©åŸ·è¡Œ
        await asyncio.sleep(0.1)
        
        if strategy == "kilocode_fallback":
            return await self._kilocode_fallback_fix(recommendation)
        else:
            return await self._standard_fix(recommendation, strategy)
    
    async def _standard_fix(self, recommendation: Dict[str, Any], 
                           strategy: str) -> Dict[str, Any]:
        """æ¨™æº–ä¿®å¾©"""
        
        return {
            "fix_id": f"fix_{int(time.time())}",
            "type": recommendation.get("type", "unknown"),
            "strategy": strategy,
            "status": "completed",
            "confidence": 0.85,
            "changes_made": [
                f"æ‡‰ç”¨{strategy}ç­–ç•¥ä¿®å¾©",
                "æ›´æ–°ç›¸é—œé…ç½®",
                "é©—è­‰ä¿®å¾©æ•ˆæœ"
            ]
        }
    
    async def _kilocode_fallback_fix(self, recommendation: Dict[str, Any]) -> Dict[str, Any]:
        """KiloCodeå…œåº•ä¿®å¾©"""
        
        return {
            "fix_id": f"kilocode_fix_{int(time.time())}",
            "type": recommendation.get("type", "unknown"),
            "strategy": "kilocode_fallback",
            "status": "completed",
            "confidence": 1.0,
            "changes_made": [
                "ä½¿ç”¨KiloCodeå¼•æ“é‡æ–°å‰µå»º",
                "æ‡‰ç”¨æœ€ä½³å¯¦è¸æ¨¡å¼",
                "é›†æˆç¾æœ‰ç³»çµ±"
            ],
            "kilocode_features": [
                "å·¥ä½œæµæ„ŸçŸ¥å‰µå»º",
                "æ™ºèƒ½ä»£ç¢¼ç”Ÿæˆ",
                "è‡ªå‹•æ¸¬è©¦é›†æˆ"
            ]
        }
    
    def _determine_overall_status(self, fix_results: List[Dict[str, Any]]) -> str:
        """ç¢ºå®šæ•´é«”ä¿®å¾©ç‹€æ…‹"""
        
        if not fix_results:
            return "no_fixes_needed"
        
        completed_fixes = [r for r in fix_results if r.get("status") == "completed"]
        
        if len(completed_fixes) == len(fix_results):
            return "all_completed"
        elif len(completed_fixes) > 0:
            return "partial_completed"
        else:
            return "failed"

class EnhancedTestFlowMCPv5:
    """Enhanced Test Flow MCP v5.0 - é›†æˆå¢å¼·å ±å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {
            "manus_api.base_url": "https://manus.chat",
            "comparison.threshold": 0.7,
            "fix.strategies": ["conservative", "aggressive", "intelligent", "kilocode_fallback"]
        }
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("Enhanced Test Flow MCP v5.0 åˆå§‹åŒ–å®Œæˆ (é›†æˆå¢å¼·å ±å‘Šç”Ÿæˆå™¨)")
        
        # åˆå§‹åŒ–å„å€‹å¼•æ“
        self.requirement_sync_engine = RequirementSyncEngine(self.config)
        self.comparison_analysis_engine = ComparisonAnalysisEngine(self.config)
        self.evaluation_report_generator = EvaluationReportGenerator(self.config)
        self.code_fix_adapter = IntegratedCodeFixAdapter(self.config)
        
        # åˆå§‹åŒ–å¢å¼·å ±å‘Šç”Ÿæˆå™¨
        self.enhanced_report_generator = EnhancedSummaryReportGenerator()
        
        # è™•ç†æ­·å²
        self.processing_history = []
    
    async def process_developer_request(self, requirement: str, 
                                      mode: str = "developer",
                                      manus_context: Optional[Dict[str, Any]] = None,
                                      fix_strategy: str = "intelligent") -> Dict[str, Any]:
        """è™•ç†é–‹ç™¼è€…è«‹æ±‚ï¼ˆä¸»è¦å…¥å£ï¼‰"""
        
        try:
            # å‰µå»ºè«‹æ±‚å°è±¡
            request = DeveloperRequest(
                requirement=requirement,
                mode=UserMode.DEVELOPER if mode == "developer" else UserMode.USER,
                manus_context=manus_context,
                fix_strategy=FixStrategy(fix_strategy)
            )
            
            # åŸ·è¡Œå·¥ä½œæµ
            workflow_result = await self._execute_developer_workflow(request)
            
            # ç”Ÿæˆå¢å¼·å ±å‘Š
            enhanced_report = await self._generate_enhanced_report(request, workflow_result)
            
            # è¨˜éŒ„è™•ç†æ­·å²
            self._record_processing_history(request, workflow_result)
            
            return {
                "success": True,
                "workflow_result": workflow_result,
                "enhanced_report": enhanced_report,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"é–‹ç™¼è€…æ¨¡å¼è™•ç†å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def _execute_developer_workflow(self, request: DeveloperRequest) -> Dict[str, Any]:
        """åŸ·è¡Œé–‹ç™¼è€…æ¨¡å¼å·¥ä½œæµ"""
        
        self.logger.info("é–‹å§‹åŸ·è¡Œé–‹ç™¼è€…æ¨¡å¼å››éšæ®µè™•ç†æµç¨‹")
        
        workflow_results = {}
        
        # éšæ®µ1: éœ€æ±‚åŒæ­¥å¼•æ“
        self.logger.info("éšæ®µ1: éœ€æ±‚åŒæ­¥å¼•æ“")
        sync_result = await self.requirement_sync_engine.sync_requirement(request)
        workflow_results["requirement_sync"] = sync_result
        
        if not sync_result.success:
            return {"stage": "requirement_sync", "error": "éœ€æ±‚åŒæ­¥å¤±æ•—", "results": workflow_results}
        
        # éšæ®µ2: æ¯”è¼ƒåˆ†æå¼•æ“
        self.logger.info("éšæ®µ2: æ¯”è¼ƒåˆ†æå¼•æ“")
        analysis_result = await self.comparison_analysis_engine.analyze_comparison(sync_result)
        workflow_results["comparison_analysis"] = analysis_result
        
        if not analysis_result.success:
            return {"stage": "comparison_analysis", "error": "æ¯”è¼ƒåˆ†æå¤±æ•—", "results": workflow_results}
        
        # éšæ®µ3: è©•ä¼°å ±å‘Šç”Ÿæˆå™¨
        self.logger.info("éšæ®µ3: è©•ä¼°å ±å‘Šç”Ÿæˆå™¨")
        evaluation_result = await self.evaluation_report_generator.generate_report(analysis_result)
        workflow_results["evaluation_report"] = evaluation_result
        
        if not evaluation_result.success:
            return {"stage": "evaluation_report", "error": "è©•ä¼°å ±å‘Šç”Ÿæˆå¤±æ•—", "results": workflow_results}
        
        # éšæ®µ4: Code Fix Adapter
        self.logger.info("éšæ®µ4: Code Fix Adapter")
        fix_result = await self.code_fix_adapter.execute_fix(evaluation_result)
        workflow_results["code_fix"] = fix_result
        
        return {
            "stage": "completed",
            "success": True,
            "results": workflow_results,
            "final_status": fix_result.data.get("overall_fix_status", "unknown")
        }
    
    async def _generate_enhanced_report(self, request: DeveloperRequest, 
                                      workflow_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¢å¼·å ±å‘Š"""
        
        # æº–å‚™å ±å‘Šæ•¸æ“š
        test_metadata = {
            "timestamp": datetime.now().isoformat(),
            "version": "Enhanced Test Flow MCP v5.0",
            "test_scenario": "é–‹ç™¼è€…æ¨¡å¼è™•ç†",
            "mode": request.mode.value
        }
        
        test_request_data = {
            "requirement": request.requirement,
            "mode": request.mode.value,
            "manus_context": request.manus_context,
            "fix_strategy": request.fix_strategy.value
        }
        
        # è½‰æ›è™•ç†çµæœæ ¼å¼
        processing_results = {}
        if "results" in workflow_result:
            for stage_name, stage_result in workflow_result["results"].items():
                if hasattr(stage_result, '__dict__'):
                    processing_results[stage_name] = {
                        "stage": stage_result.stage.value,
                        "success": stage_result.success,
                        "data": stage_result.data,
                        "confidence_score": stage_result.confidence_score,
                        "recommendations": stage_result.recommendations,
                        "execution_time": stage_result.execution_time
                    }
        
        # ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨ï¼ˆæ¨¡æ“¬ï¼‰
        generated_files = [
            "enhanced_test_flow_mcp_v5_complete_report.json",
            "enhanced_test_flow_mcp_v5_summary.md",
            "enhanced_test_flow_mcp_v5_testing_guide.md",
            "enhanced_test_flow_mcp_v5_reports.tar.gz"
        ]
        
        # ç”Ÿæˆå¢å¼·å ±å‘Š
        enhanced_report = self.enhanced_report_generator.generate_comprehensive_report(
            test_metadata, test_request_data, processing_results, generated_files
        )
        
        return enhanced_report
    
    def _record_processing_history(self, request: DeveloperRequest, result: Dict[str, Any]):
        """è¨˜éŒ„è™•ç†æ­·å²"""
        
        history_entry = {
            "timestamp": datetime.now().isoformat(),
            "requirement": request.requirement,
            "mode": request.mode.value,
            "success": result.get("success", False),
            "stage": result.get("stage", "unknown"),
            "final_status": result.get("final_status", "unknown")
        }
        
        self.processing_history.append(history_entry)
        
        # ä¿æŒæœ€è¿‘100æ¢è¨˜éŒ„
        if len(self.processing_history) > 100:
            self.processing_history = self.processing_history[-100:]
    
    async def get_capabilities(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±èƒ½åŠ›"""
        
        return {
            "version": "5.0.0",
            "components": {
                "requirement_sync_engine": "æ´»èº",
                "comparison_analysis_engine": "æ´»èº", 
                "evaluation_report_generator": "æ´»èº",
                "code_fix_adapter": "æ´»èº",
                "enhanced_report_generator": "æ´»èº"
            },
            "supported_modes": ["developer", "user"],
            "fix_strategies": ["conservative", "aggressive", "intelligent", "kilocode_fallback"],
            "features": [
                "å››éšæ®µé–‹ç™¼è€…æ¨¡å¼è™•ç†",
                "Manusæ·±åº¦æ•´åˆ",
                "KiloCodeå…œåº•ä¿®å¾©",
                "å¢å¼·å ±å‘Šç”Ÿæˆ",
                "å¯¦æ™‚æ§åˆ¶å°è¼¸å‡º"
            ]
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        
        return {
            "status": "healthy",
            "version": "5.0.0",
            "timestamp": datetime.now().isoformat(),
            "components": {
                "requirement_sync_engine": "ok",
                "comparison_analysis_engine": "ok",
                "evaluation_report_generator": "ok", 
                "code_fix_adapter": "ok",
                "enhanced_report_generator": "ok"
            },
            "processing_history_count": len(self.processing_history)
        }

# ä¾¿æ·å‡½æ•¸
async def run_enhanced_test_flow_v5(requirement: str,
                                   mode: str = "developer", 
                                   manus_context: Optional[Dict[str, Any]] = None,
                                   fix_strategy: str = "intelligent") -> str:
    """é‹è¡ŒEnhanced Test Flow MCP v5.0ä¸¦è¿”å›å¢å¼·å ±å‘Š"""
    
    mcp = EnhancedTestFlowMCPv5()
    result = await mcp.process_developer_request(requirement, mode, manus_context, fix_strategy)
    
    if result.get("success"):
        return result.get("enhanced_report", "å ±å‘Šç”Ÿæˆå¤±æ•—")
    else:
        return f"è™•ç†å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}"

# ä¸»å‡½æ•¸
async def main():
    """ä¸»å‡½æ•¸ - ç”¨æ–¼æ¸¬è©¦"""
    
    print("ğŸš€ Enhanced Test Flow MCP v5.0 å•Ÿå‹•ä¸­...")
    
    # å‰µå»ºæ¸¬è©¦è«‹æ±‚
    test_requirement = "VSIXæ–‡ä»¶å®‰è£å¤±æ•—ï¼ŒéŒ¯èª¤ä¿¡æ¯é¡¯ç¤ºæš«ä¸æ”¯æŒå®‰è£æ’ä»¶çš„3.0.0ç‰ˆæœ¬ï¼Œéœ€è¦å‰µå»ºå…¼å®¹æ›´å¤šVSCodeç‰ˆæœ¬çš„VSIXæ–‡ä»¶"
    test_context = {
        "project_type": "vscode_extension",
        "complexity": "medium",
        "error_type": "compatibility_issue",
        "target_platform": "VSCode 1.x+"
    }
    
    # é‹è¡Œæ¸¬è©¦
    enhanced_report = await run_enhanced_test_flow_v5(
        requirement=test_requirement,
        mode="developer",
        manus_context=test_context,
        fix_strategy="intelligent"
    )
    
    # è¼¸å‡ºå¢å¼·å ±å‘Š
    print(enhanced_report)

if __name__ == "__main__":
    asyncio.run(main())

