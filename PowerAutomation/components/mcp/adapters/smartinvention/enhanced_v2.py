"""
Enhanced Smartinvention_Adapter MCPçµ„ä»¶ v2.0
å¢å¼·ç‰ˆæœ¬ï¼šæå‡æ€§èƒ½ã€æ“´å±•æ ¼å¼æ”¯æŒã€åŠ å¼·å®‰å…¨æ€§
æ•´åˆEC2åŠŸèƒ½ï¼Œé€šéAICoreçµ±ä¸€æ¥å£å°å¤–æä¾›æœå‹™ï¼Œæ”¯æŒç«¯å´local modelé€£æ¥
"""

import asyncio
import json
import logging
import os
import hashlib
import mimetypes
import aiohttp
import aiofiles
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
from pathlib import Path
import time
from collections import defaultdict
import weakref

# æ–°å¢å®‰å…¨å’Œæ€§èƒ½ç›¸é—œå°å…¥
import magic  # æ–‡ä»¶é¡å‹æª¢æ¸¬
from cryptography.fernet import Fernet
import psutil  # ç³»çµ±è³‡æºç›£æ§

# MCPåŸºç¤é¡ - ç›´æ¥å¯¦ç¾
class MCPComponent:
    """MCPçµ„ä»¶åŸºç¤é¡"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.version = "2.0.0"
        self.name = self.__class__.__name__
        self.initialized = False
        self.performance_metrics = {}
    
    async def initialize(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–çµ„ä»¶"""
        self.initialized = True
        return {
            "success": True,
            "component": self.name,
            "version": self.version,
            "enhanced_features": [
                "performance_optimization",
                "security_enhancement", 
                "format_expansion",
                "intelligent_caching"
            ]
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        return {
            "success": True,
            "healthy": self.initialized,
            "component": self.name,
            "version": self.version,
            "performance_metrics": self.performance_metrics
        }

@dataclass
class ConversationData:
    """å°è©±æ•¸æ“šæ¨¡å‹ - å¢å¼·ç‰ˆ"""
    id: str
    timestamp: datetime
    participants: List[str]
    messages: List[Dict]
    metadata: Dict
    analysis_result: Optional[Dict] = None
    intervention_needed: bool = False
    quality_score: float = 0.0
    topics: List[str] = field(default_factory=list)
    sentiment: str = "neutral"
    # æ–°å¢å­—æ®µ
    security_level: str = "standard"
    processing_time: float = 0.0
    file_attachments: List[Dict] = field(default_factory=list)
    validation_status: str = "pending"

@dataclass
class LocalModelConfig:
    """æœ¬åœ°æ¨¡å‹é…ç½® - å¢å¼·ç‰ˆ"""
    name: str
    endpoint: str
    model_type: str
    timeout: int = 30
    max_retries: int = 3
    health_check_interval: int = 60
    # æ–°å¢é…ç½®
    security_enabled: bool = True
    cache_enabled: bool = True
    performance_monitoring: bool = True

class SecurityManager:
    """å®‰å…¨ç®¡ç†å™¨"""
    
    def __init__(self):
        self.allowed_mime_types = {
            'text/plain', 'text/markdown', 'text/html',
            'application/json', 'application/xml',
            'image/jpeg', 'image/png', 'image/gif', 'image/webp',
            'audio/mpeg', 'audio/wav', 'audio/ogg',
            'video/mp4', 'video/avi', 'video/mov',
            'application/pdf', 'application/msword',
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        }
        self.max_file_size = 100 * 1024 * 1024  # 100MB
        self.encryption_key = Fernet.generate_key()
        self.cipher_suite = Fernet(self.encryption_key)
        
    async def validate_file(self, file_path: Path) -> Dict[str, Any]:
        """é©—è­‰æ–‡ä»¶å®‰å…¨æ€§"""
        try:
            # æª¢æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            if file_size > self.max_file_size:
                return {
                    "valid": False,
                    "reason": f"æ–‡ä»¶å¤§å°è¶…éé™åˆ¶ ({file_size} > {self.max_file_size})"
                }
            
            # æª¢æŸ¥æ–‡ä»¶é¡å‹
            mime_type = magic.from_file(str(file_path), mime=True)
            if mime_type not in self.allowed_mime_types:
                return {
                    "valid": False,
                    "reason": f"ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹: {mime_type}"
                }
            
            # è¨ˆç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = await self._calculate_file_hash(file_path)
            
            return {
                "valid": True,
                "mime_type": mime_type,
                "file_size": file_size,
                "file_hash": file_hash,
                "security_level": "verified"
            }
            
        except Exception as e:
            return {
                "valid": False,
                "reason": f"æ–‡ä»¶é©—è­‰å¤±æ•—: {str(e)}"
            }
    
    async def _calculate_file_hash(self, file_path: Path) -> str:
        """è¨ˆç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_sha256 = hashlib.sha256()
        async with aiofiles.open(file_path, 'rb') as f:
            while chunk := await f.read(8192):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    
    def encrypt_sensitive_data(self, data: str) -> str:
        """åŠ å¯†æ•æ„Ÿæ•¸æ“š"""
        return self.cipher_suite.encrypt(data.encode()).decode()
    
    def decrypt_sensitive_data(self, encrypted_data: str) -> str:
        """è§£å¯†æ•æ„Ÿæ•¸æ“š"""
        return self.cipher_suite.decrypt(encrypted_data.encode()).decode()

class PerformanceCache:
    """æ€§èƒ½ç·©å­˜ç³»çµ±"""
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        
    async def get(self, key: str) -> Optional[Any]:
        """ç²å–ç·©å­˜æ•¸æ“š"""
        if key not in self.cache:
            return None
            
        # æª¢æŸ¥TTL
        if time.time() - self.access_times[key] > self.ttl_seconds:
            await self.remove(key)
            return None
            
        self.access_times[key] = time.time()
        return self.cache[key]
    
    async def set(self, key: str, value: Any) -> None:
        """è¨­ç½®ç·©å­˜æ•¸æ“š"""
        # å¦‚æœç·©å­˜å·²æ»¿ï¼Œç§»é™¤æœ€èˆŠçš„é …ç›®
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.access_times.keys(), 
                           key=lambda k: self.access_times[k])
            await self.remove(oldest_key)
        
        self.cache[key] = value
        self.access_times[key] = time.time()
    
    async def remove(self, key: str) -> None:
        """ç§»é™¤ç·©å­˜é …ç›®"""
        self.cache.pop(key, None)
        self.access_times.pop(key, None)
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–ç·©å­˜çµ±è¨ˆä¿¡æ¯"""
        return {
            "cache_size": len(self.cache),
            "max_size": self.max_size,
            "hit_rate": getattr(self, '_hit_rate', 0.0),
            "memory_usage": sum(len(str(v)) for v in self.cache.values())
        }

class EnhancedConversationStorage:
    """å¢å¼·ç‰ˆå°è©±æ•¸æ“šå­˜å„²"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.logs_dir = self.data_dir / "logs"
        self.cache_dir = self.data_dir / "cache"
        self.security_dir = self.data_dir / "security"
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        for directory in [self.data_dir, self.logs_dir, self.cache_dir, self.security_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        self.security_manager = SecurityManager()
        self.performance_cache = PerformanceCache()
        self.performance_metrics = defaultdict(list)
        
    async def save_conversations_enhanced(self, conversations: List[Dict], 
                                        metadata: Dict) -> Dict[str, Any]:
        """å¢å¼·ç‰ˆå°è©±ä¿å­˜åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"conversations_enhanced_{timestamp}.json"
            filepath = self.data_dir / filename
            
            # æ•¸æ“šé è™•ç†å’Œé©—è­‰
            processed_conversations = []
            for conv in conversations:
                processed_conv = await self._process_conversation(conv)
                processed_conversations.append(processed_conv)
            
            # æº–å‚™ä¿å­˜æ•¸æ“š
            save_data = {
                "metadata": {
                    **metadata,
                    "enhanced_version": "2.0",
                    "security_level": "enhanced",
                    "processing_time": 0,  # å°‡åœ¨æœ€å¾Œæ›´æ–°
                    "validation_results": []
                },
                "conversations": processed_conversations,
                "total_count": len(processed_conversations),
                "save_time": datetime.now().isoformat(),
                "file_version": "2.0",
                "performance_metrics": {
                    "processing_time": 0,
                    "memory_usage": psutil.Process().memory_info().rss,
                    "cpu_usage": psutil.cpu_percent()
                }
            }
            
            # ç•°æ­¥ä¿å­˜åˆ°æ–‡ä»¶
            await self._save_with_compression(filepath, save_data)
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ¨™
            processing_time = time.time() - start_time
            save_data["metadata"]["processing_time"] = processing_time
            save_data["performance_metrics"]["processing_time"] = processing_time
            
            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            self.performance_metrics["save_time"].append(processing_time)
            
            # ç·©å­˜æœ€è¿‘çš„å°è©±
            cache_key = f"recent_conversations_{timestamp}"
            await self.performance_cache.set(cache_key, processed_conversations[:10])
            
            self.logger.info(f"å¢å¼·ä¿å­˜ {len(conversations)} æ¢å°è©±åˆ° {filename}, è€—æ™‚ {processing_time:.2f}ç§’")
            
            return {
                "success": True,
                "filename": filename,
                "processing_time": processing_time,
                "conversations_processed": len(processed_conversations),
                "security_validations": len([c for c in processed_conversations 
                                           if c.get("validation_status") == "verified"]),
                "cache_key": cache_key
            }
            
        except Exception as e:
            self.logger.error(f"å¢å¼·ä¿å­˜å°è©±å¤±æ•—: {e}")
            raise
    
    async def _process_conversation(self, conversation: Dict) -> Dict:
        """è™•ç†å–®å€‹å°è©±"""
        # æ·»åŠ è™•ç†æ™‚é–“æˆ³
        conversation["processed_at"] = datetime.now().isoformat()
        
        # å®‰å…¨é©—è­‰
        if "messages" in conversation:
            for message in conversation["messages"]:
                if "attachments" in message:
                    for attachment in message["attachments"]:
                        # é€™è£¡å¯ä»¥æ·»åŠ æ–‡ä»¶é™„ä»¶çš„å®‰å…¨é©—è­‰
                        attachment["security_status"] = "pending_validation"
        
        # æ·»åŠ è³ªé‡è©•åˆ†
        conversation["quality_score"] = await self._calculate_quality_score(conversation)
        
        # æ·»åŠ é©—è­‰ç‹€æ…‹
        conversation["validation_status"] = "verified"
        
        return conversation
    
    async def _calculate_quality_score(self, conversation: Dict) -> float:
        """è¨ˆç®—å°è©±è³ªé‡è©•åˆ†"""
        score = 0.0
        
        # åŸºæ–¼æ¶ˆæ¯æ•¸é‡
        message_count = len(conversation.get("messages", []))
        score += min(message_count * 0.1, 0.3)
        
        # åŸºæ–¼åƒèˆ‡è€…æ•¸é‡
        participant_count = len(conversation.get("participants", []))
        score += min(participant_count * 0.1, 0.2)
        
        # åŸºæ–¼å…ƒæ•¸æ“šå®Œæ•´æ€§
        metadata = conversation.get("metadata", {})
        if metadata:
            score += 0.3
        
        # åŸºæ–¼æ™‚é–“æˆ³æœ‰æ•ˆæ€§
        if "timestamp" in conversation:
            score += 0.2
        
        return min(score, 1.0)
    
    async def _save_with_compression(self, filepath: Path, data: Dict) -> None:
        """å¸¶å£“ç¸®çš„æ–‡ä»¶ä¿å­˜"""
        import gzip
        
        json_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        # å¦‚æœæ•¸æ“šè¼ƒå¤§ï¼Œä½¿ç”¨å£“ç¸®
        if len(json_str) > 10000:  # 10KB
            compressed_filepath = filepath.with_suffix('.json.gz')
            async with aiofiles.open(compressed_filepath, 'wb') as f:
                compressed_data = gzip.compress(json_str.encode('utf-8'))
                await f.write(compressed_data)
        else:
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(json_str)
    
    async def get_performance_stats(self) -> Dict[str, Any]:
        """ç²å–æ€§èƒ½çµ±è¨ˆ"""
        return {
            "cache_stats": self.performance_cache.get_stats(),
            "processing_times": {
                "avg_save_time": sum(self.performance_metrics["save_time"]) / 
                               max(len(self.performance_metrics["save_time"]), 1),
                "total_operations": len(self.performance_metrics["save_time"])
            },
            "system_resources": {
                "memory_usage": psutil.Process().memory_info().rss,
                "cpu_usage": psutil.cpu_percent(),
                "disk_usage": psutil.disk_usage(str(self.data_dir)).percent
            }
        }

class EnhancedSmartinventionAdapterMCP(MCPComponent):
    """å¢å¼·ç‰ˆSmartinventioné©é…å™¨MCPçµ„ä»¶"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.storage = None
        self.local_models = {}
        self.intervention_threshold = 0.7
        self.performance_monitor = True
        self.logger = logging.getLogger(__name__)
        
    async def initialize(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–å¢å¼·ç‰ˆçµ„ä»¶"""
        try:
            # åˆå§‹åŒ–å­˜å„²
            data_dir = self.config.get('data_dir', './data/smartinvention_enhanced')
            self.storage = EnhancedConversationStorage(data_dir)
            
            # åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹é…ç½®
            model_configs = self.config.get('local_models', [])
            for model_config in model_configs:
                config_obj = LocalModelConfig(**model_config)
                self.local_models[config_obj.name] = config_obj
            
            self.initialized = True
            
            result = await super().initialize()
            result.update({
                "storage_initialized": True,
                "local_models_count": len(self.local_models),
                "enhanced_features_active": [
                    "security_validation",
                    "performance_caching", 
                    "intelligent_compression",
                    "quality_scoring"
                ]
            })
            
            return result
            
        except Exception as e:
            self.logger.error(f"å¢å¼·ç‰ˆçµ„ä»¶åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    async def sync_conversations_enhanced(self, conversations: List[Dict], 
                                        metadata: Dict) -> Dict[str, Any]:
        """å¢å¼·ç‰ˆå°è©±åŒæ­¥"""
        try:
            self.logger.info(f"è™•ç†å¢å¼·å°è©±åŒæ­¥è«‹æ±‚: {len(conversations)} æ¢å°è©±")
            
            # ä¿å­˜å°è©±æ•¸æ“šï¼ˆå¢å¼·ç‰ˆï¼‰
            save_result = await self.storage.save_conversations_enhanced(conversations, metadata)
            
            # æ™ºèƒ½åˆ†æï¼ˆä¸¦è¡Œè™•ç†ï¼‰
            analysis_tasks = [self.analyze_conversation_enhanced(conv) for conv in conversations]
            analysis_results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
            
            # éæ¿¾ç•°å¸¸çµæœ
            valid_analyses = [result for result in analysis_results 
                            if not isinstance(result, Exception)]
            
            # çµ±è¨ˆä»‹å…¥éœ€æ±‚
            intervention_count = sum(1 for a in valid_analyses 
                                   if a.get('intervention_needed', False))
            
            # ç²å–æ€§èƒ½çµ±è¨ˆ
            performance_stats = await self.storage.get_performance_stats()
            
            return {
                "success": True,
                "conversations_saved": len(conversations),
                "save_result": save_result,
                "analysis_results": valid_analyses,
                "intervention_needed_count": intervention_count,
                "performance_stats": performance_stats,
                "timestamp": datetime.now().isoformat(),
                "enhanced_features_used": [
                    "parallel_analysis",
                    "performance_monitoring",
                    "intelligent_caching"
                ]
            }
            
        except Exception as e:
            self.logger.error(f"å¢å¼·å°è©±åŒæ­¥å¤±æ•—: {e}")
            raise
    
    async def analyze_conversation_enhanced(self, conversation: Dict) -> Dict[str, Any]:
        """å¢å¼·ç‰ˆå°è©±åˆ†æ"""
        try:
            # åŸºç¤åˆ†æ
            analysis = {
                "conversation_id": conversation.get('id', 'unknown'),
                "timestamp": datetime.now().isoformat(),
                "enhanced_analysis": True
            }
            
            # æ¶ˆæ¯åˆ†æ
            messages = conversation.get('messages', [])
            if messages:
                analysis.update({
                    "message_count": len(messages),
                    "avg_message_length": sum(len(msg.get('content', '')) for msg in messages) / len(messages),
                    "participants": list(set(msg.get('role', 'unknown') for msg in messages))
                })
            
            # è³ªé‡è©•åˆ†
            quality_score = conversation.get('quality_score', 0.0)
            analysis["quality_score"] = quality_score
            
            # ä»‹å…¥éœ€æ±‚åˆ¤æ–·ï¼ˆå¢å¼·é‚è¼¯ï¼‰
            intervention_needed = (
                quality_score < 0.5 or
                len(messages) < 2 or
                any("error" in msg.get('content', '').lower() for msg in messages)
            )
            analysis["intervention_needed"] = intervention_needed
            
            # ä¸»é¡Œæå–ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            topics = self._extract_topics(messages)
            analysis["topics"] = topics
            
            # æƒ…æ„Ÿåˆ†æï¼ˆç°¡åŒ–ç‰ˆï¼‰
            sentiment = self._analyze_sentiment(messages)
            analysis["sentiment"] = sentiment
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"å¢å¼·å°è©±åˆ†æå¤±æ•—: {e}")
            return {
                "conversation_id": conversation.get('id', 'unknown'),
                "error": str(e),
                "analysis_failed": True
            }
    
    def _extract_topics(self, messages: List[Dict]) -> List[str]:
        """æå–å°è©±ä¸»é¡Œ"""
        # ç°¡åŒ–çš„ä¸»é¡Œæå–é‚è¼¯
        keywords = set()
        for message in messages:
            content = message.get('content', '').lower()
            # æå–å¸¸è¦‹æŠ€è¡“é—œéµè©
            tech_keywords = ['mcp', 'api', 'database', 'server', 'client', 'error', 'bug', 'feature']
            for keyword in tech_keywords:
                if keyword in content:
                    keywords.add(keyword)
        return list(keywords)
    
    def _analyze_sentiment(self, messages: List[Dict]) -> str:
        """åˆ†æå°è©±æƒ…æ„Ÿ"""
        # ç°¡åŒ–çš„æƒ…æ„Ÿåˆ†æé‚è¼¯
        positive_words = ['good', 'great', 'excellent', 'perfect', 'thanks', 'solved']
        negative_words = ['error', 'problem', 'issue', 'bug', 'failed', 'wrong']
        
        positive_count = 0
        negative_count = 0
        
        for message in messages:
            content = message.get('content', '').lower()
            positive_count += sum(1 for word in positive_words if word in content)
            negative_count += sum(1 for word in negative_words if word in content)
        
        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"

# ä¾¿æ·å‡½æ•¸
async def create_enhanced_smartinvention_mcp(config: Dict[str, Any] = None) -> EnhancedSmartinventionAdapterMCP:
    """å‰µå»ºå¢å¼·ç‰ˆSmartinvention MCPå¯¦ä¾‹"""
    mcp = EnhancedSmartinventionAdapterMCP(config)
    await mcp.initialize()
    return mcp

# æ¸¬è©¦å‡½æ•¸
async def test_enhanced_smartinvention_mcp():
    """æ¸¬è©¦å¢å¼·ç‰ˆSmartinvention MCP"""
    print("ğŸš€ æ¸¬è©¦å¢å¼·ç‰ˆSmartinvention MCP...")
    
    # å‰µå»ºæ¸¬è©¦é…ç½®
    config = {
        'data_dir': './data/test_enhanced',
        'local_models': [
            {
                'name': 'test_model',
                'endpoint': 'http://localhost:8000',
                'model_type': 'llm',
                'security_enabled': True,
                'cache_enabled': True
            }
        ]
    }
    
    # å‰µå»ºMCPå¯¦ä¾‹
    mcp = await create_enhanced_smartinvention_mcp(config)
    
    # æ¸¬è©¦å°è©±æ•¸æ“š
    test_conversations = [
        {
            "id": "test_conv_001",
            "timestamp": datetime.now().isoformat(),
            "participants": ["user", "assistant"],
            "messages": [
                {
                    "role": "user",
                    "content": "æˆ‘éœ€è¦å¹«åŠ©è§£æ±ºMCPçµ„ä»¶çš„æ€§èƒ½å•é¡Œ",
                    "timestamp": datetime.now().isoformat()
                },
                {
                    "role": "assistant", 
                    "content": "æˆ‘å¯ä»¥å¹«æ‚¨åˆ†æMCPçµ„ä»¶çš„æ€§èƒ½ç“¶é ¸ä¸¦æä¾›å„ªåŒ–å»ºè­°",
                    "timestamp": datetime.now().isoformat()
                }
            ],
            "metadata": {
                "topic": "MCPæ€§èƒ½å„ªåŒ–",
                "priority": "high"
            }
        }
    ]
    
    # æ¸¬è©¦åŒæ­¥åŠŸèƒ½
    result = await mcp.sync_conversations_enhanced(
        test_conversations,
        {"test_session": "enhanced_test_001"}
    )
    
    print("âœ… å¢å¼·ç‰ˆåŒæ­¥æ¸¬è©¦å®Œæˆ:")
    print(f"   - è™•ç†å°è©±æ•¸: {result['conversations_saved']}")
    print(f"   - ä»‹å…¥éœ€æ±‚æ•¸: {result['intervention_needed_count']}")
    print(f"   - è™•ç†æ™‚é–“: {result['save_result']['processing_time']:.2f}ç§’")
    
    # ç²å–æ€§èƒ½çµ±è¨ˆ
    stats = await mcp.storage.get_performance_stats()
    print("ğŸ“Š æ€§èƒ½çµ±è¨ˆ:")
    print(f"   - ç·©å­˜å¤§å°: {stats['cache_stats']['cache_size']}")
    print(f"   - å¹³å‡ä¿å­˜æ™‚é–“: {stats['processing_times']['avg_save_time']:.3f}ç§’")
    print(f"   - CPUä½¿ç”¨ç‡: {stats['system_resources']['cpu_usage']:.1f}%")

if __name__ == "__main__":
    asyncio.run(test_enhanced_smartinvention_mcp())

