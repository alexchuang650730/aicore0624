"""
General_Processor MCP çµ„ä»¶
çµ±ä¸€çš„é€šç”¨è™•ç†å™¨MCPçµ„ä»¶ï¼Œæ•´åˆdefault_processorå’Œgeneral_processorçš„æ‰€æœ‰åŠŸèƒ½
"""

import time
import json
import logging
import asyncio
from typing import Any, Dict, Optional, List
from dataclasses import dataclass, asdict
from enum import Enum

logger = logging.getLogger(__name__)

class ProcessingMode(Enum):
    """è™•ç†æ¨¡å¼"""
    DEFAULT = "default"      # é»˜èªå›é€€æ¨¡å¼
    GENERAL = "general"      # é€šç”¨è™•ç†æ¨¡å¼
    TEXT = "text"           # æ–‡æœ¬è™•ç†æ¨¡å¼
    JSON = "json"           # JSONè™•ç†æ¨¡å¼
    AUTO = "auto"           # è‡ªå‹•é¸æ“‡æ¨¡å¼

@dataclass
class ProcessingResult:
    """è™•ç†çµæœ"""
    success: bool
    data: Any
    mode_used: str
    execution_time: float
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return asdict(self)

class GeneralProcessorMCP:
    """
    çµ±ä¸€çš„é€šç”¨è™•ç†å™¨MCPçµ„ä»¶
    æ•´åˆdefault_processorå’Œgeneral_processorçš„æ‰€æœ‰åŠŸèƒ½
    
    åŠŸèƒ½ç‰¹é»:
    1. äº”ç¨®è™•ç†æ¨¡å¼ï¼šDefault, General, Text, JSON, Auto
    2. æ™ºèƒ½æ¨¡å¼é¸æ“‡ï¼šè‡ªå‹•æª¢æ¸¬è¼¸å…¥é¡å‹ä¸¦é¸æ“‡æœ€ä½³æ¨¡å¼
    3. å°ˆå®¶æ´å¯Ÿè™•ç†ï¼šå°ˆé–€è™•ç†åŒ…å«å°ˆå®¶åˆ†æçš„è¤‡é›œæ•¸æ“š
    4. å‘å¾Œå…¼å®¹ï¼šå®Œå…¨æ›¿ä»£åŸæœ‰çš„default_processorå’Œgeneral_processor
    """
    
    def __init__(self):
        self.component_name = "General_Processor MCP"
        self.version = "2.0.0"
        self.description = "çµ±ä¸€çš„é€šç”¨è™•ç†å™¨MCPçµ„ä»¶ï¼Œæ•´åˆå¤šç¨®è™•ç†èƒ½åŠ›"
        
        # èƒ½åŠ›å®šç¾©
        self.capabilities = {
            "general_processing": {
                "description": "é€šç”¨è™•ç†èƒ½åŠ›ï¼Œæ”¯æŒå¤šç¨®æ•¸æ“šæ ¼å¼",
                "input_types": ["text", "json", "auto"],
                "output_types": ["text", "json"],
                "modes": ["default", "general", "auto"]
            },
            "text_processing": {
                "description": "å°ˆæ¥­æ–‡æœ¬è™•ç†å’Œåˆ†æèƒ½åŠ›",
                "input_types": ["text"],
                "output_types": ["text", "json"],
                "modes": ["text", "general", "auto"]
            },
            "json_processing": {
                "description": "JSONæ•¸æ“šè™•ç†å’Œè½‰æ›èƒ½åŠ›",
                "input_types": ["json"],
                "output_types": ["json", "text"],
                "modes": ["json", "default", "auto"]
            },
            "fallback_processing": {
                "description": "ç³»çµ±å›é€€è™•ç†èƒ½åŠ›ï¼Œç¢ºä¿ç©©å®šæ€§",
                "input_types": ["any"],
                "output_types": ["text", "json"],
                "modes": ["default", "auto"]
            },
            "auto_processing": {
                "description": "æ™ºèƒ½è‡ªå‹•è™•ç†èƒ½åŠ›ï¼Œè‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å¼",
                "input_types": ["any"],
                "output_types": ["text", "json"],
                "modes": ["auto"]
            },
            "expert_insight_processing": {
                "description": "å°ˆå®¶æ´å¯Ÿè™•ç†èƒ½åŠ›ï¼Œæ•´åˆå¤šå°ˆå®¶åˆ†æ",
                "input_types": ["dict_with_expert_insights"],
                "output_types": ["json"],
                "modes": ["general", "auto"]
            }
        }
        
        # åŸ·è¡Œçµ±è¨ˆ
        self.execution_stats = {
            'total_executions': 0,
            'successful_executions': 0,
            'failed_executions': 0,
            'mode_usage': {mode.value: 0 for mode in ProcessingMode},
            'average_execution_time': 0.0,
            'success_rate': 0.0,
            'last_execution_time': None
        }
        
        logger.info(f"{self.component_name} v{self.version} åˆå§‹åŒ–å®Œæˆ")
    
    async def process(self, data: Any, mode: str = "auto", options: Dict[str, Any] = None) -> ProcessingResult:
        """
        çµ±ä¸€çš„è™•ç†æ¥å£
        
        Args:
            data: è¦è™•ç†çš„æ•¸æ“š
            mode: è™•ç†æ¨¡å¼ (default, general, text, json, auto)
            options: é¡å¤–é¸é …
        
        Returns:
            ProcessingResult: è™•ç†çµæœ
        """
        start_time = time.time()
        options = options or {}
        
        logger.info(f"ğŸ”„ {self.component_name} é–‹å§‹è™•ç†ï¼Œæ¨¡å¼: {mode}")
        
        try:
            # é©—è­‰æ¨¡å¼
            if mode not in [m.value for m in ProcessingMode]:
                logger.warning(f"æœªçŸ¥æ¨¡å¼ {mode}ï¼Œå›é€€åˆ°autoæ¨¡å¼")
                mode = "auto"
            
            # è‡ªå‹•æ¨¡å¼ï¼šæ™ºèƒ½é¸æ“‡æœ€ä½³è™•ç†æ¨¡å¼
            if mode == "auto":
                mode = await self._auto_select_mode(data)
                logger.info(f"ğŸ¯ è‡ªå‹•é¸æ“‡æ¨¡å¼: {mode}")
            
            # æ ¹æ“šæ¨¡å¼åŸ·è¡Œè™•ç†
            if mode == "default":
                result_data = await self._default_processing(data, options)
            elif mode == "general":
                result_data = await self._general_processing(data, options)
            elif mode == "text":
                result_data = await self._text_processing(data, options)
            elif mode == "json":
                result_data = await self._json_processing(data, options)
            else:
                # æœªçŸ¥æ¨¡å¼ï¼Œå›é€€åˆ°default
                logger.warning(f"è™•ç†æ¨¡å¼ {mode} ä¸æ”¯æŒï¼Œå›é€€åˆ°defaultæ¨¡å¼")
                mode = "default"
                result_data = await self._default_processing(data, options)
            
            execution_time = time.time() - start_time
            
            # å‰µå»ºè™•ç†çµæœ
            result = ProcessingResult(
                success=True,
                data=result_data,
                mode_used=mode,
                execution_time=execution_time,
                metadata={
                    "component": self.component_name,
                    "version": self.version,
                    "input_type": type(data).__name__,
                    "processing_mode": mode,
                    "options_used": options,
                    "timestamp": time.time(),
                    "capabilities_applied": self._get_applied_capabilities(mode)
                }
            )
            
            # æ›´æ–°çµ±è¨ˆ
            self._update_execution_stats(result)
            
            logger.info(f"âœ… è™•ç†å®Œæˆï¼Œæ¨¡å¼: {mode}ï¼Œè€—æ™‚: {execution_time:.3f}s")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ è™•ç†å¤±æ•—: {str(e)}")
            
            result = ProcessingResult(
                success=False,
                data=f"è™•ç†å¤±æ•—: {str(e)}",
                mode_used=mode,
                execution_time=execution_time,
                metadata={
                    "component": self.component_name,
                    "error": str(e), 
                    "input_type": type(data).__name__,
                    "timestamp": time.time()
                }
            )
            
            self._update_execution_stats(result)
            return result
    
    async def _auto_select_mode(self, data: Any) -> str:
        """è‡ªå‹•é¸æ“‡æœ€ä½³è™•ç†æ¨¡å¼"""
        
        # æª¢æŸ¥æ•¸æ“šé¡å‹å’Œå…§å®¹
        if isinstance(data, dict):
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å°ˆå®¶æ´å¯Ÿ
            if "expert_insights" in data or "expert_analysis" in data:
                return "general"  # æœ‰å°ˆå®¶æ´å¯Ÿï¼Œä½¿ç”¨é€šç”¨æ¨¡å¼
            elif "content" in data and "context" in data:
                return "general"  # çµæ§‹åŒ–è«‹æ±‚æ•¸æ“š
            else:
                return "json"     # ç´”JSONæ•¸æ“š
        
        elif isinstance(data, str):
            # å˜—è©¦è§£æç‚ºJSON
            try:
                parsed = json.loads(data)
                if isinstance(parsed, dict) and ("expert_insights" in parsed or "content" in parsed):
                    return "general"
                else:
                    return "json"
            except:
                # æª¢æŸ¥æ–‡æœ¬ç‰¹å¾µ
                if len(data) > 1000 or '\n' in data:
                    return "text"     # é•·æ–‡æœ¬æˆ–å¤šè¡Œæ–‡æœ¬
                else:
                    return "text"     # çŸ­æ–‡æœ¬
        
        elif isinstance(data, list):
            return "json"         # åˆ—è¡¨æ•¸æ“š
        
        else:
            return "default"      # å…¶ä»–é¡å‹ï¼Œä½¿ç”¨é»˜èªæ¨¡å¼
    
    async def _default_processing(self, data: Any, options: Dict) -> Any:
        """é»˜èªè™•ç†æ¨¡å¼ - æœ€å¼·å…¼å®¹æ€§ï¼Œç¢ºä¿ç³»çµ±ç©©å®šæ€§"""
        logger.info("ğŸ›¡ï¸ åŸ·è¡Œé»˜èªè™•ç†æ¨¡å¼")
        
        # è™•ç†å„ç¨®æ•¸æ“šé¡å‹
        if isinstance(data, dict):
            # è™•ç†å­—å…¸æ•¸æ“š
            processed_data = {
                "processing_result": {
                    "status": "success",
                    "mode": "default",
                    "message": "æ•¸æ“šå·²é€šéé»˜èªæ¨¡å¼æˆåŠŸè™•ç†"
                },
                "original_data": data,
                "data_analysis": {
                    "type": "dictionary",
                    "key_count": len(data),
                    "keys": list(data.keys())[:10],  # åªé¡¯ç¤ºå‰10å€‹éµ
                    "has_nested_data": any(isinstance(v, (dict, list)) for v in data.values())
                },
                "capabilities_applied": ["general_processing", "fallback_processing"],
                "processing_metadata": {
                    "component": self.component_name,
                    "mode": "default",
                    "timestamp": time.time()
                }
            }
            
            # å¦‚æœåŒ…å«å°ˆå®¶æ´å¯Ÿï¼Œé€²è¡Œç‰¹æ®Šè™•ç†
            if "expert_insights" in data:
                expert_summary = []
                for insight in data["expert_insights"]:
                    expert_summary.append({
                        "expert_type": insight.get('expert_type', 'unknown'),
                        "analysis_preview": insight.get('analysis', '')[:100] + "..." if len(insight.get('analysis', '')) > 100 else insight.get('analysis', ''),
                        "recommendation_count": len(insight.get('recommendations', []))
                    })
                
                processed_data["expert_analysis_summary"] = {
                    "total_experts": len(data["expert_insights"]),
                    "expert_summaries": expert_summary,
                    "processing_note": "å°ˆå®¶æ´å¯Ÿå·²é€šéé»˜èªæ¨¡å¼è™•ç†"
                }
            
            return processed_data
        
        elif isinstance(data, str):
            # è™•ç†æ–‡æœ¬æ•¸æ“š
            return {
                "processing_result": {
                    "status": "success",
                    "mode": "default",
                    "message": "æ–‡æœ¬å·²é€šéé»˜èªæ¨¡å¼è™•ç†"
                },
                "original_text": data,
                "text_analysis": {
                    "length": len(data),
                    "word_count": len(data.split()),
                    "line_count": len(data.split('\n')),
                    "preview": data[:200] + "..." if len(data) > 200 else data
                },
                "capabilities_applied": ["general_processing", "text_processing"],
                "processing_metadata": {
                    "component": self.component_name,
                    "mode": "default",
                    "timestamp": time.time()
                }
            }
        
        else:
            # è™•ç†å…¶ä»–é¡å‹
            return {
                "processing_result": {
                    "status": "success",
                    "mode": "default",
                    "message": f"æ•¸æ“šé¡å‹ {type(data).__name__} å·²é€šéé»˜èªæ¨¡å¼è™•ç†"
                },
                "original_data": str(data),
                "data_analysis": {
                    "type": type(data).__name__,
                    "string_representation": str(data)[:200] + "..." if len(str(data)) > 200 else str(data)
                },
                "capabilities_applied": ["fallback_processing"],
                "processing_metadata": {
                    "component": self.component_name,
                    "mode": "default",
                    "timestamp": time.time()
                }
            }
    
    async def _general_processing(self, data: Any, options: Dict) -> Any:
        """é€šç”¨è™•ç†æ¨¡å¼ - è™•ç†ä¸€èˆ¬æ€§ä»»å‹™å’Œå°ˆå®¶æ´å¯Ÿ"""
        logger.info("âš™ï¸ åŸ·è¡Œé€šç”¨è™•ç†æ¨¡å¼")
        
        if isinstance(data, dict) and ("expert_insights" in data or "expert_analysis" in data):
            # è™•ç†åŒ…å«å°ˆå®¶æ´å¯Ÿçš„æ•¸æ“š
            content = data.get("content", "")
            context = data.get("context", {})
            expert_insights = data.get("expert_insights", data.get("expert_analysis", []))
            
            # åˆ†æå°ˆå®¶æ´å¯Ÿ
            expert_analysis = []
            all_recommendations = []
            confidence_scores = []
            
            for insight in expert_insights:
                expert_type = insight.get("expert_type", "unknown")
                analysis = insight.get("analysis", "")
                recommendations = insight.get("recommendations", [])
                confidence = insight.get("confidence", 0.8)
                
                expert_analysis.append({
                    "expert": expert_type,
                    "analysis_summary": analysis[:300] + "..." if len(analysis) > 300 else analysis,
                    "key_recommendations": recommendations[:5],  # åªå–å‰5å€‹å»ºè­°
                    "confidence": confidence,
                    "recommendation_count": len(recommendations)
                })
                
                all_recommendations.extend(recommendations)
                confidence_scores.append(confidence)
            
            # è¨ˆç®—ç¶œåˆä¿¡å¿ƒåº¦
            avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
            
            # å»é‡ä¸¦æ’åºå»ºè­°
            unique_recommendations = list(dict.fromkeys(all_recommendations))  # ä¿æŒé †åºçš„å»é‡
            
            return {
                "processing_result": {
                    "status": "success",
                    "mode": "general",
                    "message": f"é€šç”¨æ¨¡å¼æˆåŠŸè™•ç†äº†ä¾†è‡ª {len(expert_insights)} ä½å°ˆå®¶çš„æ´å¯Ÿ"
                },
                "original_content": content,
                "context_summary": context,
                "expert_analysis": {
                    "total_experts": len(expert_insights),
                    "expert_details": expert_analysis,
                    "average_confidence": round(avg_confidence, 2)
                },
                "consolidated_recommendations": {
                    "total_recommendations": len(all_recommendations),
                    "unique_recommendations": unique_recommendations[:20],  # åªé¡¯ç¤ºå‰20å€‹
                    "recommendation_sources": len(expert_insights)
                },
                "processing_summary": {
                    "experts_consulted": [exp["expert"] for exp in expert_analysis],
                    "total_insights": len(expert_insights),
                    "processing_quality": "high" if avg_confidence > 0.8 else "medium" if avg_confidence > 0.6 else "low"
                },
                "capabilities_applied": ["general_processing", "expert_insight_processing"],
                "processing_metadata": {
                    "component": self.component_name,
                    "mode": "general",
                    "timestamp": time.time()
                }
            }
        
        else:
            # è™•ç†ä¸€èˆ¬æ•¸æ“š
            data_str = str(data)
            complexity = "simple" if len(data_str) < 100 else "medium" if len(data_str) < 1000 else "complex"
            
            return {
                "processing_result": {
                    "status": "success",
                    "mode": "general",
                    "message": "æ•¸æ“šå·²é€šéé€šç”¨æ¨¡å¼æˆåŠŸè™•ç†"
                },
                "input_summary": data_str[:300] + "..." if len(data_str) > 300 else data_str,
                "data_characteristics": {
                    "type": type(data).__name__,
                    "size": len(data_str),
                    "complexity": complexity,
                    "estimated_processing_difficulty": complexity
                },
                "processing_analysis": {
                    "data_structure": "structured" if isinstance(data, (dict, list)) else "unstructured",
                    "processing_approach": "general_purpose",
                    "quality_assessment": "standard"
                },
                "capabilities_applied": ["general_processing"],
                "processing_metadata": {
                    "component": self.component_name,
                    "mode": "general",
                    "timestamp": time.time()
                }
            }
    
    async def _text_processing(self, data: Any, options: Dict) -> Any:
        """æ–‡æœ¬è™•ç†æ¨¡å¼ - å°ˆé–€è™•ç†æ–‡æœ¬æ•¸æ“š"""
        logger.info("ğŸ“ åŸ·è¡Œæ–‡æœ¬è™•ç†æ¨¡å¼")
        
        text = str(data)
        
        # æ–‡æœ¬åˆ†æ
        words = text.split()
        lines = text.split('\n')
        sentences = text.split('.')
        
        # è¨ˆç®—æ–‡æœ¬ç‰¹å¾µ
        features = {
            "basic_stats": {
                "character_count": len(text),
                "word_count": len(words),
                "line_count": len(lines),
                "sentence_count": len([s for s in sentences if s.strip()]),
                "paragraph_count": len([p for p in text.split('\n\n') if p.strip()])
            },
            "content_analysis": {
                "average_word_length": round(sum(len(word) for word in words) / len(words), 2) if words else 0,
                "average_sentence_length": round(len(words) / len([s for s in sentences if s.strip()]), 2) if sentences else 0,
                "has_numbers": any(char.isdigit() for char in text),
                "has_special_chars": any(not char.isalnum() and not char.isspace() for char in text),
                "has_urls": "http" in text.lower() or "www." in text.lower(),
                "has_emails": "@" in text and "." in text
            },
            "language_features": {
                "uppercase_ratio": round(sum(1 for c in text if c.isupper()) / len(text), 3) if text else 0,
                "punctuation_density": round(sum(1 for c in text if c in '.,!?;:') / len(text), 3) if text else 0,
                "whitespace_ratio": round(sum(1 for c in text if c.isspace()) / len(text), 3) if text else 0
            }
        }
        
        # æ–‡æœ¬åˆ†é¡
        text_type = "unknown"
        if len(text) < 50:
            text_type = "short_text"
        elif len(lines) > 10:
            text_type = "multi_line_document"
        elif any(keyword in text.lower() for keyword in ["api", "function", "class", "import"]):
            text_type = "code_or_technical"
        elif any(keyword in text.lower() for keyword in ["åˆ†æ", "å»ºè­°", "å°ˆå®¶", "è™•ç†"]):
            text_type = "analysis_or_report"
        else:
            text_type = "general_text"
        
        return {
            "processing_result": {
                "status": "success",
                "mode": "text",
                "message": f"æ–‡æœ¬è™•ç†å®Œæˆï¼š{features['basic_stats']['word_count']} è©ï¼Œ{features['basic_stats']['character_count']} å­—ç¬¦"
            },
            "original_text": text,
            "text_preview": text[:200] + "..." if len(text) > 200 else text,
            "text_features": features,
            "text_classification": {
                "type": text_type,
                "complexity": "simple" if len(text) < 200 else "medium" if len(text) < 1000 else "complex",
                "readability": "high" if features["content_analysis"]["average_word_length"] < 6 else "medium"
            },
            "processing_insights": {
                "dominant_content": "text_heavy" if len(words) > 100 else "concise",
                "structure_quality": "well_structured" if len(lines) > 1 and len([p for p in text.split('\n\n') if p.strip()]) > 1 else "simple",
                "information_density": "high" if len(words) / len(text) > 0.1 else "low"
            },
            "capabilities_applied": ["text_processing"],
            "processing_metadata": {
                "component": self.component_name,
                "mode": "text",
                "timestamp": time.time()
            }
        }
    
    async def _json_processing(self, data: Any, options: Dict) -> Any:
        """JSONè™•ç†æ¨¡å¼ - å°ˆé–€è™•ç†JSONæ•¸æ“š"""
        logger.info("ğŸ”§ åŸ·è¡ŒJSONè™•ç†æ¨¡å¼")
        
        try:
            # ç¢ºä¿æ•¸æ“šæ˜¯JSONæ ¼å¼
            if isinstance(data, str):
                json_data = json.loads(data)
            else:
                json_data = data
            
            # JSONçµæ§‹åˆ†æ
            def analyze_json_structure(obj, depth=0, max_depth=5):
                if depth > max_depth:
                    return {"type": "deep_object", "depth_exceeded": True, "max_depth_reached": max_depth}
                
                if isinstance(obj, dict):
                    nested_analysis = {}
                    if depth < 2:  # åªåˆ†æå‰å…©å±¤
                        for k, v in list(obj.items())[:10]:  # åªåˆ†æå‰10å€‹éµ
                            nested_analysis[k] = analyze_json_structure(v, depth+1, max_depth)
                    
                    return {
                        "type": "object",
                        "keys": list(obj.keys())[:20],  # åªé¡¯ç¤ºå‰20å€‹éµ
                        "key_count": len(obj),
                        "nested_structure": nested_analysis,
                        "has_nested_objects": any(isinstance(v, dict) for v in obj.values()),
                        "has_arrays": any(isinstance(v, list) for v in obj.values())
                    }
                elif isinstance(obj, list):
                    element_types = []
                    sample_elements = []
                    for item in obj[:5]:  # åªæª¢æŸ¥å‰5å€‹å…ƒç´ 
                        element_types.append(type(item).__name__)
                        if len(sample_elements) < 3:
                            sample_elements.append(analyze_json_structure(item, depth+1, max_depth))
                    
                    return {
                        "type": "array",
                        "length": len(obj),
                        "element_types": list(set(element_types)),
                        "sample_elements": sample_elements,
                        "is_homogeneous": len(set(element_types)) == 1
                    }
                else:
                    return {
                        "type": type(obj).__name__,
                        "value_preview": str(obj)[:100] + "..." if len(str(obj)) > 100 else str(obj),
                        "value_length": len(str(obj))
                    }
            
            structure_analysis = analyze_json_structure(json_data)
            
            # è¨ˆç®—JSONè¤‡é›œåº¦
            json_str = json.dumps(json_data, ensure_ascii=False)
            complexity_score = 0
            complexity_score += len(json_str) // 1000  # å¤§å°å› å­
            complexity_score += str(json_data).count('{') + str(json_data).count('[')  # åµŒå¥—å› å­
            
            complexity_level = "simple" if complexity_score < 5 else "medium" if complexity_score < 20 else "complex"
            
            return {
                "processing_result": {
                    "status": "success",
                    "mode": "json",
                    "message": "JSONæ•¸æ“šçµæ§‹åˆ†æå®Œæˆ"
                },
                "original_data": json_data,
                "structure_analysis": structure_analysis,
                "data_metrics": {
                    "total_size_bytes": len(json_str),
                    "total_size_chars": len(json_str),
                    "complexity_score": complexity_score,
                    "complexity_level": complexity_level,
                    "nesting_depth": self._calculate_max_depth(json_data),
                    "total_keys": self._count_total_keys(json_data),
                    "total_values": self._count_total_values(json_data)
                },
                "data_quality": {
                    "structure_validity": "valid",
                    "data_consistency": "consistent" if isinstance(json_data, (dict, list)) else "simple",
                    "information_richness": "high" if complexity_score > 10 else "medium" if complexity_score > 3 else "low"
                },
                "processing_insights": {
                    "recommended_usage": "structured_data_processing" if isinstance(json_data, dict) else "list_processing" if isinstance(json_data, list) else "simple_value",
                    "optimization_suggestions": self._get_json_optimization_suggestions(json_data, complexity_score)
                },
                "capabilities_applied": ["json_processing"],
                "processing_metadata": {
                    "component": self.component_name,
                    "mode": "json",
                    "timestamp": time.time()
                }
            }
            
        except json.JSONDecodeError as e:
            return {
                "processing_result": {
                    "status": "error",
                    "mode": "json",
                    "message": f"JSONè§£æå¤±æ•—: {str(e)}"
                },
                "error_details": {
                    "error_type": "JSONDecodeError",
                    "error_message": str(e),
                    "error_position": getattr(e, 'pos', 'unknown')
                },
                "original_data": str(data)[:500] + "..." if len(str(data)) > 500 else str(data),
                "fallback_processing": {
                    "applied": True,
                    "method": "text_processing",
                    "note": "å·²è½‰ç‚ºæ–‡æœ¬è™•ç†æ¨¡å¼"
                },
                "capabilities_applied": ["json_processing", "fallback_processing"],
                "processing_metadata": {
                    "component": self.component_name,
                    "mode": "json",
                    "timestamp": time.time()
                }
            }
    
    def _calculate_max_depth(self, obj, current_depth=0):
        """è¨ˆç®—JSONæœ€å¤§åµŒå¥—æ·±åº¦"""
        if isinstance(obj, dict):
            if not obj:
                return current_depth
            return max(self._calculate_max_depth(v, current_depth + 1) for v in obj.values())
        elif isinstance(obj, list):
            if not obj:
                return current_depth
            return max(self._calculate_max_depth(item, current_depth + 1) for item in obj)
        else:
            return current_depth
    
    def _count_total_keys(self, obj):
        """è¨ˆç®—JSONä¸­ç¸½éµæ•¸"""
        if isinstance(obj, dict):
            return len(obj) + sum(self._count_total_keys(v) for v in obj.values())
        elif isinstance(obj, list):
            return sum(self._count_total_keys(item) for item in obj)
        else:
            return 0
    
    def _count_total_values(self, obj):
        """è¨ˆç®—JSONä¸­ç¸½å€¼æ•¸"""
        if isinstance(obj, dict):
            return len(obj) + sum(self._count_total_values(v) for v in obj.values())
        elif isinstance(obj, list):
            return len(obj) + sum(self._count_total_values(item) for item in obj)
        else:
            return 1
    
    def _get_json_optimization_suggestions(self, data, complexity_score):
        """ç²å–JSONå„ªåŒ–å»ºè­°"""
        suggestions = []
        
        if complexity_score > 20:
            suggestions.append("è€ƒæ…®åˆ†è§£ç‚ºæ›´å°çš„æ•¸æ“šå¡Š")
            suggestions.append("ä½¿ç”¨æ•¸æ“šå£“ç¸®æŠ€è¡“")
        
        if isinstance(data, dict) and len(data) > 50:
            suggestions.append("è€ƒæ…®ä½¿ç”¨ç´¢å¼•æˆ–åˆ†é ")
        
        if isinstance(data, list) and len(data) > 100:
            suggestions.append("è€ƒæ…®å¯¦ç¾æ‡¶åŠ è¼‰")
        
        if self._calculate_max_depth(data) > 5:
            suggestions.append("è€ƒæ…®æ‰å¹³åŒ–æ•¸æ“šçµæ§‹")
        
        return suggestions if suggestions else ["æ•¸æ“šçµæ§‹å·²å„ªåŒ–"]
    
    def _get_applied_capabilities(self, mode: str) -> List[str]:
        """ç²å–æ‡‰ç”¨çš„èƒ½åŠ›åˆ—è¡¨"""
        capability_mapping = {
            "default": ["general_processing", "fallback_processing"],
            "general": ["general_processing", "expert_insight_processing"],
            "text": ["text_processing"],
            "json": ["json_processing"],
            "auto": ["auto_processing"]
        }
        return capability_mapping.get(mode, ["general_processing"])
    
    def _update_execution_stats(self, result: ProcessingResult):
        """æ›´æ–°åŸ·è¡Œçµ±è¨ˆ"""
        self.execution_stats['total_executions'] += 1
        
        if result.success:
            self.execution_stats['successful_executions'] += 1
        else:
            self.execution_stats['failed_executions'] += 1
        
        # æ›´æ–°æ¨¡å¼ä½¿ç”¨çµ±è¨ˆ
        mode = result.mode_used
        if mode in self.execution_stats['mode_usage']:
            self.execution_stats['mode_usage'][mode] += 1
        
        # æ›´æ–°å¹³å‡åŸ·è¡Œæ™‚é–“
        current_avg = self.execution_stats['average_execution_time']
        total_executions = self.execution_stats['total_executions']
        self.execution_stats['average_execution_time'] = (
            (current_avg * (total_executions - 1) + result.execution_time) / total_executions
        )
        
        # æ›´æ–°æˆåŠŸç‡
        self.execution_stats['success_rate'] = (
            self.execution_stats['successful_executions'] / total_executions
        )
        
        # æ›´æ–°æœ€å¾ŒåŸ·è¡Œæ™‚é–“
        self.execution_stats['last_execution_time'] = time.time()
    
    async def get_capabilities(self) -> Dict[str, Any]:
        """ç²å–èƒ½åŠ›æè¿°"""
        return {
            "component_name": self.component_name,
            "version": self.version,
            "description": self.description,
            "capabilities": self.capabilities,
            "supported_modes": [mode.value for mode in ProcessingMode],
            "mode_descriptions": {
                "default": "é»˜èªå›é€€æ¨¡å¼ï¼Œæœ€å¼·å…¼å®¹æ€§",
                "general": "é€šç”¨è™•ç†æ¨¡å¼ï¼Œè™•ç†å°ˆå®¶æ´å¯Ÿ",
                "text": "æ–‡æœ¬è™•ç†æ¨¡å¼ï¼Œå°ˆæ¥­æ–‡æœ¬åˆ†æ",
                "json": "JSONè™•ç†æ¨¡å¼ï¼Œçµæ§‹åŒ–æ•¸æ“šåˆ†æ",
                "auto": "è‡ªå‹•é¸æ“‡æ¨¡å¼ï¼Œæ™ºèƒ½æ¨¡å¼é¸æ“‡"
            },
            "statistics": self.execution_stats,
            "features": [
                "æ™ºèƒ½æ¨¡å¼é¸æ“‡",
                "å°ˆå®¶æ´å¯Ÿè™•ç†",
                "å¤šæ ¼å¼æ•¸æ“šæ”¯æŒ",
                "å‘å¾Œå…¼å®¹æ€§",
                "å¯¦æ™‚çµ±è¨ˆç›£æ§"
            ]
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        current_time = time.time()
        last_execution = self.execution_stats.get('last_execution_time')
        
        # è¨ˆç®—å¥åº·ç‹€æ…‹
        health_status = "healthy"
        if self.execution_stats['total_executions'] > 0:
            if self.execution_stats['success_rate'] < 0.8:
                health_status = "degraded"
            elif self.execution_stats['success_rate'] < 0.5:
                health_status = "unhealthy"
        
        return {
            "status": health_status,
            "component": self.component_name,
            "version": self.version,
            "uptime_status": "running",
            "health_metrics": {
                "total_executions": self.execution_stats['total_executions'],
                "success_rate": round(self.execution_stats['success_rate'], 3),
                "average_execution_time": round(self.execution_stats['average_execution_time'], 3),
                "last_execution_ago": round(current_time - last_execution, 2) if last_execution else None
            },
            "capabilities_status": {
                "total_capabilities": len(self.capabilities),
                "modes_available": len(ProcessingMode),
                "auto_mode_enabled": True
            },
            "performance_indicators": {
                "response_time": "optimal" if self.execution_stats['average_execution_time'] < 1.0 else "acceptable",
                "reliability": "high" if self.execution_stats['success_rate'] > 0.9 else "medium",
                "throughput": "normal"
            },
            "timestamp": current_time
        }
    
    async def get_statistics(self) -> Dict[str, Any]:
        """ç²å–è©³ç´°çµ±è¨ˆä¿¡æ¯"""
        return {
            "execution_statistics": self.execution_stats,
            "mode_distribution": {
                mode: {
                    "count": count,
                    "percentage": round(count / max(self.execution_stats['total_executions'], 1) * 100, 2)
                }
                for mode, count in self.execution_stats['mode_usage'].items()
            },
            "performance_metrics": {
                "average_execution_time": self.execution_stats['average_execution_time'],
                "success_rate": self.execution_stats['success_rate'],
                "total_processing_time": self.execution_stats['average_execution_time'] * self.execution_stats['total_executions']
            },
            "component_info": {
                "name": self.component_name,
                "version": self.version,
                "capabilities_count": len(self.capabilities),
                "modes_count": len(ProcessingMode)
            }
        }

# å·¥å» å‡½æ•¸
def create_general_processor_mcp() -> GeneralProcessorMCP:
    """å‰µå»ºGeneral_Processor MCPå¯¦ä¾‹"""
    return GeneralProcessorMCP()

# å‘å¾Œå…¼å®¹æ€§æ”¯æŒ
class LegacyProcessorAdapter:
    """å‘å¾Œå…¼å®¹æ€§é©é…å™¨"""
    
    def __init__(self, general_processor: GeneralProcessorMCP):
        self.general_processor = general_processor
    
    async def default_processor_process(self, data: Any) -> Dict[str, Any]:
        """æ¨¡æ“¬èˆŠçš„default_processorè¡Œç‚º"""
        result = await self.general_processor.process(data, mode="default")
        return result.to_dict()
    
    async def general_processor_process(self, data: Any) -> Dict[str, Any]:
        """æ¨¡æ“¬èˆŠçš„general_processorè¡Œç‚º"""
        result = await self.general_processor.process(data, mode="general")
        return result.to_dict()

# ç¤ºä¾‹ä½¿ç”¨
async def example_usage():
    """ç¤ºä¾‹ç”¨æ³•"""
    # å‰µå»ºGeneral_Processor MCPå¯¦ä¾‹
    processor = create_general_processor_mcp()
    
    # æ¸¬è©¦ä¸åŒæ¨¡å¼
    test_data = {
        "content": "æ¸¬è©¦å…§å®¹",
        "expert_insights": [
            {
                "expert_type": "technical_expert",
                "analysis": "æŠ€è¡“åˆ†æçµæœ",
                "recommendations": ["å»ºè­°1", "å»ºè­°2"],
                "confidence": 0.9
            }
        ]
    }
    
    # è‡ªå‹•æ¨¡å¼è™•ç†
    result = await processor.process(test_data)
    print(f"è™•ç†çµæœ: {result.success}, ä½¿ç”¨æ¨¡å¼: {result.mode_used}")
    
    # ç²å–èƒ½åŠ›ä¿¡æ¯
    capabilities = await processor.get_capabilities()
    print(f"çµ„ä»¶èƒ½åŠ›: {len(capabilities['capabilities'])} ç¨®")
    
    # å¥åº·æª¢æŸ¥
    health = await processor.health_check()
    print(f"å¥åº·ç‹€æ…‹: {health['status']}")

if __name__ == "__main__":
    asyncio.run(example_usage())

