#!/usr/bin/env python3
"""
AICore éœ€æ±‚è™•ç†å™¨æ¸¬è©¦è…³æœ¬
æ¸¬è©¦å’Œé©—è­‰ AICore éœ€æ±‚è™•ç†æµç¨‹çš„åŠŸèƒ½
"""

import asyncio
import json
import logging
import sys
import traceback
from datetime import datetime
from pathlib import Path

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MockAICore3:
    """æ¨¡æ“¬ AICore 3.0"""
    
    async def initialize(self):
        logger.info("ğŸ”§ æ¨¡æ“¬ AICore 3.0 åˆå§‹åŒ–")
        await asyncio.sleep(0.1)  # æ¨¡æ“¬åˆå§‹åŒ–æ™‚é–“
        return True

class MockEnhancedSmartinventionMCP:
    """æ¨¡æ“¬ Enhanced Smartinvention MCP"""
    
    async def initialize(self):
        logger.info("ğŸ”§ æ¨¡æ“¬ Enhanced Smartinvention MCP åˆå§‹åŒ–")
        await asyncio.sleep(0.1)
        return True
    
    async def get_all_tasks(self):
        """æ¨¡æ“¬ç²å–æ‰€æœ‰ä»»å‹™"""
        logger.info("ğŸ“Š æ¨¡æ“¬ç²å–æ‰€æœ‰ä»»å‹™")
        return [
            {"task_id": "TASK_001", "task_name": "UI/UXè¨­è¨ˆä»»å‹™ç¾¤çµ„"},
            {"task_id": "TASK_002", "task_name": "AIæ¨¡å‹æ¯”è¼ƒç¾¤çµ„"},
            {"task_id": "TASK_003", "task_name": "MCPæ¶æ§‹ç¾¤çµ„"}
        ]
    
    async def search_tasks(self, keyword):
        """æ¨¡æ“¬æœå°‹ä»»å‹™"""
        logger.info(f"ğŸ” æ¨¡æ“¬æœå°‹ä»»å‹™: {keyword}")
        
        mock_tasks = []
        if keyword in ["UI", "è¨­è¨ˆ", "ç•Œé¢"]:
            mock_tasks = [
                type('Task', (), {
                    'task_id': 'TASK_001',
                    'task_name': 'UI/UXè¨­è¨ˆä»»å‹™ç¾¤çµ„',
                    'description': 'å¦‚ä½•å°‡æ™ºæ…§ä¸‹è¼‰ç§»è‡³å°èˆªæ¬„å¹¶ç§»é™¤åŸåŠŸèƒ½'
                })(),
                type('Task', (), {
                    'task_id': 'TASK_003',
                    'task_name': 'MCPæ¶æ§‹ç¾¤çµ„',
                    'description': 'ç«¯é›²å”åŒç³»çµ±çš„ç•Œé¢è¨­è¨ˆå„ªåŒ–'
                })()
            ]
        
        return mock_tasks
    
    async def get_task_conversations(self, task_id):
        """æ¨¡æ“¬ç²å–ä»»å‹™å°è©±"""
        logger.info(f"ğŸ’¬ æ¨¡æ“¬ç²å–ä»»å‹™ {task_id} å°è©±")
        return [
            {"conversation_id": f"CONV_{task_id}_001", "timestamp": "2025-06-24T10:00:00Z"},
            {"conversation_id": f"CONV_{task_id}_002", "timestamp": "2025-06-24T11:00:00Z"}
        ]
    
    async def get_task_files(self, task_id):
        """æ¨¡æ“¬ç²å–ä»»å‹™æª”æ¡ˆ"""
        logger.info(f"ğŸ“ æ¨¡æ“¬ç²å–ä»»å‹™ {task_id} æª”æ¡ˆ")
        return [
            {
                "file_path": f"/home/ec2-user/smartinvention_mcp/tasks/{task_id}/metadata/task_info.json",
                "file_type": "ä»»å‹™å…ƒæ•¸æ“š"
            },
            {
                "file_path": f"/home/ec2-user/smartinvention_mcp/tasks/{task_id}/conversations/conv_001.json",
                "file_type": "å°è©±è¨˜éŒ„"
            }
        ]
    
    async def analyze_task_requirements(self, task_id):
        """æ¨¡æ“¬åˆ†æä»»å‹™éœ€æ±‚"""
        logger.info(f"ğŸ¯ æ¨¡æ“¬åˆ†æä»»å‹™ {task_id} éœ€æ±‚")
        return {
            "requirements": [
                {
                    "requirement_id": f"REQ_{task_id}_001",
                    "title": f"ä»»å‹™ {task_id} ä¸»è¦éœ€æ±‚",
                    "priority": "é«˜"
                }
            ]
        }

class AICoreRequirementProcessorTest:
    """AICore éœ€æ±‚è™•ç†å™¨æ¸¬è©¦é¡"""
    
    def __init__(self):
        self.test_results = []
        self.total_tests = 0
        self.passed_tests = 0
    
    async def run_all_tests(self):
        """é‹è¡Œæ‰€æœ‰æ¸¬è©¦"""
        logger.info("ğŸ§ª é–‹å§‹ AICore éœ€æ±‚è™•ç†å™¨æ¸¬è©¦")
        
        test_methods = [
            self.test_requirement_parser,
            self.test_expert_coordinator,
            self.test_mock_data_acquisition,
            self.test_result_formatting,
            self.test_end_to_end_processing
        ]
        
        for test_method in test_methods:
            try:
                await test_method()
                self.passed_tests += 1
                logger.info(f"âœ… {test_method.__name__} é€šé")
            except Exception as e:
                logger.error(f"âŒ {test_method.__name__} å¤±æ•—: {e}")
                self.test_results.append({
                    "test": test_method.__name__,
                    "status": "FAILED",
                    "error": str(e),
                    "traceback": traceback.format_exc()
                })
            
            self.total_tests += 1
        
        # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
        await self.generate_test_report()
    
    async def test_requirement_parser(self):
        """æ¸¬è©¦éœ€æ±‚è§£æå™¨"""
        logger.info("ğŸ” æ¸¬è©¦éœ€æ±‚è§£æå™¨")
        
        # å°å…¥éœ€æ±‚è§£æå™¨ï¼ˆä½¿ç”¨ç›¸å°å°å…¥æ¨¡æ“¬ï¼‰
        sys.path.append('/home/ubuntu')
        
        # æ¨¡æ“¬éœ€æ±‚è§£æå™¨
        class MockRequirementParser:
            def __init__(self):
                self.requirement_patterns = {
                    'req_id': r'REQ[_-]?(\d+)',
                    'target_entity': r'(REQ[_-]?\d+)',
                    'analysis_keywords': ['åˆ—å‡º', 'åˆ†æ', 'æ˜ç¢ºéœ€æ±‚', 'manus action', 'æª”æ¡ˆåˆ—è¡¨'],
                    'cross_task_keywords': ['è·¨ä»»å‹™', 'åŒä¸€å€‹éœ€æ±‚', 'å¤šä»»å‹™']
                }
            
            def parse_requirement(self, requirement_text: str):
                return {
                    "requirement_type": "requirement_analysis",
                    "target_entity": "REQ_001",
                    "analysis_scope": "full",
                    "output_format": ["requirements_list", "manus_actions", "file_list"],
                    "cross_task_analysis": True,
                    "data_sources": ["smartinvention_mcp"],
                    "expert_domains": ["requirement_analysis", "ui_ux_design", "data_analysis"]
                }
        
        parser = MockRequirementParser()
        test_requirement = "é¦–å…ˆå…ˆé‡å° REQ_001: ç”¨æˆ¶ç•Œé¢è¨­è¨ˆéœ€æ±‚ åˆ—å‡ºæˆ‘çš„æ˜ç¢ºéœ€æ±‚ åŠmanus action åŒ…å«ç›¸é—œçš„æª”æ¡ˆåˆ—è¡¨ æ³¨æ„åŒä¸€å€‹éœ€æ±‚å¯èƒ½è·¨ä»»å‹™"
        
        result = parser.parse_requirement(test_requirement)
        
        # é©—è­‰è§£æçµæœ
        assert result["requirement_type"] == "requirement_analysis"
        assert result["target_entity"] == "REQ_001"
        assert result["cross_task_analysis"] == True
        assert "requirements_list" in result["output_format"]
        assert "manus_actions" in result["output_format"]
        assert "file_list" in result["output_format"]
        
        self.test_results.append({
            "test": "test_requirement_parser",
            "status": "PASSED",
            "result": result
        })
    
    async def test_expert_coordinator(self):
        """æ¸¬è©¦å°ˆå®¶å”èª¿å™¨"""
        logger.info("ğŸ¤ æ¸¬è©¦å°ˆå®¶å”èª¿å™¨")
        
        # æ¨¡æ“¬å°ˆå®¶å”èª¿å™¨
        class MockExpertCoordinator:
            async def coordinate_experts(self, parsed_requirement, smartinvention_data):
                return {
                    "requirements": [
                        {
                            "requirement_id": "REQ_001_TASK_001_UI",
                            "title": "æ™ºæ…§ä¸‹è¼‰å°èˆªæ¬„æ•´åˆ",
                            "description": "å°‡æ™ºæ…§ä¸‹è¼‰åŠŸèƒ½æ•´åˆåˆ°å°èˆªæ¬„ä¸­",
                            "priority": "é«˜",
                            "source_tasks": ["TASK_001"],
                            "technical_complexity": "ä¸­ç­‰",
                            "estimated_hours": 40,
                            "category": "UI/UXè¨­è¨ˆ"
                        }
                    ],
                    "actions": [
                        {
                            "action_id": "ACTION_TASK_001_NAV",
                            "action_type": "å°èˆªå„ªåŒ–",
                            "description": "å„ªåŒ–å°èˆªæ¬„åŠŸèƒ½",
                            "related_tasks": ["TASK_001"],
                            "execution_status": "å¾…åŸ·è¡Œ",
                            "priority": "é«˜",
                            "estimated_effort": "2-3å¤©"
                        }
                    ],
                    "file_analysis": {
                        "/home/ec2-user/smartinvention_mcp/tasks/TASK_001/metadata/task_info.json": {
                            "type": "ä»»å‹™å…ƒæ•¸æ“š",
                            "relevance_score": 0.95,
                            "related_tasks": ["TASK_001"]
                        }
                    },
                    "cross_task_analysis": {
                        "related_task_count": 3,
                        "shared_requirements": ["UIå„ªåŒ–", "ç”¨æˆ¶é«”é©—æå‡"],
                        "dependency_chain": "TASK_001 â†’ TASK_003 â†’ TASK_006"
                    },
                    "expert_insights": {
                        "requirement_analysis": {"confidence": 0.85},
                        "ui_ux_design": {"confidence": 0.80},
                        "data_analysis": {"confidence": 0.90}
                    }
                }
        
        coordinator = MockExpertCoordinator()
        
        parsed_requirement = {
            "requirement_type": "requirement_analysis",
            "target_entity": "REQ_001",
            "expert_domains": ["requirement_analysis", "ui_ux_design", "data_analysis"]
        }
        
        smartinvention_data = {
            "tasks": {
                "TASK_001": {
                    "task_info": {"task_id": "TASK_001", "description": "UIè¨­è¨ˆä»»å‹™"}
                }
            }
        }
        
        result = await coordinator.coordinate_experts(parsed_requirement, smartinvention_data)
        
        # é©—è­‰å”èª¿çµæœ
        assert len(result["requirements"]) > 0
        assert len(result["actions"]) > 0
        assert "file_analysis" in result
        assert "cross_task_analysis" in result
        assert "expert_insights" in result
        
        self.test_results.append({
            "test": "test_expert_coordinator",
            "status": "PASSED",
            "result": result
        })
    
    async def test_mock_data_acquisition(self):
        """æ¸¬è©¦æ¨¡æ“¬æ•¸æ“šç²å–"""
        logger.info("ğŸ“Š æ¸¬è©¦æ¨¡æ“¬æ•¸æ“šç²å–")
        
        mock_mcp = MockEnhancedSmartinventionMCP()
        await mock_mcp.initialize()
        
        # æ¸¬è©¦ç²å–æ‰€æœ‰ä»»å‹™
        all_tasks = await mock_mcp.get_all_tasks()
        assert len(all_tasks) > 0
        
        # æ¸¬è©¦æœå°‹ä»»å‹™
        ui_tasks = await mock_mcp.search_tasks("UI")
        assert len(ui_tasks) > 0
        
        # æ¸¬è©¦ç²å–ä»»å‹™è©³ç´°ä¿¡æ¯
        if ui_tasks:
            task_id = ui_tasks[0].task_id
            conversations = await mock_mcp.get_task_conversations(task_id)
            files = await mock_mcp.get_task_files(task_id)
            
            assert len(conversations) > 0
            assert len(files) > 0
        
        self.test_results.append({
            "test": "test_mock_data_acquisition",
            "status": "PASSED",
            "result": {
                "all_tasks_count": len(all_tasks),
                "ui_tasks_count": len(ui_tasks)
            }
        })
    
    async def test_result_formatting(self):
        """æ¸¬è©¦çµæœæ ¼å¼åŒ–"""
        logger.info("ğŸ“‹ æ¸¬è©¦çµæœæ ¼å¼åŒ–")
        
        # æ¨¡æ“¬çµæœæ ¼å¼åŒ–
        mock_expert_analysis = {
            "requirements": [
                {
                    "requirement_id": "REQ_001_001",
                    "title": "æ¸¬è©¦éœ€æ±‚",
                    "description": "æ¸¬è©¦éœ€æ±‚æè¿°",
                    "priority": "é«˜",
                    "source_tasks": ["TASK_001"],
                    "technical_complexity": "ä¸­ç­‰",
                    "estimated_hours": 40,
                    "category": "UI/UXè¨­è¨ˆ"
                }
            ],
            "actions": [
                {
                    "action_id": "ACTION_001",
                    "action_type": "UIå„ªåŒ–",
                    "description": "æ¸¬è©¦è¡Œå‹•",
                    "related_tasks": ["TASK_001"],
                    "execution_status": "å¾…åŸ·è¡Œ",
                    "priority": "é«˜",
                    "estimated_effort": "2å¤©"
                }
            ],
            "file_analysis": {
                "/test/file.json": {
                    "type": "æ¸¬è©¦æª”æ¡ˆ",
                    "relevance_score": 0.9,
                    "related_tasks": ["TASK_001"]
                }
            },
            "cross_task_analysis": {
                "related_task_count": 1,
                "shared_requirements": ["æ¸¬è©¦éœ€æ±‚"],
                "dependency_chain": "TASK_001"
            }
        }
        
        # é©—è­‰æ ¼å¼åŒ–é‚è¼¯
        assert len(mock_expert_analysis["requirements"]) == 1
        assert len(mock_expert_analysis["actions"]) == 1
        assert len(mock_expert_analysis["file_analysis"]) == 1
        
        self.test_results.append({
            "test": "test_result_formatting",
            "status": "PASSED",
            "result": "æ ¼å¼åŒ–æ¸¬è©¦é€šé"
        })
    
    async def test_end_to_end_processing(self):
        """æ¸¬è©¦ç«¯åˆ°ç«¯è™•ç†æµç¨‹"""
        logger.info("ğŸ”„ æ¸¬è©¦ç«¯åˆ°ç«¯è™•ç†æµç¨‹")
        
        # æ¨¡æ“¬å®Œæ•´çš„è™•ç†æµç¨‹
        class MockAICoreRequirementProcessor:
            def __init__(self):
                self.aicore = MockAICore3()
                self.smartinvention_mcp = MockEnhancedSmartinventionMCP()
                self.processing_stats = {
                    "total_requests": 0,
                    "successful_requests": 0,
                    "average_processing_time": 0.0
                }
            
            async def initialize(self):
                await self.aicore.initialize()
                await self.smartinvention_mcp.initialize()
            
            async def process_requirement(self, requirement_text, context=None):
                # æ¨¡æ“¬è™•ç†æµç¨‹
                await asyncio.sleep(0.1)  # æ¨¡æ“¬è™•ç†æ™‚é–“
                
                return {
                    "requirement_id": "REQ_001",
                    "analysis_timestamp": datetime.now().isoformat(),
                    "requirements_list": [
                        {
                            "requirement_id": "REQ_001_001",
                            "title": "æ™ºæ…§ä¸‹è¼‰å°èˆªæ¬„æ•´åˆ",
                            "priority": "é«˜"
                        }
                    ],
                    "manus_actions": [
                        {
                            "action_id": "ACTION_001",
                            "action_type": "UIå„ªåŒ–",
                            "description": "å°èˆªæ¬„å„ªåŒ–"
                        }
                    ],
                    "file_references": [
                        {
                            "file_path": "/test/task_info.json",
                            "file_type": "ä»»å‹™å…ƒæ•¸æ“š",
                            "relevance_score": 0.95
                        }
                    ],
                    "processing_metrics": {
                        "total_tasks_analyzed": 3,
                        "requirements_identified": 1,
                        "actions_generated": 1,
                        "files_analyzed": 1
                    }
                }
        
        processor = MockAICoreRequirementProcessor()
        await processor.initialize()
        
        test_requirement = "é¦–å…ˆå…ˆé‡å° REQ_001: ç”¨æˆ¶ç•Œé¢è¨­è¨ˆéœ€æ±‚ åˆ—å‡ºæˆ‘çš„æ˜ç¢ºéœ€æ±‚ åŠmanus action åŒ…å«ç›¸é—œçš„æª”æ¡ˆåˆ—è¡¨ æ³¨æ„åŒä¸€å€‹éœ€æ±‚å¯èƒ½è·¨ä»»å‹™"
        
        result = await processor.process_requirement(test_requirement)
        
        # é©—è­‰ç«¯åˆ°ç«¯çµæœ
        assert result["requirement_id"] == "REQ_001"
        assert len(result["requirements_list"]) > 0
        assert len(result["manus_actions"]) > 0
        assert len(result["file_references"]) > 0
        assert "processing_metrics" in result
        
        self.test_results.append({
            "test": "test_end_to_end_processing",
            "status": "PASSED",
            "result": result
        })
    
    async def generate_test_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæ¸¬è©¦å ±å‘Š")
        
        success_rate = (self.passed_tests / self.total_tests) * 100 if self.total_tests > 0 else 0
        
        report = {
            "test_summary": {
                "total_tests": self.total_tests,
                "passed_tests": self.passed_tests,
                "failed_tests": self.total_tests - self.passed_tests,
                "success_rate": f"{success_rate:.1f}%",
                "test_timestamp": datetime.now().isoformat()
            },
            "test_results": self.test_results,
            "recommendations": self._generate_recommendations()
        }
        
        # ä¿å­˜æ¸¬è©¦å ±å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"/home/ubuntu/aicore_requirement_processor_test_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… æ¸¬è©¦å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # è¼¸å‡ºæ¸¬è©¦æ‘˜è¦
        print(f"\nğŸ§ª AICore éœ€æ±‚è™•ç†å™¨æ¸¬è©¦æ‘˜è¦:")
        print(f"ğŸ“Š ç¸½æ¸¬è©¦æ•¸: {self.total_tests}")
        print(f"âœ… é€šéæ¸¬è©¦: {self.passed_tests}")
        print(f"âŒ å¤±æ•—æ¸¬è©¦: {self.total_tests - self.passed_tests}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"ğŸ“„ è©³ç´°å ±å‘Š: {report_file}")
        
        return report
    
    def _generate_recommendations(self):
        """ç”Ÿæˆå»ºè­°"""
        recommendations = []
        
        if self.passed_tests == self.total_tests:
            recommendations.append("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼å¯ä»¥é€²è¡Œä¸‹ä¸€éšæ®µçš„æ•´åˆã€‚")
        else:
            recommendations.append("âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦ä¿®å¾©å•é¡Œå¾Œé‡æ–°æ¸¬è©¦ã€‚")
        
        recommendations.extend([
            "ğŸ”§ å»ºè­°åœ¨å¯¦éš›ç’°å¢ƒä¸­æ¸¬è©¦çœŸå¯¦çš„ AICore 3.0 å’Œ smartinvention MCP æ•´åˆã€‚",
            "ğŸ“ˆ å»ºè­°æ·»åŠ æ€§èƒ½æ¸¬è©¦å’Œå£“åŠ›æ¸¬è©¦ã€‚",
            "ğŸ›¡ï¸ å»ºè­°æ·»åŠ éŒ¯èª¤è™•ç†å’Œå®¹éŒ¯æ©Ÿåˆ¶çš„æ¸¬è©¦ã€‚",
            "ğŸ“Š å»ºè­°æ·»åŠ æ›´å¤šçš„é‚Šç•Œæ¢ä»¶æ¸¬è©¦ã€‚"
        ])
        
        return recommendations

async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    logger.info("ğŸš€ å•Ÿå‹• AICore éœ€æ±‚è™•ç†å™¨æ¸¬è©¦")
    
    try:
        tester = AICoreRequirementProcessorTest()
        await tester.run_all_tests()
        
        logger.info("âœ… æ¸¬è©¦å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())

