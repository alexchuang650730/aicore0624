#!/usr/bin/env python3
"""
Claude Code vs Manus å°æ¯”æ¸¬è©¦åŸ·è¡Œå™¨
åŸºæ–¼ YAML æ¸¬è©¦æ¨¡æ¿åŸ·è¡Œå¯¦éš›çš„å°æ¯”æ¸¬è©¦
"""

import asyncio
import json
import yaml
import time
import logging
from datetime import datetime
from typing import Dict, List, Any
from smartinvention_claude_code_adapter import SmartInventionClaudeCodeAdapter

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComparisonTestRunner:
    """å°æ¯”æ¸¬è©¦åŸ·è¡Œå™¨"""
    
    def __init__(self, template_path: str):
        self.template_path = template_path
        self.test_template = None
        self.results = []
        
        # åˆå§‹åŒ–é©é…å™¨
        self.aicore_adapter = SmartInventionClaudeCodeAdapter()
        
    def load_template(self):
        """åŠ è¼‰æ¸¬è©¦æ¨¡æ¿"""
        try:
            with open(self.template_path, 'r', encoding='utf-8') as f:
                self.test_template = yaml.safe_load(f)
            logger.info(f"æ¸¬è©¦æ¨¡æ¿åŠ è¼‰æˆåŠŸ: {self.template_path}")
        except Exception as e:
            logger.error(f"åŠ è¼‰æ¸¬è©¦æ¨¡æ¿å¤±æ•—: {e}")
            raise
    
    async def run_single_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦ç”¨ä¾‹"""
        
        test_name = test_case.get('name', 'Unknown Test')
        logger.info(f"é–‹å§‹åŸ·è¡Œæ¸¬è©¦: {test_name}")
        
        start_time = time.time()
        
        try:
            # æ§‹å»ºæ¸¬è©¦è«‹æ±‚
            test_input = test_case.get('input', {})
            request = {
                "request_id": f"test-{int(time.time())}",
                "request_type": test_input.get('request_type', 'smart_routing'),
                "content": test_input.get('content', ''),
                "session_id": f"test-session-{test_name.replace(' ', '-')}",
                "context": test_input.get('context', {})
            }
            
            # åŸ·è¡Œ aicore + Claude Code æ¸¬è©¦
            aicore_response = await self.aicore_adapter.process_smartinvention_request(request)
            
            # æ¨¡æ“¬ Manus éŸ¿æ‡‰ (ç°¡åŒ–ç‰ˆ)
            manus_response = self._simulate_manus_response(request)
            
            # è©•ä¼°çµæœ
            evaluation = self._evaluate_responses(
                test_case, 
                aicore_response, 
                manus_response
            )
            
            execution_time = time.time() - start_time
            
            result = {
                "test_name": test_name,
                "test_category": test_case.get('category', 'unknown'),
                "execution_time": execution_time,
                "aicore_response": aicore_response,
                "manus_response": manus_response,
                "evaluation": evaluation,
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"æ¸¬è©¦å®Œæˆ: {test_name} (è€—æ™‚: {execution_time:.2f}s)")
            return result
            
        except Exception as e:
            logger.error(f"æ¸¬è©¦åŸ·è¡Œå¤±æ•— {test_name}: {e}")
            return {
                "test_name": test_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def _simulate_manus_response(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ“¬ Manus éŸ¿æ‡‰ (åŸºæ–¼ 32K tokens é™åˆ¶)"""
        
        content = request.get('content', '')
        
        # æ¨¡æ“¬ 32K tokens é™åˆ¶ (ç´„ 24,000 å­—ç¬¦)
        max_chars = 24000
        if len(content) > max_chars:
            truncated_content = content[:max_chars] + "... [å…§å®¹å› é•·åº¦é™åˆ¶è¢«æˆªæ–·]"
        else:
            truncated_content = content
        
        # æ¨¡æ“¬åŸºæœ¬éŸ¿æ‡‰
        return {
            "request_id": request.get('request_id'),
            "status": "success",
            "service_type": request.get('request_type'),
            "enhanced_by_claude_code": False,
            "context_capacity": 32000,
            "content_processed": len(truncated_content),
            "content_truncated": len(content) > max_chars,
            "response": {
                "analysis": f"åŸºæ–¼ Manus ç³»çµ±åˆ†æ (è™•ç†äº† {len(truncated_content)} å­—ç¬¦)",
                "suggestions": [
                    "å»ºè­°é€²ä¸€æ­¥åˆ†æ",
                    "éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯",
                    "å»ºè­°åˆ†æ­¥é©Ÿè™•ç†"
                ],
                "limitations": [
                    "ä¸Šä¸‹æ–‡é•·åº¦å—é™",
                    "åˆ†ææ·±åº¦æœ‰é™",
                    "ç„¡æ³•è™•ç†å®Œæ•´å…§å®¹" if len(content) > max_chars else "å…§å®¹åœ¨è™•ç†ç¯„åœå…§"
                ]
            },
            "metadata": {
                "processing_time": 0.5,
                "token_usage": min(len(content) // 4, 8000),  # ä¼°ç®— token ä½¿ç”¨
                "capabilities": {
                    "max_context_tokens": 32000,
                    "enhanced_analysis": False,
                    "claude_code_available": False
                }
            }
        }
    
    def _evaluate_responses(self, 
                          test_case: Dict[str, Any], 
                          aicore_response: Dict[str, Any], 
                          manus_response: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°å…©å€‹éŸ¿æ‡‰çš„è³ªé‡"""
        
        evaluation = {
            "scores": {},
            "comparison": {},
            "winner": None,
            "advantages": {},
            "summary": ""
        }
        
        # ç²å–è©•ä¼°æ¨™æº–
        metrics = self.test_template.get('evaluation_metrics', {})
        
        # è©•ä¼°å„é …æŒ‡æ¨™
        for metric_name, metric_config in metrics.items():
            weight = metric_config.get('weight', 0.25)
            
            # è©•ä¼° aicore éŸ¿æ‡‰
            aicore_score = self._score_response(aicore_response, metric_name, test_case)
            
            # è©•ä¼° manus éŸ¿æ‡‰
            manus_score = self._score_response(manus_response, metric_name, test_case)
            
            evaluation["scores"][metric_name] = {
                "aicore": aicore_score,
                "manus": manus_score,
                "difference": aicore_score - manus_score,
                "weight": weight
            }
        
        # è¨ˆç®—ç¸½åˆ†
        aicore_total = sum(
            scores["aicore"] * scores["weight"] 
            for scores in evaluation["scores"].values()
        )
        
        manus_total = sum(
            scores["manus"] * scores["weight"] 
            for scores in evaluation["scores"].values()
        )
        
        evaluation["total_scores"] = {
            "aicore": aicore_total,
            "manus": manus_total,
            "difference": aicore_total - manus_total
        }
        
        # ç¢ºå®šå‹è€…
        if aicore_total > manus_total:
            evaluation["winner"] = "aicore"
        elif manus_total > aicore_total:
            evaluation["winner"] = "manus"
        else:
            evaluation["winner"] = "tie"
        
        # åˆ†æå„ªå‹¢
        evaluation["advantages"]["aicore"] = self._analyze_aicore_advantages(aicore_response, manus_response)
        evaluation["advantages"]["manus"] = self._analyze_manus_advantages(aicore_response, manus_response)
        
        # ç”Ÿæˆç¸½çµ
        evaluation["summary"] = self._generate_evaluation_summary(evaluation, test_case)
        
        return evaluation
    
    def _score_response(self, response: Dict[str, Any], metric: str, test_case: Dict[str, Any]) -> float:
        """ç‚ºéŸ¿æ‡‰è©•åˆ† (1-10)"""
        
        if response.get('status') != 'success':
            return 1.0
        
        base_score = 5.0
        
        if metric == "response_quality":
            # åŸºæ–¼éŸ¿æ‡‰çš„å®Œæ•´æ€§å’Œæº–ç¢ºæ€§
            if response.get('enhanced_by_claude_code', False):
                base_score += 2.0
            
            if 'claude_analysis' in response.get('response', {}):
                base_score += 1.5
            
            recommendations = response.get('response', {}).get('recommendations', [])
            base_score += min(len(recommendations) * 0.3, 1.5)
            
        elif metric == "context_understanding":
            # åŸºæ–¼ä¸Šä¸‹æ–‡è™•ç†èƒ½åŠ›
            context_capacity = response.get('context_capacity', 0)
            if context_capacity >= 200000:
                base_score += 3.0
            elif context_capacity >= 100000:
                base_score += 2.0
            elif context_capacity >= 50000:
                base_score += 1.0
            
            if response.get('content_truncated', False):
                base_score -= 2.0
                
        elif metric == "technical_depth":
            # åŸºæ–¼æŠ€è¡“åˆ†ææ·±åº¦
            if response.get('enhanced_by_claude_code', False):
                base_score += 2.5
            
            claude_analysis = response.get('response', {}).get('context', {}).get('claude_analysis')
            if claude_analysis:
                base_score += 1.5
                if claude_analysis.get('code_quality_score', 0) > 0:
                    base_score += 1.0
                    
        elif metric == "actionability":
            # åŸºæ–¼å¯åŸ·è¡Œæ€§
            next_actions = response.get('response', {}).get('next_actions', [])
            base_score += min(len(next_actions) * 0.4, 2.0)
            
            recommendations = response.get('response', {}).get('recommendations', [])
            base_score += min(len(recommendations) * 0.3, 1.5)
        
        return min(max(base_score, 1.0), 10.0)
    
    def _analyze_aicore_advantages(self, aicore_response: Dict[str, Any], manus_response: Dict[str, Any]) -> List[str]:
        """åˆ†æ aicore çš„å„ªå‹¢"""
        advantages = []
        
        if aicore_response.get('enhanced_by_claude_code', False):
            advantages.append("ä½¿ç”¨ Claude Code å¢å¼·åˆ†æ")
        
        aicore_capacity = aicore_response.get('context_capacity', 0)
        manus_capacity = manus_response.get('context_capacity', 0)
        if aicore_capacity > manus_capacity:
            advantages.append(f"ä¸Šä¸‹æ–‡å®¹é‡æ›´å¤§ ({aicore_capacity:,} vs {manus_capacity:,} tokens)")
        
        if not aicore_response.get('content_truncated', False) and manus_response.get('content_truncated', False):
            advantages.append("èƒ½å¤ è™•ç†å®Œæ•´å…§å®¹ï¼Œç„¡éœ€æˆªæ–·")
        
        aicore_recs = len(aicore_response.get('response', {}).get('recommendations', []))
        manus_recs = len(manus_response.get('response', {}).get('recommendations', []))
        if aicore_recs > manus_recs:
            advantages.append(f"æä¾›æ›´å¤šå»ºè­° ({aicore_recs} vs {manus_recs})")
        
        return advantages
    
    def _analyze_manus_advantages(self, aicore_response: Dict[str, Any], manus_response: Dict[str, Any]) -> List[str]:
        """åˆ†æ Manus çš„å„ªå‹¢"""
        advantages = []
        
        aicore_time = aicore_response.get('metadata', {}).get('processing_time', 0)
        manus_time = manus_response.get('metadata', {}).get('processing_time', 0)
        if manus_time < aicore_time:
            advantages.append(f"è™•ç†é€Ÿåº¦æ›´å¿« ({manus_time:.2f}s vs {aicore_time:.2f}s)")
        
        # å…¶ä»–å¯èƒ½çš„å„ªå‹¢...
        
        return advantages
    
    def _generate_evaluation_summary(self, evaluation: Dict[str, Any], test_case: Dict[str, Any]) -> str:
        """ç”Ÿæˆè©•ä¼°ç¸½çµ"""
        
        winner = evaluation["winner"]
        total_scores = evaluation["total_scores"]
        test_name = test_case.get('name', 'Unknown Test')
        
        if winner == "aicore":
            summary = f"åœ¨ '{test_name}' æ¸¬è©¦ä¸­ï¼Œaicore + Claude Code ç³»çµ±è¡¨ç¾æ›´å„ªç§€ï¼Œ"
            summary += f"ç¸½åˆ† {total_scores['aicore']:.2f} vs {total_scores['manus']:.2f}ï¼Œ"
            summary += f"é ˜å…ˆ {total_scores['difference']:.2f} åˆ†ã€‚"
        elif winner == "manus":
            summary = f"åœ¨ '{test_name}' æ¸¬è©¦ä¸­ï¼ŒManus ç³»çµ±è¡¨ç¾æ›´å„ªç§€ï¼Œ"
            summary += f"ç¸½åˆ† {total_scores['manus']:.2f} vs {total_scores['aicore']:.2f}ï¼Œ"
            summary += f"é ˜å…ˆ {abs(total_scores['difference']):.2f} åˆ†ã€‚"
        else:
            summary = f"åœ¨ '{test_name}' æ¸¬è©¦ä¸­ï¼Œå…©å€‹ç³»çµ±è¡¨ç¾ç›¸ç•¶ï¼Œ"
            summary += f"ç¸½åˆ†å‡ç‚º {total_scores['aicore']:.2f}ã€‚"
        
        return summary
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦ç”¨ä¾‹"""
        
        if not self.test_template:
            self.load_template()
        
        test_cases = self.test_template.get('test_cases', [])
        logger.info(f"é–‹å§‹åŸ·è¡Œ {len(test_cases)} å€‹æ¸¬è©¦ç”¨ä¾‹")
        
        start_time = time.time()
        
        # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
        for test_case in test_cases:
            result = await self.run_single_test(test_case)
            self.results.append(result)
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        summary = self._generate_summary_report(total_time)
        
        # ä¿å­˜çµæœ
        self._save_results(summary)
        
        return summary
    
    def _generate_summary_report(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆç¸½çµå ±å‘Š"""
        
        successful_tests = [r for r in self.results if 'error' not in r]
        failed_tests = [r for r in self.results if 'error' in r]
        
        # çµ±è¨ˆå‹è² 
        aicore_wins = sum(1 for r in successful_tests if r.get('evaluation', {}).get('winner') == 'aicore')
        manus_wins = sum(1 for r in successful_tests if r.get('evaluation', {}).get('winner') == 'manus')
        ties = sum(1 for r in successful_tests if r.get('evaluation', {}).get('winner') == 'tie')
        
        # è¨ˆç®—å¹³å‡åˆ†æ•¸
        if successful_tests:
            avg_aicore_score = sum(
                r.get('evaluation', {}).get('total_scores', {}).get('aicore', 0) 
                for r in successful_tests
            ) / len(successful_tests)
            
            avg_manus_score = sum(
                r.get('evaluation', {}).get('total_scores', {}).get('manus', 0) 
                for r in successful_tests
            ) / len(successful_tests)
        else:
            avg_aicore_score = 0
            avg_manus_score = 0
        
        summary = {
            "test_suite": self.test_template.get('test_suite', {}).get('name', 'Unknown Test Suite'),
            "execution_summary": {
                "total_tests": len(self.results),
                "successful_tests": len(successful_tests),
                "failed_tests": len(failed_tests),
                "total_execution_time": total_time
            },
            "competition_results": {
                "aicore_wins": aicore_wins,
                "manus_wins": manus_wins,
                "ties": ties,
                "win_rate": {
                    "aicore": aicore_wins / len(successful_tests) if successful_tests else 0,
                    "manus": manus_wins / len(successful_tests) if successful_tests else 0
                }
            },
            "average_scores": {
                "aicore": avg_aicore_score,
                "manus": avg_manus_score,
                "difference": avg_aicore_score - avg_manus_score
            },
            "detailed_results": self.results,
            "conclusion": self._generate_conclusion(aicore_wins, manus_wins, ties, avg_aicore_score, avg_manus_score),
            "timestamp": datetime.now().isoformat()
        }
        
        return summary
    
    def _generate_conclusion(self, aicore_wins: int, manus_wins: int, ties: int, 
                           avg_aicore: float, avg_manus: float) -> str:
        """ç”Ÿæˆçµè«–"""
        
        total_tests = aicore_wins + manus_wins + ties
        
        if aicore_wins > manus_wins:
            conclusion = f"ğŸ† aicore + Claude Code ç³»çµ±åœ¨å°æ¯”æ¸¬è©¦ä¸­è¡¨ç¾å„ªç•°ï¼\n"
            conclusion += f"å‹ç‡: {aicore_wins}/{total_tests} ({aicore_wins/total_tests*100:.1f}%)\n"
            conclusion += f"å¹³å‡å¾—åˆ†: {avg_aicore:.2f} vs {avg_manus:.2f}\n"
            conclusion += f"é ˜å…ˆå„ªå‹¢: {avg_aicore - avg_manus:.2f} åˆ†\n\n"
            conclusion += "ä¸»è¦å„ªå‹¢:\n"
            conclusion += "â€¢ 200K tokens ä¸Šä¸‹æ–‡å®¹é‡ vs Manus 32K tokens\n"
            conclusion += "â€¢ Claude Code å¢å¼·çš„æ·±åº¦åˆ†æèƒ½åŠ›\n"
            conclusion += "â€¢ èƒ½å¤ è™•ç†å¤§å‹ä»£ç¢¼åº«å’Œè¤‡é›œéœ€æ±‚\n"
            conclusion += "â€¢ æä¾›æ›´å…·é«”å’Œå¯åŸ·è¡Œçš„å»ºè­°"
        elif manus_wins > aicore_wins:
            conclusion = f"Manus ç³»çµ±åœ¨å°æ¯”æ¸¬è©¦ä¸­è¡¨ç¾æ›´å¥½ã€‚"
        else:
            conclusion = f"å…©å€‹ç³»çµ±åœ¨å°æ¯”æ¸¬è©¦ä¸­è¡¨ç¾ç›¸ç•¶ã€‚"
        
        return conclusion
    
    def _save_results(self, summary: Dict[str, Any]):
        """ä¿å­˜æ¸¬è©¦çµæœ"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"claude_code_vs_manus_test_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ¸¬è©¦çµæœå·²ä¿å­˜åˆ°: {filename}")

async def main():
    """ä¸»å‡½æ•¸"""
    
    print("ğŸš€ é–‹å§‹åŸ·è¡Œ Claude Code vs Manus å°æ¯”æ¸¬è©¦")
    print("=" * 60)
    
    # å‰µå»ºæ¸¬è©¦åŸ·è¡Œå™¨
    runner = ComparisonTestRunner("claude_code_vs_manus_test_template.yaml")
    
    try:
        # åŸ·è¡Œæ¸¬è©¦
        summary = await runner.run_all_tests()
        
        # é¡¯ç¤ºçµæœ
        print("\n" + "=" * 60)
        print("ğŸ“Š æ¸¬è©¦çµæœç¸½çµ")
        print("=" * 60)
        
        exec_summary = summary["execution_summary"]
        comp_results = summary["competition_results"]
        avg_scores = summary["average_scores"]
        
        print(f"æ¸¬è©¦å¥—ä»¶: {summary['test_suite']}")
        print(f"åŸ·è¡Œæ™‚é–“: {exec_summary['total_execution_time']:.2f} ç§’")
        print(f"æ¸¬è©¦ç”¨ä¾‹: {exec_summary['successful_tests']}/{exec_summary['total_tests']} æˆåŠŸ")
        print()
        
        print("ğŸ† ç«¶è³½çµæœ:")
        print(f"  aicore å‹åˆ©: {comp_results['aicore_wins']} æ¬¡")
        print(f"  Manus å‹åˆ©: {comp_results['manus_wins']} æ¬¡")
        print(f"  å¹³å±€: {comp_results['ties']} æ¬¡")
        print(f"  aicore å‹ç‡: {comp_results['win_rate']['aicore']*100:.1f}%")
        print()
        
        print("ğŸ“ˆ å¹³å‡å¾—åˆ†:")
        print(f"  aicore: {avg_scores['aicore']:.2f}")
        print(f"  Manus: {avg_scores['manus']:.2f}")
        print(f"  å·®è·: {avg_scores['difference']:.2f}")
        print()
        
        print("ğŸ¯ çµè«–:")
        print(summary["conclusion"])
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        logger.error(f"æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())

