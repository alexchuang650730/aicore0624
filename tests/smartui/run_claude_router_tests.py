#!/usr/bin/env python3
"""
Claude Code è·¯ç”±å™¨æ¸¬è©¦åŸ·è¡Œå™¨
åŸºæ–¼ tests/templates æ ¼å¼åŸ·è¡Œå®Œæ•´æ¸¬è©¦
"""

import asyncio
import yaml
import json
import time
from typing import Dict, Any, List
from datetime import datetime
from claude_code_real_router import ClaudeCodeRealRouter

class TestExecutor:
    """æ¸¬è©¦åŸ·è¡Œå™¨"""
    
    def __init__(self, test_config_path: str):
        with open(test_config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        self.router = ClaudeCodeRealRouter()
        self.results = []
        
    async def run_all_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print(f"ğŸš€ é–‹å§‹åŸ·è¡Œæ¸¬è©¦å¥—ä»¶: {self.config['test_suite']['name']}")
        print(f"ğŸ“ æè¿°: {self.config['test_suite']['description']}")
        print("="*60)
        
        start_time = time.time()
        
        # åŸ·è¡ŒåŸºæœ¬åŠŸèƒ½æ¸¬è©¦
        basic_results = await self._run_basic_tests()
        
        # åŸ·è¡Œæ€§èƒ½æ¸¬è©¦
        performance_results = await self._run_performance_tests()
        
        # åŸ·è¡Œå°æ¯”æ¸¬è©¦
        comparison_results = await self._run_comparison_tests()
        
        # åŸ·è¡Œå°ˆå®¶é©—è­‰æ¸¬è©¦
        expert_results = await self._run_expert_validation()
        
        end_time = time.time()
        
        # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
        report = self._generate_test_report(
            basic_results, performance_results, 
            comparison_results, expert_results,
            end_time - start_time
        )
        
        return report
    
    async def _run_basic_tests(self) -> List[Dict[str, Any]]:
        """åŸ·è¡ŒåŸºæœ¬åŠŸèƒ½æ¸¬è©¦"""
        print("\nğŸ“‹ åŸ·è¡ŒåŸºæœ¬åŠŸèƒ½æ¸¬è©¦")
        print("-" * 40)
        
        results = []
        test_cases = self.config.get('test_cases', [])
        
        for i, test_case in enumerate(test_cases):
            print(f"\nğŸ” æ¸¬è©¦ {i+1}/{len(test_cases)}: {test_case['name']}")
            
            try:
                # åŸ·è¡Œæ¸¬è©¦æ­¥é©Ÿ
                step = test_case['steps'][0]  # å‡è¨­æ¯å€‹æ¸¬è©¦åªæœ‰ä¸€å€‹æ­¥é©Ÿ
                payload = step['payload']
                
                start_time = time.time()
                response = await self.router.analyze_and_route(
                    payload['user_input'], 
                    payload['context']
                )
                end_time = time.time()
                
                # é©—è­‰çµæœ
                validation_results = self._validate_response(response, step.get('validation', []))
                
                result = {
                    "test_name": test_case['name'],
                    "status": "PASS" if all(validation_results.values()) else "FAIL",
                    "response_time": end_time - start_time,
                    "response": response,
                    "validations": validation_results
                }
                
                results.append(result)
                
                # æ‰“å°çµæœ
                status_icon = "âœ…" if result["status"] == "PASS" else "âŒ"
                print(f"{status_icon} {result['status']} - {result['response_time']:.2f}s")
                
                if response['status'] == 'success':
                    analysis = response['scenario_analysis']
                    print(f"   å ´æ™¯: {analysis['scenario_type']}")
                    print(f"   è¤‡é›œåº¦: {analysis['complexity_level']}")
                    if analysis['recommended_experts']:
                        expert = analysis['recommended_experts'][0]
                        print(f"   å°ˆå®¶: {expert['expert_type']} (ä¿¡å¿ƒåº¦: {expert['confidence']})")
                
            except Exception as e:
                result = {
                    "test_name": test_case['name'],
                    "status": "ERROR",
                    "error": str(e),
                    "response_time": 0
                }
                results.append(result)
                print(f"âŒ ERROR - {str(e)}")
        
        return results
    
    async def _run_performance_tests(self) -> List[Dict[str, Any]]:
        """åŸ·è¡Œæ€§èƒ½æ¸¬è©¦"""
        print("\nâš¡ åŸ·è¡Œæ€§èƒ½æ¸¬è©¦")
        print("-" * 40)
        
        results = []
        performance_tests = self.config.get('performance_tests', [])
        
        for test in performance_tests:
            print(f"\nğŸ” æ€§èƒ½æ¸¬è©¦: {test['name']}")
            
            try:
                payload = test['payload']
                expected_perf = test['expected_performance']
                
                start_time = time.time()
                response = await self.router.analyze_and_route(
                    payload['user_input'], 
                    payload['context']
                )
                end_time = time.time()
                
                response_time_ms = (end_time - start_time) * 1000
                max_time_ms = expected_perf['max_response_time']
                
                result = {
                    "test_name": test['name'],
                    "status": "PASS" if response_time_ms <= max_time_ms else "FAIL",
                    "response_time_ms": response_time_ms,
                    "max_allowed_ms": max_time_ms,
                    "response": response
                }
                
                results.append(result)
                
                status_icon = "âœ…" if result["status"] == "PASS" else "âŒ"
                print(f"{status_icon} {result['status']} - {response_time_ms:.0f}ms (é™åˆ¶: {max_time_ms}ms)")
                
            except Exception as e:
                result = {
                    "test_name": test['name'],
                    "status": "ERROR",
                    "error": str(e)
                }
                results.append(result)
                print(f"âŒ ERROR - {str(e)}")
        
        return results
    
    async def _run_comparison_tests(self) -> List[Dict[str, Any]]:
        """åŸ·è¡Œå°æ¯”æ¸¬è©¦"""
        print("\nğŸ”„ åŸ·è¡Œå°æ¯”æ¸¬è©¦ (Claude Code vs å‚³çµ±è·¯ç”±)")
        print("-" * 40)
        
        results = []
        comparison_tests = self.config.get('comparison_tests', [])
        
        for test in comparison_tests:
            print(f"\nğŸ” å°æ¯”æ¸¬è©¦: {test['name']}")
            
            scenarios = test.get('scenarios', [])
            for scenario in scenarios:
                try:
                    response = await self.router.analyze_and_route(scenario['input'], {})
                    
                    if response['status'] == 'success':
                        analysis = response['scenario_analysis']
                        expected_claude = scenario['expected_claude_code']
                        
                        # æª¢æŸ¥å ´æ™¯è­˜åˆ¥æº–ç¢ºæ€§
                        scenario_match = analysis['scenario_type'] == expected_claude['scenario_type']
                        
                        # æª¢æŸ¥å°ˆå®¶æ¨è–¦æº–ç¢ºæ€§
                        expert_match = False
                        if analysis['recommended_experts']:
                            expert_match = analysis['recommended_experts'][0]['expert_type'] == expected_claude['expert_type']
                        
                        # æª¢æŸ¥ä¿¡å¿ƒåº¦
                        confidence_ok = analysis['confidence_score'] >= float(expected_claude['confidence'].replace('>= ', ''))
                        
                        result = {
                            "input": scenario['input'][:50] + "...",
                            "scenario_match": scenario_match,
                            "expert_match": expert_match,
                            "confidence_ok": confidence_ok,
                            "actual_scenario": analysis['scenario_type'],
                            "actual_expert": analysis['recommended_experts'][0]['expert_type'] if analysis['recommended_experts'] else "none",
                            "actual_confidence": analysis['confidence_score']
                        }
                        
                        results.append(result)
                        
                        status = "âœ… PASS" if all([scenario_match, expert_match, confidence_ok]) else "âŒ FAIL"
                        print(f"{status} - å ´æ™¯: {scenario_match}, å°ˆå®¶: {expert_match}, ä¿¡å¿ƒåº¦: {confidence_ok}")
                        
                except Exception as e:
                    print(f"âŒ ERROR - {str(e)}")
        
        return results
    
    async def _run_expert_validation(self) -> List[Dict[str, Any]]:
        """åŸ·è¡Œå°ˆå®¶é©—è­‰æ¸¬è©¦"""
        print("\nğŸ‘¨â€ğŸ’¼ åŸ·è¡Œå°ˆå®¶é©—è­‰æ¸¬è©¦")
        print("-" * 40)
        
        results = []
        expert_validations = self.config.get('expert_validation', [])
        
        for expert_config in expert_validations:
            expert_type = expert_config['expert_type']
            test_scenarios = expert_config['test_scenarios']
            expected_keywords = expert_config['expected_keywords']
            
            print(f"\nğŸ” é©—è­‰å°ˆå®¶: {expert_type}")
            
            expert_results = []
            
            for scenario in test_scenarios:
                try:
                    response = await self.router.analyze_and_route(scenario, {})
                    
                    if response['status'] == 'success':
                        # æª¢æŸ¥æ˜¯å¦æ¨è–¦äº†æ­£ç¢ºçš„å°ˆå®¶
                        recommended_experts = response['scenario_analysis']['recommended_experts']
                        correct_expert = any(exp['expert_type'] == expert_type for exp in recommended_experts)
                        
                        # æª¢æŸ¥å°ˆå®¶å›æ‡‰ä¸­æ˜¯å¦åŒ…å«é æœŸé—œéµè©
                        expert_response = ""
                        if 'processing_result' in response and 'expert_analysis' in response['processing_result']:
                            expert_response = response['processing_result']['expert_analysis'].get('expert_response', '').lower()
                        
                        keyword_matches = sum(1 for keyword in expected_keywords if keyword.lower() in expert_response)
                        keyword_score = keyword_matches / len(expected_keywords)
                        
                        expert_results.append({
                            "scenario": scenario,
                            "correct_expert": correct_expert,
                            "keyword_score": keyword_score,
                            "keyword_matches": keyword_matches
                        })
                        
                        status = "âœ…" if correct_expert and keyword_score >= 0.5 else "âŒ"
                        print(f"{status} {scenario[:30]}... - å°ˆå®¶: {correct_expert}, é—œéµè©: {keyword_matches}/{len(expected_keywords)}")
                        
                except Exception as e:
                    print(f"âŒ ERROR - {str(e)}")
            
            results.append({
                "expert_type": expert_type,
                "results": expert_results
            })
        
        return results
    
    def _validate_response(self, response: Dict[str, Any], validations: List[str]) -> Dict[str, bool]:
        """é©—è­‰å›æ‡‰çµæœ"""
        results = {}
        
        for validation in validations:
            try:
                # ç°¡å–®çš„é©—è­‰é‚è¼¯
                if "response.status == 'success'" in validation:
                    results[validation] = response.get('status') == 'success'
                elif "scenario_type ==" in validation:
                    expected_type = validation.split("== '")[1].split("'")[0]
                    actual_type = response.get('scenario_analysis', {}).get('scenario_type', '')
                    results[validation] = actual_type == expected_type
                elif "complexity_level in" in validation:
                    allowed_levels = validation.split("in ")[1].strip("[]").replace("'", "").split(", ")
                    actual_level = response.get('scenario_analysis', {}).get('complexity_level', '')
                    results[validation] = actual_level in allowed_levels
                elif "len(response.scenario_analysis.recommended_experts)" in validation:
                    experts = response.get('scenario_analysis', {}).get('recommended_experts', [])
                    expected_count = int(validation.split(">= ")[1])
                    results[validation] = len(experts) >= expected_count
                elif "expert_type ==" in validation:
                    expected_expert = validation.split("== '")[1].split("'")[0]
                    experts = response.get('scenario_analysis', {}).get('recommended_experts', [])
                    actual_expert = experts[0].get('expert_type', '') if experts else ''
                    results[validation] = actual_expert == expected_expert
                elif "confidence_score >=" in validation:
                    expected_score = float(validation.split(">= ")[1])
                    actual_score = response.get('scenario_analysis', {}).get('confidence_score', 0)
                    results[validation] = actual_score >= expected_score
                else:
                    # å…¶ä»–é©—è­‰é‚è¼¯
                    results[validation] = True
                    
            except Exception as e:
                results[validation] = False
        
        return results
    
    def _generate_test_report(self, basic_results: List, performance_results: List, 
                            comparison_results: List, expert_results: List, 
                            total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        total_basic = len(basic_results)
        passed_basic = sum(1 for r in basic_results if r.get('status') == 'PASS')
        
        total_performance = len(performance_results)
        passed_performance = sum(1 for r in performance_results if r.get('status') == 'PASS')
        
        # è¨ˆç®—æˆåŠŸç‡
        basic_success_rate = (passed_basic / total_basic * 100) if total_basic > 0 else 0
        performance_success_rate = (passed_performance / total_performance * 100) if total_performance > 0 else 0
        
        report = {
            "test_summary": {
                "test_suite": self.config['test_suite']['name'],
                "total_execution_time": f"{total_time:.2f}s",
                "timestamp": datetime.now().isoformat()
            },
            "basic_tests": {
                "total": total_basic,
                "passed": passed_basic,
                "failed": total_basic - passed_basic,
                "success_rate": f"{basic_success_rate:.1f}%",
                "results": basic_results
            },
            "performance_tests": {
                "total": total_performance,
                "passed": passed_performance,
                "failed": total_performance - passed_performance,
                "success_rate": f"{performance_success_rate:.1f}%",
                "results": performance_results
            },
            "comparison_tests": {
                "results": comparison_results
            },
            "expert_validation": {
                "results": expert_results
            },
            "overall_assessment": {
                "claude_code_routing_effectiveness": "HIGH" if basic_success_rate >= 80 else "MEDIUM" if basic_success_rate >= 60 else "LOW",
                "expert_recommendation_accuracy": "HIGH" if passed_basic >= total_basic * 0.8 else "MEDIUM",
                "performance_satisfaction": "GOOD" if performance_success_rate >= 80 else "FAIR"
            }
        }
        
        return report

async def main():
    """ä¸»å‡½æ•¸"""
    executor = TestExecutor('/home/ubuntu/claude_code_router_test.yaml')
    report = await executor.run_all_tests()
    
    # ä¿å­˜æ¸¬è©¦å ±å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"/home/ubuntu/claude_router_test_report_{timestamp}.json"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç¸½çµ
    print("\n" + "="*60)
    print("ğŸ“Š æ¸¬è©¦ç¸½çµå ±å‘Š")
    print("="*60)
    print(f"âœ… åŸºæœ¬åŠŸèƒ½æ¸¬è©¦: {report['basic_tests']['success_rate']} ({report['basic_tests']['passed']}/{report['basic_tests']['total']})")
    print(f"âš¡ æ€§èƒ½æ¸¬è©¦: {report['performance_tests']['success_rate']} ({report['performance_tests']['passed']}/{report['performance_tests']['total']})")
    print(f"ğŸ¯ Claude Code è·¯ç”±æ•ˆæœ: {report['overall_assessment']['claude_code_routing_effectiveness']}")
    print(f"ğŸ‘¨â€ğŸ’¼ å°ˆå®¶æ¨è–¦æº–ç¢ºæ€§: {report['overall_assessment']['expert_recommendation_accuracy']}")
    print(f"ğŸ“ˆ æ€§èƒ½è¡¨ç¾: {report['overall_assessment']['performance_satisfaction']}")
    print(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {report['test_summary']['total_execution_time']}")
    print(f"ğŸ“„ è©³ç´°å ±å‘Š: {report_file}")
    
    return report

if __name__ == "__main__":
    asyncio.run(main())

