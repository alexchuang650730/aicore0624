#!/usr/bin/env python3
"""
å°æ¯”å¼•æ“æ¸¬è©¦è…³æœ¬
æ¯”è¼ƒ Claude SDK é›†æˆç³»çµ± vs åŸå§‹ Manus ç³»çµ±çš„èƒ½åŠ›å·®ç•°
"""

import asyncio
import json
import time
import requests
from typing import Dict, List, Any, Tuple
from datetime import datetime
import statistics

class ComparisonEngine:
    """å°æ¯”å¼•æ“ - é‡åŒ–æ¯”è¼ƒå…©å€‹ç³»çµ±çš„èƒ½åŠ›å·®ç•°"""
    
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.test_results = []
        
    def calculate_response_quality_score(self, response: Dict[str, Any]) -> float:
        """è¨ˆç®—å›æ‡‰è³ªé‡è©•åˆ† (0-10)"""
        score = 0.0
        
        # åŸºç¤åˆ†æ•¸
        if 'error' not in response:
            score += 2.0
        
        # å…§å®¹è±å¯Œåº¦
        if isinstance(response.get('analysis'), dict):
            score += 2.0
        if isinstance(response.get('recommendations'), list) and len(response.get('recommendations', [])) > 0:
            score += 2.0
        if isinstance(response.get('tools_used'), list) and len(response.get('tools_used', [])) > 0:
            score += 1.0
            
        # çµæ§‹åŒ–ç¨‹åº¦
        if 'timestamp' in response:
            score += 0.5
        if 'system_type' in response:
            score += 0.5
            
        # Claude Code ç‰¹å®šåŠŸèƒ½
        if 'claude_code_analysis' in response:
            score += 1.5
        if response.get('system_type') == 'fully_integrated_intelligent_system_asgi':
            score += 0.5
            
        return min(score, 10.0)
    
    def calculate_performance_score(self, response_time: float, cache_hit: bool = False) -> float:
        """è¨ˆç®—æ€§èƒ½è©•åˆ† (0-10)"""
        # åŸºæº–ï¼š1ç§’å…§ = 10åˆ†ï¼Œ2ç§’å…§ = 8åˆ†ï¼Œ3ç§’å…§ = 6åˆ†ï¼Œä»¥æ­¤é¡æ¨
        if cache_hit:
            return 10.0  # ç·©å­˜å‘½ä¸­çµ¦æ»¿åˆ†
            
        if response_time <= 1.0:
            return 10.0
        elif response_time <= 2.0:
            return 8.0
        elif response_time <= 3.0:
            return 6.0
        elif response_time <= 5.0:
            return 4.0
        else:
            return max(2.0, 10.0 - response_time)
    
    def calculate_feature_completeness_score(self, response: Dict[str, Any]) -> float:
        """è¨ˆç®—åŠŸèƒ½å®Œæ•´æ€§è©•åˆ† (0-10)"""
        features = []
        
        # æª¢æŸ¥å„ç¨®åŠŸèƒ½
        if 'analysis' in response:
            features.append('analysis')
        if 'recommendations' in response:
            features.append('recommendations')
        if 'tools_used' in response:
            features.append('tools_used')
        if 'claude_code_analysis' in response:
            features.append('claude_code')
        if response.get('from_cache'):
            features.append('caching')
        if 'smartinvention' in str(response).lower():
            features.append('smartinvention')
        if 'manus_direct_response' in response:
            features.append('manus_integration')
            
        # æ¯å€‹åŠŸèƒ½ 1.43 åˆ† (7å€‹åŠŸèƒ½ = 10åˆ†)
        return min(len(features) * 1.43, 10.0)
    
    async def test_our_system(self, test_input: str, context: Dict[str, Any]) -> Tuple[Dict[str, Any], float]:
        """æ¸¬è©¦æˆ‘å€‘çš„ Claude SDK é›†æˆç³»çµ±"""
        start_time = time.time()
        
        try:
            # èª¿ç”¨ Claude Code åˆ†æç«¯é»
            response = requests.post(
                f"{self.base_url}/api/claude_code/analyze",
                json={
                    "requirements_text": test_input,
                    "user_role": context.get("user_role", "user")
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
            else:
                # å¦‚æœ Claude Code ç«¯é»ä¸å¯ç”¨ï¼Œä½¿ç”¨é€šç”¨è™•ç†ç«¯é»
                response = requests.post(
                    f"{self.base_url}/api/process",
                    json={
                        "request": test_input,
                        "context": {**context, "use_claude_code": True}
                    },
                    timeout=30
                )
                result = response.json() if response.status_code == 200 else {"error": f"HTTP {response.status_code}"}
                
        except Exception as e:
            result = {"error": str(e)}
            
        response_time = time.time() - start_time
        return result, response_time
    
    async def test_manus_system(self, test_input: str, context: Dict[str, Any]) -> Tuple[Dict[str, Any], float]:
        """æ¸¬è©¦åŸå§‹ Manus ç³»çµ±ï¼ˆæ¨¡æ“¬åŸºç¤åŠŸèƒ½ï¼‰"""
        start_time = time.time()
        
        try:
            # èª¿ç”¨åŸºç¤è™•ç†ç«¯é»ï¼Œä¸ä½¿ç”¨ Claude Code
            basic_context = {k: v for k, v in context.items() if k != "use_claude_code"}
            
            response = requests.post(
                f"{self.base_url}/api/process",
                json={
                    "request": test_input,
                    "context": basic_context
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                # ç§»é™¤ Claude Code ç‰¹å®šåŠŸèƒ½ä¾†æ¨¡æ“¬åŸå§‹ç³»çµ±
                if 'claude_code_analysis' in result:
                    del result['claude_code_analysis']
                result['system_type'] = 'basic_manus_system'
            else:
                result = {"error": f"HTTP {response.status_code}"}
                
        except Exception as e:
            result = {"error": str(e)}
            
        response_time = time.time() - start_time
        return result, response_time
    
    async def run_comparison_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹å°æ¯”æ¸¬è©¦"""
        test_id = test_case.get("test_id", "unknown")
        test_input = test_case.get("input", "")
        context = test_case.get("context", {})
        
        print(f"\nğŸ§ª åŸ·è¡Œæ¸¬è©¦: {test_id}")
        print(f"è¼¸å…¥: {test_input[:100]}...")
        
        # æ¸¬è©¦æˆ‘å€‘çš„ç³»çµ±
        our_result, our_time = await self.test_our_system(test_input, context)
        
        # æ¸¬è©¦ Manus ç³»çµ±
        manus_result, manus_time = await self.test_manus_system(test_input, context)
        
        # è¨ˆç®—è©•åˆ†
        our_quality = self.calculate_response_quality_score(our_result)
        manus_quality = self.calculate_response_quality_score(manus_result)
        
        our_performance = self.calculate_performance_score(our_time, our_result.get('from_cache', False))
        manus_performance = self.calculate_performance_score(manus_time, manus_result.get('from_cache', False))
        
        our_completeness = self.calculate_feature_completeness_score(our_result)
        manus_completeness = self.calculate_feature_completeness_score(manus_result)
        
        # è¨ˆç®—ç¸½åˆ†
        our_total = (our_quality + our_performance + our_completeness) / 3
        manus_total = (manus_quality + manus_performance + manus_completeness) / 3
        
        # è¨ˆç®—æ”¹é€²ç™¾åˆ†æ¯”
        improvement = ((our_total - manus_total) / manus_total * 100) if manus_total > 0 else 0
        
        comparison_result = {
            "test_id": test_id,
            "test_input": test_input,
            "our_system": {
                "response": our_result,
                "response_time": our_time,
                "quality_score": our_quality,
                "performance_score": our_performance,
                "completeness_score": our_completeness,
                "total_score": our_total
            },
            "manus_system": {
                "response": manus_result,
                "response_time": manus_time,
                "quality_score": manus_quality,
                "performance_score": manus_performance,
                "completeness_score": manus_completeness,
                "total_score": manus_total
            },
            "comparison": {
                "improvement_percentage": improvement,
                "quality_improvement": our_quality - manus_quality,
                "performance_improvement": our_performance - manus_performance,
                "completeness_improvement": our_completeness - manus_completeness,
                "winner": "our_system" if our_total > manus_total else "manus_system" if manus_total > our_total else "tie"
            },
            "timestamp": datetime.now().isoformat()
        }
        
        self.test_results.append(comparison_result)
        
        # è¼¸å‡ºæ¸¬è©¦çµæœ
        print(f"âœ… æˆ‘å€‘çš„ç³»çµ±: {our_total:.2f}/10 (è³ªé‡:{our_quality:.1f}, æ€§èƒ½:{our_performance:.1f}, å®Œæ•´æ€§:{our_completeness:.1f})")
        print(f"âšª Manus ç³»çµ±: {manus_total:.2f}/10 (è³ªé‡:{manus_quality:.1f}, æ€§èƒ½:{manus_performance:.1f}, å®Œæ•´æ€§:{manus_completeness:.1f})")
        print(f"ğŸ“ˆ æ”¹é€²å¹…åº¦: {improvement:+.1f}%")
        
        return comparison_result
    
    def generate_summary_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¸½çµå ±å‘Š"""
        if not self.test_results:
            return {"error": "No test results available"}
        
        # è¨ˆç®—å¹³å‡åˆ†æ•¸
        our_scores = [r["our_system"]["total_score"] for r in self.test_results]
        manus_scores = [r["manus_system"]["total_score"] for r in self.test_results]
        improvements = [r["comparison"]["improvement_percentage"] for r in self.test_results]
        
        # çµ±è¨ˆå‹è² 
        wins = sum(1 for r in self.test_results if r["comparison"]["winner"] == "our_system")
        losses = sum(1 for r in self.test_results if r["comparison"]["winner"] == "manus_system")
        ties = sum(1 for r in self.test_results if r["comparison"]["winner"] == "tie")
        
        return {
            "summary": {
                "total_tests": len(self.test_results),
                "our_average_score": statistics.mean(our_scores),
                "manus_average_score": statistics.mean(manus_scores),
                "average_improvement": statistics.mean(improvements),
                "win_rate": wins / len(self.test_results) * 100,
                "wins": wins,
                "losses": losses,
                "ties": ties
            },
            "detailed_results": self.test_results,
            "generated_at": datetime.now().isoformat()
        }

async def main():
    """ä¸»å‡½æ•¸ - åŸ·è¡Œæ‰€æœ‰å°æ¯”æ¸¬è©¦"""
    print("ğŸš€ é–‹å§‹åŸ·è¡Œå°æ¯”å¼•æ“æ¸¬è©¦...")
    
    # åˆå§‹åŒ–å°æ¯”å¼•æ“
    engine = ComparisonEngine()
    
    # å®šç¾©æ¸¬è©¦ç”¨ä¾‹
    test_cases = [
        {
            "test_id": "CMP-001",
            "input": "é–‹ç™¼ä¸€å€‹åœ¨ç·šæ•™è‚²å¹³å°ï¼Œéœ€è¦æ”¯æŒè¦–é »ç›´æ’­ã€äº’å‹•ç™½æ¿ã€ä½œæ¥­æäº¤ã€æˆç¸¾ç®¡ç†ã€æ”¯ä»˜ç³»çµ±ç­‰åŠŸèƒ½",
            "context": {
                "user_role": "developer",
                "task_type": "requirement_analysis"
            }
        },
        {
            "test_id": "CMP-002", 
            "input": "å¹«æˆ‘å¯«ä¸€å€‹ React çµ„ä»¶ï¼Œå¯¦ç¾ç”¨æˆ¶ç™»éŒ„è¡¨å–®ï¼ŒåŒ…æ‹¬ç”¨æˆ¶åã€å¯†ç¢¼è¼¸å…¥ï¼Œè¨˜ä½å¯†ç¢¼åŠŸèƒ½ï¼Œä»¥åŠè¡¨å–®é©—è­‰",
            "context": {
                "user_role": "developer",
                "task_type": "code_generation"
            }
        },
        {
            "test_id": "CMP-003",
            "input": "æˆ‘éœ€è¦è¨­è¨ˆä¸€å€‹å¾®æœå‹™æ¶æ§‹ï¼Œæ”¯æŒé«˜ä½µç™¼ç”¨æˆ¶è¨ªå•ï¼ŒåŒ…æ‹¬è² è¼‰å‡è¡¡ã€æ•¸æ“šåº«åˆ†ç‰‡ã€ç·©å­˜ç­–ç•¥ç­‰",
            "context": {
                "user_role": "architect",
                "task_type": "architecture_design"
            }
        },
        {
            "test_id": "CMP-004",
            "input": "ç³»çµ±å‡ºç¾æ€§èƒ½å•é¡Œï¼ŒAPI éŸ¿æ‡‰æ™‚é–“éé•·ï¼Œéœ€è¦è¨ºæ–·å’Œå„ªåŒ–å»ºè­°",
            "context": {
                "user_role": "user",
                "task_type": "problem_diagnosis"
            }
        },
        {
            "test_id": "CMP-005",
            "input": "å¦‚ä½•å„ªåŒ– Python ä»£ç¢¼çš„åŸ·è¡Œæ•ˆç‡ï¼Œç‰¹åˆ¥æ˜¯æ•¸æ“šè™•ç†å’Œç®—æ³•éƒ¨åˆ†",
            "context": {
                "user_role": "developer",
                "task_type": "performance_optimization"
            }
        }
    ]
    
    # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
    for test_case in test_cases:
        await engine.run_comparison_test(test_case)
        await asyncio.sleep(1)  # é¿å…è«‹æ±‚éæ–¼é »ç¹
    
    # ç”Ÿæˆç¸½çµå ±å‘Š
    summary = engine.generate_summary_report()
    
    # è¼¸å‡ºç¸½çµ
    print("\n" + "="*80)
    print("ğŸ“Š å°æ¯”æ¸¬è©¦ç¸½çµå ±å‘Š")
    print("="*80)
    print(f"ç¸½æ¸¬è©¦æ•¸: {summary['summary']['total_tests']}")
    print(f"æˆ‘å€‘çš„å¹³å‡åˆ†: {summary['summary']['our_average_score']:.2f}/10")
    print(f"Manus å¹³å‡åˆ†: {summary['summary']['manus_average_score']:.2f}/10")
    print(f"å¹³å‡æ”¹é€²å¹…åº¦: {summary['summary']['average_improvement']:+.1f}%")
    print(f"å‹ç‡: {summary['summary']['win_rate']:.1f}% ({summary['summary']['wins']}/{summary['summary']['total_tests']})")
    
    # ä¿å­˜è©³ç´°å ±å‘Š
    with open('/home/ubuntu/comparison_test_results.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: /home/ubuntu/comparison_test_results.json")
    
    return summary

if __name__ == "__main__":
    asyncio.run(main())

