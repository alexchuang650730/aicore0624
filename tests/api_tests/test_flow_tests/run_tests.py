#!/usr/bin/env python3
"""
test_flow_mcp API æ¸¬è©¦åŸ·è¡Œè…³æœ¬
æä¾›å¤šç¨®æ¸¬è©¦åŸ·è¡Œæ¨¡å¼å’Œå ±å‘Šç”ŸæˆåŠŸèƒ½
"""

import os
import sys
import json
import argparse
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

class TestRunner:
    """æ¸¬è©¦åŸ·è¡Œå™¨"""
    
    def __init__(self, config_file: str = "test_config.json"):
        self.config_file = config_file
        self.config = self.load_config()
        self.results_dir = Path("test_results")
        self.results_dir.mkdir(exist_ok=True)
    
    def load_config(self) -> Dict:
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"é…ç½®æ–‡ä»¶æ ¼å¼éŒ¯èª¤: {e}")
            sys.exit(1)
    
    def run_smoke_tests(self) -> bool:
        """åŸ·è¡Œå†’ç…™æ¸¬è©¦"""
        print("ğŸ”¥ åŸ·è¡Œå†’ç…™æ¸¬è©¦...")
        
        cmd = [
            "python", "-m", "pytest",
            "test_api_suite.py::TestExecuteAPI::test_requirement_analysis_basic",
            "test_api_suite.py::TestSystemAPI::test_system_health_check",
            "-v", "--tb=short",
            f"--junitxml={self.results_dir}/smoke_test_results.xml"
        ]
        
        return self._run_pytest_command(cmd, "smoke_test")
    
    def run_regression_tests(self) -> bool:
        """åŸ·è¡Œå›æ­¸æ¸¬è©¦"""
        print("ğŸ”„ åŸ·è¡Œå›æ­¸æ¸¬è©¦...")
        
        cmd = [
            "python", "-m", "pytest",
            "test_api_suite.py",
            "-m", "not slow and not performance",
            "-v", "--tb=short",
            f"--junitxml={self.results_dir}/regression_test_results.xml"
        ]
        
        return self._run_pytest_command(cmd, "regression_test")
    
    def run_performance_tests(self) -> bool:
        """åŸ·è¡Œæ€§èƒ½æ¸¬è©¦"""
        print("âš¡ åŸ·è¡Œæ€§èƒ½æ¸¬è©¦...")
        
        cmd = [
            "python", "-m", "pytest",
            "test_api_suite.py::TestPerformance",
            "-v", "--tb=short",
            f"--junitxml={self.results_dir}/performance_test_results.xml"
        ]
        
        return self._run_pytest_command(cmd, "performance_test")
    
    def run_security_tests(self) -> bool:
        """åŸ·è¡Œå®‰å…¨æ¸¬è©¦"""
        print("ğŸ”’ åŸ·è¡Œå®‰å…¨æ¸¬è©¦...")
        
        cmd = [
            "python", "-m", "pytest",
            "test_api_suite.py::TestExecuteAPINegative",
            "-v", "--tb=short",
            f"--junitxml={self.results_dir}/security_test_results.xml"
        ]
        
        return self._run_pytest_command(cmd, "security_test")
    
    def run_full_test_suite(self) -> bool:
        """åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶"""
        print("ğŸ¯ åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶...")
        
        cmd = [
            "python", "-m", "pytest",
            "test_api_suite.py",
            "-v", "--tb=short",
            f"--junitxml={self.results_dir}/full_test_results.xml",
            f"--html={self.results_dir}/full_test_report.html",
            "--self-contained-html"
        ]
        
        return self._run_pytest_command(cmd, "full_test")
    
    def run_parallel_tests(self, num_workers: int = 4) -> bool:
        """åŸ·è¡Œä¸¦è¡Œæ¸¬è©¦"""
        print(f"ğŸš€ åŸ·è¡Œä¸¦è¡Œæ¸¬è©¦ (å·¥ä½œé€²ç¨‹æ•¸: {num_workers})...")
        
        cmd = [
            "python", "-m", "pytest",
            "test_api_suite.py",
            "-m", "parallel",
            f"-n", str(num_workers),
            "-v", "--tb=short",
            f"--junitxml={self.results_dir}/parallel_test_results.xml"
        ]
        
        return self._run_pytest_command(cmd, "parallel_test")
    
    def _run_pytest_command(self, cmd: List[str], test_type: str) -> bool:
        """åŸ·è¡Œ pytest å‘½ä»¤"""
        start_time = time.time()
        
        try:
            # è¨­ç½®ç’°å¢ƒè®Šé‡
            env = os.environ.copy()
            env['PYTHONPATH'] = os.getcwd()
            
            # åŸ·è¡Œæ¸¬è©¦
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=env,
                timeout=1800  # 30åˆ†é˜è¶…æ™‚
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            # ä¿å­˜åŸ·è¡Œæ—¥èªŒ
            log_file = self.results_dir / f"{test_type}_execution.log"
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(f"æ¸¬è©¦é¡å‹: {test_type}\n")
                f.write(f"åŸ·è¡Œæ™‚é–“: {datetime.now().isoformat()}\n")
                f.write(f"æŒçºŒæ™‚é–“: {duration:.2f} ç§’\n")
                f.write(f"è¿”å›ç¢¼: {result.returncode}\n")
                f.write(f"å‘½ä»¤: {' '.join(cmd)}\n\n")
                f.write("=== STDOUT ===\n")
                f.write(result.stdout)
                f.write("\n=== STDERR ===\n")
                f.write(result.stderr)
            
            # è¼¸å‡ºçµæœæ‘˜è¦
            if result.returncode == 0:
                print(f"âœ… {test_type} æ¸¬è©¦é€šé (è€—æ™‚: {duration:.2f}ç§’)")
                return True
            else:
                print(f"âŒ {test_type} æ¸¬è©¦å¤±æ•— (è€—æ™‚: {duration:.2f}ç§’)")
                print(f"éŒ¯èª¤ä¿¡æ¯: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print(f"â° {test_type} æ¸¬è©¦è¶…æ™‚")
            return False
        except Exception as e:
            print(f"ğŸ’¥ {test_type} æ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}")
            return False
    
    def generate_summary_report(self, test_results: Dict[str, bool]):
        """ç”Ÿæˆæ¸¬è©¦æ‘˜è¦å ±å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ¸¬è©¦æ‘˜è¦å ±å‘Š...")
        
        total_tests = len(test_results)
        passed_tests = sum(1 for result in test_results.values() if result)
        failed_tests = total_tests - passed_tests
        pass_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        summary = {
            "execution_time": datetime.now().isoformat(),
            "total_test_suites": total_tests,
            "passed_test_suites": passed_tests,
            "failed_test_suites": failed_tests,
            "pass_rate": pass_rate,
            "test_results": test_results,
            "environment": {
                "base_url": self.config["api_config"]["base_url"],
                "python_version": sys.version,
                "working_directory": os.getcwd()
            }
        }
        
        # ä¿å­˜ JSON å ±å‘Š
        summary_file = self.results_dir / "test_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆ Markdown å ±å‘Š
        self._generate_markdown_report(summary)
        
        # è¼¸å‡ºæ§åˆ¶å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“‹ æ¸¬è©¦åŸ·è¡Œæ‘˜è¦")
        print("="*60)
        print(f"ç¸½æ¸¬è©¦å¥—ä»¶æ•¸: {total_tests}")
        print(f"é€šéå¥—ä»¶æ•¸: {passed_tests}")
        print(f"å¤±æ•—å¥—ä»¶æ•¸: {failed_tests}")
        print(f"é€šéç‡: {pass_rate:.1f}%")
        print("\nè©³ç´°çµæœ:")
        for test_name, result in test_results.items():
            status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
            print(f"  {test_name}: {status}")
        print("="*60)
        
        return summary
    
    def _generate_markdown_report(self, summary: Dict):
        """ç”Ÿæˆ Markdown æ ¼å¼å ±å‘Š"""
        report_file = self.results_dir / "test_summary.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# test_flow_mcp API æ¸¬è©¦æ‘˜è¦å ±å‘Š\n\n")
            f.write(f"**åŸ·è¡Œæ™‚é–“**: {summary['execution_time']}\n")
            f.write(f"**æ¸¬è©¦ç’°å¢ƒ**: {summary['environment']['base_url']}\n\n")
            
            f.write("## æ¸¬è©¦çµæœæ¦‚è¦½\n\n")
            f.write(f"- **ç¸½æ¸¬è©¦å¥—ä»¶æ•¸**: {summary['total_test_suites']}\n")
            f.write(f"- **é€šéå¥—ä»¶æ•¸**: {summary['passed_test_suites']}\n")
            f.write(f"- **å¤±æ•—å¥—ä»¶æ•¸**: {summary['failed_test_suites']}\n")
            f.write(f"- **é€šéç‡**: {summary['pass_rate']:.1f}%\n\n")
            
            f.write("## è©³ç´°çµæœ\n\n")
            f.write("| æ¸¬è©¦å¥—ä»¶ | ç‹€æ…‹ |\n")
            f.write("|---------|------|\n")
            
            for test_name, result in summary['test_results'].items():
                status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
                f.write(f"| {test_name} | {status} |\n")
            
            f.write("\n## æ¸¬è©¦æ–‡ä»¶ä½ç½®\n\n")
            f.write("æ¸¬è©¦çµæœæ–‡ä»¶ä¿å­˜åœ¨ `test_results/` ç›®éŒ„ä¸‹ï¼š\n\n")
            f.write("- `test_summary.json` - JSON æ ¼å¼æ‘˜è¦\n")
            f.write("- `*_test_results.xml` - JUnit XML æ ¼å¼çµæœ\n")
            f.write("- `*_execution.log` - åŸ·è¡Œæ—¥èªŒ\n")
            f.write("- `full_test_report.html` - HTML æ ¼å¼è©³ç´°å ±å‘Š\n")
    
    def check_environment(self) -> bool:
        """æª¢æŸ¥æ¸¬è©¦ç’°å¢ƒ"""
        print("ğŸ” æª¢æŸ¥æ¸¬è©¦ç’°å¢ƒ...")
        
        # æª¢æŸ¥å¿…è¦çš„ Python åŒ…
        required_packages = ['pytest', 'requests', 'jsonschema']
        missing_packages = []
        
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            print(f"âŒ ç¼ºå°‘å¿…è¦çš„ Python åŒ…: {', '.join(missing_packages)}")
            print("è«‹é‹è¡Œ: pip install pytest requests jsonschema")
            return False
        
        # æª¢æŸ¥ API æœå‹™å¯ç”¨æ€§
        try:
            import requests
            base_url = self.config["api_config"]["base_url"]
            api_key = self.config["api_config"]["api_key"]
            
            response = requests.get(
                f"{base_url}/api/system/health",
                headers={'X-API-Key': api_key},
                timeout=10
            )
            
            if response.status_code == 200:
                print("âœ… API æœå‹™å¯ç”¨")
                return True
            else:
                print(f"âŒ API æœå‹™ä¸å¯ç”¨ (ç‹€æ…‹ç¢¼: {response.status_code})")
                return False
                
        except Exception as e:
            print(f"âŒ ç„¡æ³•é€£æ¥åˆ° API æœå‹™: {e}")
            return False

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="test_flow_mcp API æ¸¬è©¦åŸ·è¡Œå™¨")
    parser.add_argument("--config", default="test_config.json", help="é…ç½®æ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--smoke", action="store_true", help="åŸ·è¡Œå†’ç…™æ¸¬è©¦")
    parser.add_argument("--regression", action="store_true", help="åŸ·è¡Œå›æ­¸æ¸¬è©¦")
    parser.add_argument("--performance", action="store_true", help="åŸ·è¡Œæ€§èƒ½æ¸¬è©¦")
    parser.add_argument("--security", action="store_true", help="åŸ·è¡Œå®‰å…¨æ¸¬è©¦")
    parser.add_argument("--full", action="store_true", help="åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶")
    parser.add_argument("--parallel", type=int, metavar="N", help="åŸ·è¡Œä¸¦è¡Œæ¸¬è©¦ (æŒ‡å®šå·¥ä½œé€²ç¨‹æ•¸)")
    parser.add_argument("--check-env", action="store_true", help="æª¢æŸ¥æ¸¬è©¦ç’°å¢ƒ")
    parser.add_argument("--all", action="store_true", help="åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦é¡å‹")
    
    args = parser.parse_args()
    
    # å‰µå»ºæ¸¬è©¦åŸ·è¡Œå™¨
    runner = TestRunner(args.config)
    
    # æª¢æŸ¥ç’°å¢ƒ
    if args.check_env or not any([args.smoke, args.regression, args.performance, 
                                  args.security, args.full, args.parallel, args.all]):
        if not runner.check_environment():
            sys.exit(1)
        if args.check_env:
            return
    
    # åŸ·è¡Œæ¸¬è©¦
    test_results = {}
    
    if args.all:
        # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦é¡å‹
        test_results["smoke"] = runner.run_smoke_tests()
        test_results["regression"] = runner.run_regression_tests()
        test_results["security"] = runner.run_security_tests()
        test_results["performance"] = runner.run_performance_tests()
        test_results["full"] = runner.run_full_test_suite()
    else:
        # åŸ·è¡ŒæŒ‡å®šçš„æ¸¬è©¦é¡å‹
        if args.smoke:
            test_results["smoke"] = runner.run_smoke_tests()
        
        if args.regression:
            test_results["regression"] = runner.run_regression_tests()
        
        if args.performance:
            test_results["performance"] = runner.run_performance_tests()
        
        if args.security:
            test_results["security"] = runner.run_security_tests()
        
        if args.full:
            test_results["full"] = runner.run_full_test_suite()
        
        if args.parallel:
            test_results["parallel"] = runner.run_parallel_tests(args.parallel)
    
    # å¦‚æœæ²’æœ‰æŒ‡å®šä»»ä½•æ¸¬è©¦é¡å‹ï¼ŒåŸ·è¡Œå†’ç…™æ¸¬è©¦
    if not test_results:
        test_results["smoke"] = runner.run_smoke_tests()
    
    # ç”Ÿæˆæ‘˜è¦å ±å‘Š
    summary = runner.generate_summary_report(test_results)
    
    # æ ¹æ“šæ¸¬è©¦çµæœè¨­ç½®é€€å‡ºç¢¼
    if all(test_results.values()):
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦éƒ½é€šéäº†ï¼")
        sys.exit(0)
    else:
        print("\nğŸ’¥ æœ‰æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥è©³ç´°æ—¥èªŒ")
        sys.exit(1)

if __name__ == "__main__":
    main()

